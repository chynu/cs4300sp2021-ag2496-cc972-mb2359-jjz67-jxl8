{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae293444",
   "metadata": {},
   "source": [
    "# ArtistReviewAnalyzer Class Tester\n",
    "\n",
    "Because I'm too lazy to run the file itself so I'm just forcefully doing it through jupyter lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ae73dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d62068",
   "metadata": {},
   "source": [
    "## helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa76d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "import re\n",
    "import unidecode as ud\n",
    "\n",
    "\n",
    "def clean_str(input_artist, cap_code=0):\n",
    "    \"\"\"\n",
    "    Takes in string, returns a unicode-friendly and stripped version of the string.\n",
    "    \"\"\"\n",
    "    return_artist_str = input_artist\n",
    "\n",
    "    # === REGEX REPLACE ===\n",
    "    repl_tuples = [(r'(^\\s+)|(\\s+$)', ''),  # whitespace at beg/end of string\n",
    "                   (r'\\s+', ' '),  # Remove double spaces\n",
    "                   (r'[\\n|\\r|\\t|\\0]+', ' ')\n",
    "                   ]\n",
    "    for ptn, repl_str in repl_tuples:\n",
    "        return_artist_str = re.sub(ptn, repl_str, return_artist_str)\n",
    "\n",
    "    # === UNICODE HANDLING ===\n",
    "    return_artist_str = ud.unidecode(return_artist_str)\n",
    "\n",
    "    if cap_code == -1:\n",
    "        return_artist_str = return_artist_str.lower()\n",
    "    elif cap_code == 1:\n",
    "        return_artist_str = return_artist_str.upper()\n",
    "\n",
    "    return return_artist_str\n",
    "\n",
    "\n",
    "def read_mard_json(input_filename):\n",
    "    \"\"\"\n",
    "    Takes in file name of JSON, returns a list of dictionaries in which\n",
    "    each element is a row with columns (as the keys).\n",
    "    \"\"\"\n",
    "    loaded_data_ = []\n",
    "    with open(input_filename, 'r') as file_:\n",
    "        loaded_string_ = file_.read()\n",
    "        loaded_data_ = [json.loads(s) for s in loaded_string_.split('\\n') if s is not None and len(s) > 0]\n",
    "        file_.close()\n",
    "    return loaded_data_\n",
    "\n",
    "\n",
    "def read_mard_json_as_df(input_filename):\n",
    "    \"\"\"\n",
    "    Takes in file name of JSON, returns a list of dictionaries in which\n",
    "    each element is a row with columns (as the keys).\n",
    "    \"\"\"\n",
    "    loaded_data_ = []\n",
    "    with open(input_filename, 'r') as file_:\n",
    "        loaded_string_ = file_.read()\n",
    "        loaded_data_ = [json.loads(s) for s in loaded_string_.split('\\n') if s is not None and len(s) > 0]\n",
    "        file_.close()\n",
    "    return pd.DataFrame(loaded_data_)\n",
    "\n",
    "\n",
    "def run_query_on_sqlite_db(input_query, input_filename):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns a Pandas DataFrame object containing the query results,\n",
    "    given the user's query and the filename for the sqlite database.\n",
    "\n",
    "    Input:\n",
    "     - input_query: string representation of the SQL query to run on the sqlite db\n",
    "     - input_filename: the file location of the sqlite database\n",
    "\n",
    "    \"\"\"\n",
    "    conn_ = sqlite3.connect(input_filename)\n",
    "    df_ = pd.read_sql_query(input_query, conn_)\n",
    "    conn_.close()\n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801c774",
   "metadata": {},
   "source": [
    "## ArtistReviewAnalyzer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0b39a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "\n",
    "class ArtistReviewAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes artist reviews. Forms into useful data structures for NLP and topic modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Basic data\n",
    "        self.file_loc = None\n",
    "        self.raw = None\n",
    "        self.artists_list = None\n",
    "        self.all_tokens = None\n",
    "\n",
    "        # Stop-Words for tokenization\n",
    "        self.stop_words = stopwords.words('english')  # Basic stop words\n",
    "        self.update_stopwords(['album', 'music', 'cd', 'track', 'song', 'sound'])\n",
    "\n",
    "        # Vectorizers\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.count_vectorizer = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.count_matrix = None\n",
    "        self.__tokenizer = None\n",
    "        self.tokenized_reviews = None\n",
    "\n",
    "        # NLP tools\n",
    "        self.lda_model = None\n",
    "        self.bigrams = None\n",
    "        self.lemmatized_text = None\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])  # Spacy tool\n",
    "\n",
    "\n",
    "    def load_data(self, file_loc):\n",
    "        \"\"\"\n",
    "        Loads reviews data (JSON file) into a dictionary.\n",
    "        \"\"\"\n",
    "        # Save file location\n",
    "        self.file_loc = file_loc\n",
    "\n",
    "        with open(file_loc, 'r') as file:\n",
    "            d = json.load(file)\n",
    "            file.close()\n",
    "        self.raw = d  # Save raw dictionary\n",
    "\n",
    "        self.artists_list = list(self.raw.keys())  # Save artists list\n",
    "\n",
    "        self.build_count_vectorizer(2, 0.8)  # Build count vec so we have a tokenizer\n",
    "\n",
    "        # Handling tokenized list of reviews\n",
    "        self.tokenized_reviews = self.__build_reviews_list(do_tokenize=True)\n",
    "        self.remove_stopwords_from_tokenized_list()\n",
    "        self.set_all_tokens()  # Get tokenized corpus\n",
    "\n",
    "        return self.raw\n",
    "\n",
    "    def refresh(self):\n",
    "        self.build_count_vectorizer(2, 0.8)\n",
    "        self.tokenized_reviews = self.__build_reviews_list(do_tokenize=True)\n",
    "        self.remove_stopwords_from_tokenized_list()\n",
    "        self.set_all_tokens()\n",
    "\n",
    "    def build_count_vectorizer(self, min_df, max_df):\n",
    "        if self.count_vectorizer is not None:\n",
    "            print(\"WARNING: Count vectorizer has already been built.\")\n",
    "            return self.count_vectorizer\n",
    "        self.count_vectorizer = CountVectorizer(analyzer='word', stop_words=self.stop_words, min_df=min_df, max_df=max_df)\n",
    "        self.__tokenizer = self.count_vectorizer.build_tokenizer()\n",
    "        return self.count_vectorizer\n",
    "\n",
    "    def get_count_matrix(self):\n",
    "        if self.count_vectorizer is None:\n",
    "            return None\n",
    "\n",
    "        if self.count_matrix is None:\n",
    "            # TODO: self.raw is not truly reflective of the data since we clean it.\n",
    "            self.count_matrix = self.count_vectorizer.fit_transform(self.all_tokens)\n",
    "        return self.count_matrix\n",
    "\n",
    "    def build_tfidf_vectorizer(self, min_df, max_df):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words=self.stop_words, min_df=min_df, max_df=max_df)\n",
    "        return self.tfidf_vectorizer\n",
    "\n",
    "    def get_tfidf_matrix(self):\n",
    "        if self.tfidf_vectorizer is None:\n",
    "            return None\n",
    "\n",
    "        if self.tfidf_matrix is None:\n",
    "            # TODO: self.raw is not truly reflective of the data since we clean it.\n",
    "            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.all_tokens)\n",
    "        return self.tfidf_matrix\n",
    "\n",
    "    def tokenize(self, input_string):\n",
    "        if self.__tokenizer is None:\n",
    "            print(\"WARNING: Tokenizer not set.\")\n",
    "            return None\n",
    "        return self.__tokenizer(input_string)\n",
    "\n",
    "    def update_stopwords(self, word_list):\n",
    "        self.stop_words.extend(word_list)\n",
    "        return None\n",
    "\n",
    "    def remove_stopwords_from_tokenized_list(self):\n",
    "        self.tokenized_reviews = \\\n",
    "            [[w for w in artist_review if w not in self.stop_words] for artist_review in self.tokenized_reviews]\n",
    "\n",
    "    def build_lda_model(self, num_topics=20):\n",
    "        self.bigrams = self.make_bigrams()\n",
    "        self.lemmatized_text = self.lemmatize()\n",
    "        id2word = corpora.Dictionary(self.lemmatized_text)\n",
    "        corpus = [id2word.doc2bow(w) for w in self.lemmatized_text]\n",
    "        self.lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "        return self.lda_model\n",
    "\n",
    "    def make_bigrams(self):\n",
    "        bg = gensim.models.Phrases(self.tokenized_reviews, min_count=5, threshold=100)\n",
    "        bg_mod = gensim.models.phrases.Phraser(bg)\n",
    "        return [bg_mod[d] for d in self.tokenized_reviews]\n",
    "\n",
    "    def lemmatize(self, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        texts_out = []\n",
    "        for review_words in self.tokenized_reviews:\n",
    "            joined_words = self.nlp(\" \".join(review_words))\n",
    "            texts_out.append([token.lemma_ for token in joined_words if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "\n",
    "    def set_all_tokens(self):\n",
    "        self.all_tokens = list(itertools.chain.from_iterable(self.tokenized_reviews))\n",
    "        return self.all_tokens\n",
    "\n",
    "    def get_all_tokens(self):\n",
    "        return self.all_tokens\n",
    "\n",
    "    def __build_reviews_list(self, do_tokenize=False):\n",
    "        \"\"\"\n",
    "        Builds a list of reviews for each artist.\n",
    "        \"\"\"\n",
    "        consolidated_reviews = []\n",
    "        for a in self.artists_list:\n",
    "            if do_tokenize:\n",
    "                consolidated_reviews.append(self.tokenize(clean_str(\" \".join(self.raw[a]))))\n",
    "            else:\n",
    "                consolidated_reviews.append(clean_str(\" \".join(self.raw[a])))\n",
    "        return consolidated_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2368a1f",
   "metadata": {},
   "source": [
    "## Test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb13abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_location = \"../data/processed/artist_reviews.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8bd321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ArtistReviewAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17871591",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.load_data(json_file_location);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6576dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.build_lda_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ce79245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*\"quot\" + 0.008*\"good\" + 0.007*\"make\" + 0.006*\"song\" + 0.006*\"well\" + '\n",
      "  '0.005*\"great\" + 0.005*\"even\" + 0.005*\"hear\" + 0.005*\"work\" + 0.004*\"first\"'),\n",
      " (1,\n",
      "  '0.008*\"get\" + 0.008*\"good\" + 0.008*\"great\" + 0.007*\"song\" + 0.006*\"well\" + '\n",
      "  '0.006*\"record\" + 0.005*\"first\" + 0.005*\"love\" + 0.005*\"make\" + '\n",
      "  '0.005*\"band\"'),\n",
      " (2,\n",
      "  '0.010*\"quot\" + 0.008*\"song\" + 0.007*\"great\" + 0.007*\"good\" + 0.006*\"well\" + '\n",
      "  '0.006*\"get\" + 0.006*\"record\" + 0.005*\"time\" + 0.005*\"make\" + 0.005*\"band\"'),\n",
      " (3,\n",
      "  '0.011*\"song\" + 0.009*\"good\" + 0.008*\"love\" + 0.008*\"make\" + 0.007*\"get\" + '\n",
      "  '0.006*\"time\" + 0.006*\"great\" + 0.005*\"cd\" + 0.005*\"well\" + 0.005*\"hear\"'),\n",
      " (4,\n",
      "  '0.008*\"make\" + 0.006*\"record\" + 0.006*\"band\" + 0.006*\"good\" + 0.005*\"well\" '\n",
      "  '+ 0.005*\"go\" + 0.005*\"time\" + 0.004*\"get\" + 0.004*\"song\" + 0.004*\"first\"'),\n",
      " (5,\n",
      "  '0.008*\"make\" + 0.007*\"song\" + 0.006*\"get\" + 0.006*\"well\" + 0.006*\"good\" + '\n",
      "  '0.006*\"quot\" + 0.006*\"band\" + 0.006*\"time\" + 0.005*\"record\" + 0.004*\"say\"'),\n",
      " (6,\n",
      "  '0.009*\"quot\" + 0.009*\"song\" + 0.007*\"good\" + 0.006*\"cd\" + 0.006*\"get\" + '\n",
      "  '0.006*\"great\" + 0.006*\"make\" + 0.005*\"time\" + 0.005*\"hear\" + 0.004*\"even\"'),\n",
      " (7,\n",
      "  '0.007*\"good\" + 0.007*\"song\" + 0.006*\"get\" + 0.006*\"well\" + 0.006*\"even\" + '\n",
      "  '0.005*\"great\" + 0.005*\"band\" + 0.005*\"time\" + 0.005*\"work\" + 0.004*\"come\"'),\n",
      " (8,\n",
      "  '0.007*\"good\" + 0.006*\"get\" + 0.006*\"record\" + 0.005*\"song\" + 0.005*\"make\" + '\n",
      "  '0.005*\"band\" + 0.005*\"play\" + 0.005*\"hear\" + 0.005*\"great\" + 0.004*\"cd\"'),\n",
      " (9,\n",
      "  '0.009*\"song\" + 0.007*\"good\" + 0.006*\"great\" + 0.006*\"record\" + 0.006*\"make\" '\n",
      "  '+ 0.006*\"get\" + 0.005*\"well\" + 0.005*\"first\" + 0.005*\"cd\" + 0.005*\"love\"'),\n",
      " (10,\n",
      "  '0.008*\"make\" + 0.007*\"band\" + 0.006*\"quot\" + 0.006*\"good\" + 0.006*\"time\" + '\n",
      "  '0.006*\"get\" + 0.005*\"record\" + 0.005*\"song\" + 0.005*\"much\" + 0.004*\"cd\"'),\n",
      " (11,\n",
      "  '0.008*\"quot\" + 0.006*\"good\" + 0.006*\"great\" + 0.005*\"get\" + 0.005*\"time\" + '\n",
      "  '0.005*\"record\" + 0.005*\"song\" + 0.005*\"make\" + 0.005*\"hear\" + 0.005*\"well\"'),\n",
      " (12,\n",
      "  '0.010*\"song\" + 0.008*\"good\" + 0.008*\"make\" + 0.008*\"quot\" + 0.007*\"well\" + '\n",
      "  '0.007*\"get\" + 0.007*\"great\" + 0.007*\"record\" + 0.005*\"go\" + 0.005*\"time\"'),\n",
      " (13,\n",
      "  '0.009*\"good\" + 0.009*\"make\" + 0.007*\"song\" + 0.007*\"great\" + 0.006*\"love\" + '\n",
      "  '0.005*\"even\" + 0.005*\"quot\" + 0.005*\"get\" + 0.005*\"time\" + 0.005*\"go\"'),\n",
      " (14,\n",
      "  '0.006*\"good\" + 0.006*\"get\" + 0.006*\"make\" + 0.006*\"great\" + 0.005*\"record\" '\n",
      "  '+ 0.005*\"band\" + 0.005*\"well\" + 0.005*\"listen\" + 0.005*\"song\" + '\n",
      "  '0.005*\"year\"'),\n",
      " (15,\n",
      "  '0.007*\"song\" + 0.006*\"make\" + 0.006*\"record\" + 0.006*\"well\" + 0.006*\"good\" '\n",
      "  '+ 0.005*\"get\" + 0.005*\"release\" + 0.005*\"time\" + 0.004*\"love\" + '\n",
      "  '0.004*\"great\"'),\n",
      " (16,\n",
      "  '0.009*\"song\" + 0.008*\"well\" + 0.007*\"love\" + 0.006*\"good\" + 0.006*\"band\" + '\n",
      "  '0.005*\"get\" + 0.005*\"quot\" + 0.005*\"great\" + 0.005*\"time\" + 0.005*\"come\"'),\n",
      " (17,\n",
      "  '0.010*\"good\" + 0.009*\"band\" + 0.007*\"song\" + 0.007*\"make\" + 0.006*\"great\" + '\n",
      "  '0.006*\"record\" + 0.006*\"get\" + 0.005*\"year\" + 0.005*\"time\" + 0.005*\"even\"'),\n",
      " (18,\n",
      "  '0.008*\"good\" + 0.008*\"song\" + 0.007*\"well\" + 0.006*\"play\" + 0.006*\"time\" + '\n",
      "  '0.006*\"cd\" + 0.005*\"great\" + 0.005*\"go\" + 0.005*\"hear\" + 0.005*\"get\"'),\n",
      " (19,\n",
      "  '0.008*\"good\" + 0.007*\"record\" + 0.007*\"get\" + 0.006*\"quot\" + 0.006*\"great\" '\n",
      "  '+ 0.006*\"first\" + 0.006*\"make\" + 0.005*\"even\" + 0.005*\"well\" + '\n",
      "  '0.005*\"song\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(analyzer.lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db31b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ArtistReviewAnalyzer()\n",
    "analyzer.update_stopwords(['good', 'band', 'record', 'make', 'great', 'hear'])\n",
    "analyzer.load_data(json_file_location);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3812fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.build_lda_model(num_topics=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28463935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10,\n",
      "  '0.006*\"even\" + 0.005*\"song\" + 0.005*\"get\" + 0.005*\"time\" + 0.004*\"well\" + '\n",
      "  '0.004*\"work\" + 0.004*\"make\" + 0.004*\"sound\" + 0.004*\"come\" + 0.004*\"go\"'),\n",
      " (22,\n",
      "  '0.008*\"get\" + 0.006*\"time\" + 0.006*\"song\" + 0.006*\"go\" + 0.005*\"quot\" + '\n",
      "  '0.005*\"first\" + 0.005*\"well\" + 0.004*\"love\" + 0.004*\"make\" + 0.004*\"good\"'),\n",
      " (39,\n",
      "  '0.007*\"song\" + 0.006*\"get\" + 0.005*\"come\" + 0.005*\"time\" + 0.005*\"new\" + '\n",
      "  '0.004*\"play\" + 0.004*\"first\" + 0.004*\"love\" + 0.004*\"work\" + 0.004*\"good\"'),\n",
      " (1,\n",
      "  '0.010*\"song\" + 0.008*\"quot\" + 0.007*\"well\" + 0.006*\"time\" + 0.006*\"get\" + '\n",
      "  '0.005*\"go\" + 0.005*\"love\" + 0.005*\"first\" + 0.004*\"come\" + 0.004*\"year\"'),\n",
      " (38,\n",
      "  '0.008*\"song\" + 0.006*\"go\" + 0.005*\"even\" + 0.005*\"first\" + 0.005*\"make\" + '\n",
      "  '0.004*\"well\" + 0.004*\"work\" + 0.004*\"get\" + 0.004*\"be\" + 0.004*\"voice\"'),\n",
      " (11,\n",
      "  '0.008*\"get\" + 0.007*\"song\" + 0.006*\"love\" + 0.006*\"quot\" + 0.005*\"make\" + '\n",
      "  '0.005*\"well\" + 0.005*\"time\" + 0.004*\"find\" + 0.004*\"go\" + 0.004*\"work\"'),\n",
      " (12,\n",
      "  '0.008*\"quot\" + 0.007*\"song\" + 0.007*\"get\" + 0.006*\"well\" + 0.005*\"love\" + '\n",
      "  '0.005*\"year\" + 0.005*\"come\" + 0.005*\"work\" + 0.005*\"go\" + 0.004*\"time\"'),\n",
      " (21,\n",
      "  '0.007*\"quot\" + 0.006*\"song\" + 0.006*\"cd\" + 0.006*\"time\" + 0.005*\"well\" + '\n",
      "  '0.005*\"make\" + 0.005*\"get\" + 0.005*\"love\" + 0.005*\"listen\" + 0.005*\"first\"'),\n",
      " (0,\n",
      "  '0.010*\"quot\" + 0.007*\"song\" + 0.005*\"get\" + 0.005*\"well\" + 0.005*\"love\" + '\n",
      "  '0.005*\"much\" + 0.005*\"work\" + 0.004*\"even\" + 0.004*\"good\" + 0.004*\"make\"'),\n",
      " (8,\n",
      "  '0.007*\"song\" + 0.007*\"well\" + 0.005*\"time\" + 0.005*\"get\" + 0.005*\"play\" + '\n",
      "  '0.004*\"make\" + 0.004*\"year\" + 0.004*\"take\" + 0.004*\"sound\" + 0.004*\"say\"'),\n",
      " (18,\n",
      "  '0.011*\"song\" + 0.006*\"get\" + 0.005*\"play\" + 0.005*\"well\" + 0.005*\"time\" + '\n",
      "  '0.005*\"new\" + 0.005*\"first\" + 0.005*\"come\" + 0.005*\"listen\" + 0.004*\"quot\"'),\n",
      " (24,\n",
      "  '0.007*\"song\" + 0.006*\"get\" + 0.005*\"well\" + 0.005*\"cd\" + 0.005*\"first\" + '\n",
      "  '0.005*\"year\" + 0.005*\"love\" + 0.004*\"make\" + 0.004*\"even\" + 0.004*\"work\"'),\n",
      " (33,\n",
      "  '0.008*\"song\" + 0.007*\"love\" + 0.007*\"quot\" + 0.006*\"even\" + 0.005*\"get\" + '\n",
      "  '0.005*\"cd\" + 0.005*\"well\" + 0.004*\"good\" + 0.004*\"come\" + 0.004*\"first\"'),\n",
      " (37,\n",
      "  '0.009*\"song\" + 0.008*\"well\" + 0.007*\"even\" + 0.007*\"make\" + 0.006*\"get\" + '\n",
      "  '0.006*\"love\" + 0.006*\"listen\" + 0.006*\"time\" + 0.005*\"first\" + 0.005*\"cd\"'),\n",
      " (19,\n",
      "  '0.007*\"well\" + 0.005*\"time\" + 0.005*\"recording\" + 0.005*\"make\" + '\n",
      "  '0.004*\"get\" + 0.004*\"quot\" + 0.004*\"first\" + 0.004*\"record\" + 0.004*\"also\" '\n",
      "  '+ 0.004*\"even\"'),\n",
      " (35,\n",
      "  '0.008*\"well\" + 0.007*\"quot\" + 0.007*\"song\" + 0.006*\"recording\" + 0.006*\"cd\" '\n",
      "  '+ 0.005*\"time\" + 0.005*\"get\" + 0.005*\"year\" + 0.005*\"work\" + 0.004*\"know\"'),\n",
      " (9,\n",
      "  '0.008*\"song\" + 0.007*\"quot\" + 0.006*\"first\" + 0.005*\"go\" + 0.005*\"get\" + '\n",
      "  '0.005*\"well\" + 0.004*\"much\" + 0.004*\"work\" + 0.004*\"come\" + 0.004*\"even\"'),\n",
      " (5,\n",
      "  '0.010*\"song\" + 0.008*\"get\" + 0.007*\"well\" + 0.006*\"time\" + 0.006*\"quot\" + '\n",
      "  '0.006*\"cd\" + 0.005*\"love\" + 0.005*\"first\" + 0.005*\"find\" + 0.005*\"listen\"'),\n",
      " (2,\n",
      "  '0.008*\"get\" + 0.007*\"song\" + 0.007*\"cd\" + 0.007*\"love\" + 0.006*\"time\" + '\n",
      "  '0.006*\"well\" + 0.005*\"go\" + 0.005*\"work\" + 0.005*\"make\" + 0.004*\"year\"'),\n",
      " (13,\n",
      "  '0.008*\"song\" + 0.007*\"quot\" + 0.007*\"get\" + 0.007*\"well\" + 0.005*\"love\" + '\n",
      "  '0.005*\"also\" + 0.005*\"cd\" + 0.005*\"release\" + 0.005*\"time\" + 0.005*\"even\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(analyzer.lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8fb687c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Count vectorizer has already been built.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.8, max_features=None,\n",
       "                min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 = ArtistReviewAnalyzer()\n",
    "a2.load_data(json_file_location)\n",
    "a2.build_count_vectorizer(min_df=2,max_df=0.8)\n",
    "a2.build_tfidf_vectorizer(min_df=2,max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91d98eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2_tfidf = a2.get_tfidf_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d5688f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871.0782534277441"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a2_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64ebf4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2_count = a2.get_count_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fbfeca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1008"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a2_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cedbdc02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaron', 'adrian', 'aguilar', 'alberto', 'alejandro', 'alex', 'alexander', 'alexandra', 'alexandre', 'ali', 'alice', 'america', 'ana', 'anderson', 'andre', 'andrew', 'andrews', 'andy', 'antonio', 'band', 'banda', 'bang', 'beach', 'bear', 'beck', 'belinda', 'ben', 'benjamin', 'benny', 'big', 'bill', 'billy', 'black', 'blanca', 'blind', 'blue', 'blues', 'bob', 'bobby', 'boy', 'brian', 'bronco', 'brown', 'bruce', 'carl', 'carlos', 'cartel', 'cecilia', 'chain', 'charlie', 'chico', 'chris', 'christina', 'church', 'circle', 'city', 'claudio', 'club', 'cold', 'cole', 'collective', 'conjunto', 'country', 'crazy', 'cruz', 'culture', 'da', 'daddy', 'dan', 'daniel', 'dave', 'david', 'day', 'de', 'dead', 'death', 'del', 'di', 'die', 'diego', 'dj', 'dog', 'donna', 'dr', 'dragon', 'duncan', 'durango', 'earth', 'eddie', 'edgar', 'el', 'electric', 'elliott', 'elvis', 'emilio', 'english', 'enrique', 'eric', 'erik', 'faith', 'fernandez', 'fernando', 'ferreira', 'fiona', 'francisco', 'frankie', 'franz', 'future', 'gabriel', 'gang', 'garcia', 'garden', 'gary', 'georg', 'george', 'gilberto', 'gomez', 'gonzalez', 'grande', 'green', 'grupo', 'gustav', 'gustavo', 'guzman', 'hall', 'hernandez', 'high', 'hot', 'howard', 'iglesias', 'ismael', 'james', 'jane', 'jason', 'jay', 'jean', 'jeff', 'jerry', 'jesus', 'jim', 'jimmy', 'joan', 'joao', 'joaquin', 'joe', 'joey', 'johann', 'john', 'johnny', 'jon', 'jones', 'jorge', 'jose', 'juan', 'jules', 'julian', 'julio', 'justin', 'kamen', 'karl', 'kelly', 'kid', 'kids', 'king', 'kings', 'la', 'lady', 'lalo', 'lara', 'last', 'laura', 'lee', 'leo', 'leon', 'lewis', 'lil', 'little', 'living', 'lloyd', 'local', 'lopez', 'los', 'lucia', 'luis', 'luiz', 'mac', 'man', 'maranatha', 'marco', 'marcos', 'maria', 'mark', 'marley', 'martha', 'martin', 'marvin', 'mary', 'masta', 'matt', 'mccartney', 'medicine', 'melanie', 'michael', 'midnight', 'miguel', 'mike', 'miller', 'montanez', 'moore', 'moreno', 'morrison', 'musical', 'natalia', 'natalie', 'nick', 'nicole', 'night', 'nine', 'nino', 'noel', 'norte', 'orchestra', 'oscar', 'pablo', 'passion', 'pat', 'patti', 'paul', 'paz', 'pedro', 'perry', 'pete', 'peter', 'philharmonic', 'philharmoniker', 'phillips', 'pink', 'pires', 'plan', 'pop', 'powell', 'rae', 'raul', 'renee', 'reyes', 'ricardo', 'rich', 'richard', 'richie', 'rio', 'rivera', 'robert', 'roberto', 'rocio', 'rock', 'rodriguez', 'roger', 'romeo', 'romero', 'roots', 'rosario', 'roy', 'ruben', 'ruiz', 'ryan', 'san', 'santa', 'santiago', 'santos', 'savage', 'scott', 'sebastian', 'sergio', 'sheena', 'sierra', 'silva', 'simon', 'simpson', 'sinaloa', 'singh', 'sir', 'sky', 'slim', 'smith', 'social', 'solis', 'sonora', 'soul', 'sounds', 'st', 'stan', 'star', 'stars', 'steve', 'steven', 'stone', 'strauss', 'strings', 'symphony', 'thug', 'tito', 'tom', 'tommy', 'tony', 'toro', 'torres', 'trevor', 'tropical', 'van', 'vargas', 'vaughan', 'vladimir', 'wainwright', 'walter', 'war', 'waters', 'watson', 'wax', 'wayne', 'webber', 'williams', 'willie', 'wilson', 'wizard', 'yeah', 'york', 'young', 'youth', 'zoe']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5816d4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaron', 'adrian', 'aguilar', 'alberto', 'alejandro', 'alex', 'alexander', 'alexandra', 'alexandre', 'ali', 'alice', 'america', 'ana', 'anderson', 'andre', 'andrew', 'andrews', 'andy', 'antonio', 'band', 'banda', 'bang', 'beach', 'bear', 'beck', 'belinda', 'ben', 'benjamin', 'benny', 'big', 'bill', 'billy', 'black', 'blanca', 'blind', 'blue', 'blues', 'bob', 'bobby', 'boy', 'brian', 'bronco', 'brown', 'bruce', 'carl', 'carlos', 'cartel', 'cecilia', 'chain', 'charlie', 'chico', 'chris', 'christina', 'church', 'circle', 'city', 'claudio', 'club', 'cold', 'cole', 'collective', 'conjunto', 'country', 'crazy', 'cruz', 'culture', 'da', 'daddy', 'dan', 'daniel', 'dave', 'david', 'day', 'de', 'dead', 'death', 'del', 'di', 'die', 'diego', 'dj', 'dog', 'donna', 'dr', 'dragon', 'duncan', 'durango', 'earth', 'eddie', 'edgar', 'el', 'electric', 'elliott', 'elvis', 'emilio', 'english', 'enrique', 'eric', 'erik', 'faith', 'fernandez', 'fernando', 'ferreira', 'fiona', 'francisco', 'frankie', 'franz', 'future', 'gabriel', 'gang', 'garcia', 'garden', 'gary', 'georg', 'george', 'gilberto', 'gomez', 'gonzalez', 'grande', 'green', 'grupo', 'gustav', 'gustavo', 'guzman', 'hall', 'hernandez', 'high', 'hot', 'howard', 'iglesias', 'ismael', 'james', 'jane', 'jason', 'jay', 'jean', 'jeff', 'jerry', 'jesus', 'jim', 'jimmy', 'joan', 'joao', 'joaquin', 'joe', 'joey', 'johann', 'john', 'johnny', 'jon', 'jones', 'jorge', 'jose', 'juan', 'jules', 'julian', 'julio', 'justin', 'kamen', 'karl', 'kelly', 'kid', 'kids', 'king', 'kings', 'la', 'lady', 'lalo', 'lara', 'last', 'laura', 'lee', 'leo', 'leon', 'lewis', 'lil', 'little', 'living', 'lloyd', 'local', 'lopez', 'los', 'lucia', 'luis', 'luiz', 'mac', 'man', 'maranatha', 'marco', 'marcos', 'maria', 'mark', 'marley', 'martha', 'martin', 'marvin', 'mary', 'masta', 'matt', 'mccartney', 'medicine', 'melanie', 'michael', 'midnight', 'miguel', 'mike', 'miller', 'montanez', 'moore', 'moreno', 'morrison', 'musical', 'natalia', 'natalie', 'nick', 'nicole', 'night', 'nine', 'nino', 'noel', 'norte', 'orchestra', 'oscar', 'pablo', 'passion', 'pat', 'patti', 'paul', 'paz', 'pedro', 'perry', 'pete', 'peter', 'philharmonic', 'philharmoniker', 'phillips', 'pink', 'pires', 'plan', 'pop', 'powell', 'rae', 'raul', 'renee', 'reyes', 'ricardo', 'rich', 'richard', 'richie', 'rio', 'rivera', 'robert', 'roberto', 'rocio', 'rock', 'rodriguez', 'roger', 'romeo', 'romero', 'roots', 'rosario', 'roy', 'ruben', 'ruiz', 'ryan', 'san', 'santa', 'santiago', 'santos', 'savage', 'scott', 'sebastian', 'sergio', 'sheena', 'sierra', 'silva', 'simon', 'simpson', 'sinaloa', 'singh', 'sir', 'sky', 'slim', 'smith', 'social', 'solis', 'sonora', 'soul', 'sounds', 'st', 'stan', 'star', 'stars', 'steve', 'steven', 'stone', 'strauss', 'strings', 'symphony', 'thug', 'tito', 'tom', 'tommy', 'tony', 'toro', 'torres', 'trevor', 'tropical', 'van', 'vargas', 'vaughan', 'vladimir', 'wainwright', 'walter', 'war', 'waters', 'watson', 'wax', 'wayne', 'webber', 'williams', 'willie', 'wilson', 'wizard', 'yeah', 'york', 'young', 'youth', 'zoe']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea5154f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329\n"
     ]
    }
   ],
   "source": [
    "print(len(a2.count_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "062b93d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1784, 329)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d19e4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_freq = np.array(np.sum(a2_count, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3d684eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_w = a2.count_vectorizer.get_feature_names()\n",
    "w_to_i = {w:i for i,w in enumerate(a2.count_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f79ace5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'banda'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_to_w[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c637c8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Count vectorizer has already been built.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.8, max_features=None,\n",
       "                min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3 = ArtistReviewAnalyzer()\n",
    "a3.load_data(json_file_location)\n",
    "a3.build_count_vectorizer(min_df=2,max_df=0.8)\n",
    "a3.build_tfidf_vectorizer(min_df=2,max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "72ef31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3_tfidf = a3.get_tfidf_matrix()\n",
    "a3_count = a3.get_count_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "478b79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3.count_vectorizer.get_feature_names();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dd6aceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(input_analyzer, n):\n",
    "    top_n = []\n",
    "    feats = input_analyzer.count_vectorizer.get_feature_names()\n",
    "    w_counts = np.array(np.sum(input_analyzer.get_count_matrix(), axis=0))[0]\n",
    "    sorted_indices = np.argsort(w_counts)[::-1][:n]\n",
    "    \n",
    "    for i in sorted_indices:\n",
    "        top_n.append((feats[i], w_counts[i]))\n",
    "    return top_n\n",
    "\n",
    "def get_top_n_words(input_analyzer, n):\n",
    "    top_n = []\n",
    "    feats = input_analyzer.count_vectorizer.get_feature_names()\n",
    "    w_counts = np.array(np.sum(input_analyzer.get_count_matrix(), axis=0))[0]\n",
    "    sorted_indices = np.argsort(w_counts)[::-1][:n]\n",
    "    \n",
    "    for i in sorted_indices:\n",
    "        top_n.append(feats[i])\n",
    "    return top_n\n",
    "\n",
    "def get_bottom_n_words(input_analyzer, n):\n",
    "    top_n = []\n",
    "    feats = input_analyzer.count_vectorizer.get_feature_names()\n",
    "    w_counts = np.array(np.sum(input_analyzer.get_count_matrix(), axis=0))[0]\n",
    "    sorted_indices = np.argsort(w_counts)[:n]\n",
    "    \n",
    "    for i in sorted_indices:\n",
    "        top_n.append(feats[i])\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "652b9d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('like', 13878),\n",
      " ('one', 12812),\n",
      " ('songs', 9463),\n",
      " ('quot', 7580),\n",
      " ('great', 6798),\n",
      " ('love', 6094),\n",
      " ('band', 5880),\n",
      " ('good', 5873),\n",
      " ('first', 5803),\n",
      " ('time', 5604),\n",
      " ('best', 5572),\n",
      " ('even', 5272),\n",
      " ('de', 4862),\n",
      " ('new', 4819),\n",
      " ('much', 4652),\n",
      " ('would', 4491),\n",
      " ('well', 4426),\n",
      " ('get', 4336),\n",
      " ('also', 4168),\n",
      " ('still', 4035),\n",
      " ('two', 3994),\n",
      " ('rock', 3792),\n",
      " ('tracks', 3716),\n",
      " ('really', 3672),\n",
      " ('way', 3529),\n",
      " ('record', 3344),\n",
      " ('pop', 3328),\n",
      " ('years', 3326),\n",
      " ('voice', 3256),\n",
      " ('could', 3192),\n",
      " ('many', 3064),\n",
      " ('recording', 3057),\n",
      " ('work', 3052),\n",
      " ('back', 3039),\n",
      " ('never', 3024),\n",
      " ('sounds', 2974),\n",
      " ('la', 2930),\n",
      " ('live', 2918),\n",
      " ('better', 2876),\n",
      " ('make', 2762),\n",
      " ('albums', 2677),\n",
      " ('though', 2553),\n",
      " ('ever', 2522),\n",
      " ('guitar', 2521),\n",
      " ('little', 2511),\n",
      " ('version', 2472),\n",
      " ('heard', 2470),\n",
      " ('know', 2464),\n",
      " ('listen', 2457),\n",
      " ('every', 2443)]\n"
     ]
    }
   ],
   "source": [
    "pprint(get_top_n(a3, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a21eaa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3.update_stopwords(get_top_n_words(a3, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "04761a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Count vectorizer has already been built.\n"
     ]
    }
   ],
   "source": [
    "a3.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d429d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3.build_lda_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "849fe5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.005*\"come\" + 0.004*\"take\" + 0.004*\"play\" + 0.004*\"say\" + 0.004*\"make\" + '\n",
      "  '0.004*\"go\" + 0.004*\"give\" + 0.004*\"get\" + 0.003*\"feel\" + 0.003*\"seem\"'),\n",
      " (1,\n",
      "  '0.004*\"be\" + 0.004*\"come\" + 0.003*\"release\" + 0.003*\"think\" + 0.003*\"make\" '\n",
      "  '+ 0.003*\"feel\" + 0.003*\"cd\" + 0.003*\"sing\" + 0.003*\"go\" + 0.003*\"find\"'),\n",
      " (2,\n",
      "  '0.007*\"go\" + 0.006*\"cd\" + 0.006*\"come\" + 0.005*\"make\" + 0.005*\"play\" + '\n",
      "  '0.004*\"find\" + 0.004*\"say\" + 0.004*\"fan\" + 0.003*\"want\" + 0.003*\"single\"'),\n",
      " (3,\n",
      "  '0.006*\"make\" + 0.006*\"come\" + 0.006*\"go\" + 0.005*\"feel\" + 0.005*\"play\" + '\n",
      "  '0.005*\"take\" + 0.004*\"cd\" + 0.004*\"get\" + 0.004*\"say\" + 0.004*\"be\"'),\n",
      " (4,\n",
      "  '0.005*\"make\" + 0.005*\"play\" + 0.005*\"cd\" + 0.004*\"go\" + 0.004*\"say\" + '\n",
      "  '0.004*\"get\" + 0.003*\"feel\" + 0.003*\"release\" + 0.003*\"see\" + 0.003*\"come\"'),\n",
      " (5,\n",
      "  '0.006*\"make\" + 0.005*\"go\" + 0.004*\"take\" + 0.004*\"release\" + 0.004*\"say\" + '\n",
      "  '0.004*\"get\" + 0.004*\"come\" + 0.004*\"give\" + 0.003*\"feel\" + 0.003*\"cd\"'),\n",
      " (6,\n",
      "  '0.005*\"make\" + 0.004*\"cd\" + 0.004*\"come\" + 0.004*\"be\" + 0.004*\"go\" + '\n",
      "  '0.004*\"say\" + 0.003*\"play\" + 0.003*\"think\" + 0.003*\"get\" + 0.003*\"feel\"'),\n",
      " (7,\n",
      "  '0.005*\"make\" + 0.004*\"come\" + 0.004*\"get\" + 0.004*\"go\" + 0.004*\"take\" + '\n",
      "  '0.003*\"release\" + 0.003*\"que\" + 0.003*\"play\" + 0.003*\"find\" + 0.003*\"cd\"'),\n",
      " (8,\n",
      "  '0.006*\"cd\" + 0.005*\"make\" + 0.005*\"find\" + 0.005*\"come\" + 0.004*\"go\" + '\n",
      "  '0.004*\"be\" + 0.004*\"feel\" + 0.003*\"fan\" + 0.003*\"get\" + 0.003*\"release\"'),\n",
      " (9,\n",
      "  '0.005*\"be\" + 0.005*\"go\" + 0.005*\"cd\" + 0.005*\"hit\" + 0.005*\"make\" + '\n",
      "  '0.005*\"release\" + 0.004*\"come\" + 0.004*\"get\" + 0.004*\"take\" + '\n",
      "  '0.004*\"single\"'),\n",
      " (10,\n",
      "  '0.006*\"cd\" + 0.005*\"make\" + 0.004*\"get\" + 0.004*\"go\" + 0.004*\"find\" + '\n",
      "  '0.004*\"play\" + 0.003*\"take\" + 0.003*\"come\" + 0.003*\"say\" + 0.003*\"vocal\"'),\n",
      " (11,\n",
      "  '0.005*\"go\" + 0.004*\"make\" + 0.004*\"feel\" + 0.004*\"play\" + 0.004*\"give\" + '\n",
      "  '0.004*\"movie\" + 0.003*\"find\" + 0.003*\"take\" + 0.003*\"think\" + 0.003*\"get\"'),\n",
      " (12,\n",
      "  '0.006*\"cd\" + 0.005*\"go\" + 0.004*\"play\" + 0.004*\"say\" + 0.004*\"come\" + '\n",
      "  '0.004*\"get\" + 0.004*\"take\" + 0.004*\"make\" + 0.003*\"release\" + 0.003*\"fan\"'),\n",
      " (13,\n",
      "  '0.006*\"make\" + 0.005*\"come\" + 0.005*\"cd\" + 0.005*\"release\" + 0.004*\"take\" + '\n",
      "  '0.004*\"find\" + 0.004*\"say\" + 0.004*\"play\" + 0.003*\"go\" + 0.003*\"vocal\"'),\n",
      " (14,\n",
      "  '0.005*\"come\" + 0.004*\"make\" + 0.004*\"cd\" + 0.004*\"play\" + 0.004*\"take\" + '\n",
      "  '0.004*\"release\" + 0.003*\"go\" + 0.003*\"give\" + 0.003*\"performance\" + '\n",
      "  '0.003*\"find\"'),\n",
      " (15,\n",
      "  '0.005*\"make\" + 0.004*\"feel\" + 0.004*\"come\" + 0.004*\"cd\" + 0.003*\"play\" + '\n",
      "  '0.003*\"find\" + 0.003*\"release\" + 0.003*\"take\" + 0.003*\"say\" + 0.003*\"give\"'),\n",
      " (16,\n",
      "  '0.004*\"make\" + 0.004*\"cd\" + 0.004*\"take\" + 0.004*\"come\" + 0.003*\"go\" + '\n",
      "  '0.003*\"fan\" + 0.003*\"give\" + 0.003*\"say\" + 0.003*\"feel\" + 0.003*\"que\"'),\n",
      " (17,\n",
      "  '0.004*\"feel\" + 0.004*\"cd\" + 0.004*\"make\" + 0.004*\"give\" + 0.003*\"take\" + '\n",
      "  '0.003*\"find\" + 0.003*\"seem\" + 0.003*\"get\" + 0.003*\"go\" + 0.003*\"play\"'),\n",
      " (18,\n",
      "  '0.006*\"cd\" + 0.005*\"make\" + 0.004*\"release\" + 0.004*\"come\" + 0.004*\"play\" + '\n",
      "  '0.004*\"go\" + 0.004*\"feel\" + 0.003*\"be\" + 0.003*\"think\" + 0.003*\"fan\"'),\n",
      " (19,\n",
      "  '0.005*\"cd\" + 0.005*\"go\" + 0.005*\"make\" + 0.004*\"play\" + 0.004*\"take\" + '\n",
      "  '0.003*\"feel\" + 0.003*\"say\" + 0.003*\"buy\" + 0.003*\"get\" + 0.003*\"come\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(a3.lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c8b21118",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3.update_stopwords(get_top_n_words(a3, int(0.3*len(a3.count_vectorizer.get_feature_names()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c60d2a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Count vectorizer has already been built.\n"
     ]
    }
   ],
   "source": [
    "a3.refresh()\n",
    "a3.build_lda_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "742fc12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1b73e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data.pk\", \"wb\") as f:\n",
    "    pickle.dump(a3.lda_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "73d7adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lda = pickle.load(open(\"test_data.pk\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d6609137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.019*\"cd\" + 0.008*\"be\" + 0.006*\"here\" + 0.006*\"there\" + 0.005*\"have\" + '\n",
      "  '0.004*\"now\" + 0.004*\"so\" + 0.004*\"do\" + 0.004*\"even\" + 0.003*\"make\"'),\n",
      " (1,\n",
      "  '0.020*\"be\" + 0.014*\"cd\" + 0.009*\"there\" + 0.006*\"so\" + 0.004*\"when\" + '\n",
      "  '0.004*\"have\" + 0.003*\"hit\" + 0.003*\"do\" + 0.003*\"let\" + 0.003*\"go\"'),\n",
      " (2,\n",
      "  '0.018*\"be\" + 0.010*\"cd\" + 0.006*\"there\" + 0.005*\"when\" + 0.005*\"where\" + '\n",
      "  '0.004*\"even\" + 0.004*\"so\" + 0.003*\"have\" + 0.003*\"just\" + 0.003*\"find\"'),\n",
      " (3,\n",
      "  '0.015*\"there\" + 0.013*\"cd\" + 0.009*\"be\" + 0.007*\"when\" + 0.005*\"so\" + '\n",
      "  '0.004*\"get\" + 0.004*\"even\" + 0.004*\"now\" + 0.003*\"have\" + 0.003*\"make\"'),\n",
      " (4,\n",
      "  '0.012*\"cd\" + 0.010*\"there\" + 0.008*\"be\" + 0.005*\"even\" + 0.004*\"so\" + '\n",
      "  '0.004*\"when\" + 0.003*\"come\" + 0.003*\"still\" + 0.003*\"now\" + 0.003*\"have\"'),\n",
      " (5,\n",
      "  '0.020*\"be\" + 0.015*\"cd\" + 0.015*\"there\" + 0.007*\"when\" + 0.005*\"go\" + '\n",
      "  '0.005*\"so\" + 0.004*\"here\" + 0.004*\"even\" + 0.004*\"live\" + 0.003*\"however\"'),\n",
      " (6,\n",
      "  '0.013*\"be\" + 0.009*\"cd\" + 0.006*\"there\" + 0.004*\"when\" + 0.004*\"so\" + '\n",
      "  '0.003*\"come\" + 0.003*\"just\" + 0.003*\"even\" + 0.003*\"here\" + 0.003*\"love\"'),\n",
      " (7,\n",
      "  '0.028*\"be\" + 0.009*\"cd\" + 0.009*\"there\" + 0.007*\"go\" + 0.005*\"do\" + '\n",
      "  '0.005*\"only\" + 0.005*\"so\" + 0.004*\"make\" + 0.004*\"love\" + 0.004*\"even\"'),\n",
      " (8,\n",
      "  '0.015*\"cd\" + 0.008*\"so\" + 0.008*\"there\" + 0.008*\"when\" + 0.006*\"be\" + '\n",
      "  '0.004*\"do\" + 0.004*\"even\" + 0.003*\"take\" + 0.003*\"die\" + 0.003*\"how\"'),\n",
      " (9,\n",
      "  '0.009*\"be\" + 0.008*\"there\" + 0.006*\"cd\" + 0.004*\"when\" + 0.004*\"even\" + '\n",
      "  '0.003*\"do\" + 0.003*\"have\" + 0.003*\"love\" + 0.003*\"more\" + 0.002*\"More\"'),\n",
      " (10,\n",
      "  '0.023*\"cd\" + 0.015*\"be\" + 0.011*\"there\" + 0.007*\"love\" + 0.006*\"when\" + '\n",
      "  '0.006*\"so\" + 0.005*\"take\" + 0.004*\"do\" + 0.004*\"just\" + 0.004*\"now\"'),\n",
      " (11,\n",
      "  '0.008*\"be\" + 0.008*\"there\" + 0.006*\"even\" + 0.005*\"live\" + 0.005*\"cd\" + '\n",
      "  '0.004*\"so\" + 0.003*\"come\" + 0.003*\"here\" + 0.003*\"when\" + 0.003*\"do\"'),\n",
      " (12,\n",
      "  '0.010*\"cd\" + 0.007*\"love\" + 0.007*\"be\" + 0.005*\"even\" + 0.005*\"there\" + '\n",
      "  '0.004*\"so\" + 0.003*\"know\" + 0.003*\"when\" + 0.002*\"however\" + 0.002*\"get\"'),\n",
      " (13,\n",
      "  '0.021*\"be\" + 0.016*\"cd\" + 0.009*\"there\" + 0.004*\"even\" + 0.004*\"just\" + '\n",
      "  '0.004*\"so\" + 0.003*\"have\" + 0.003*\"when\" + 0.003*\"do\" + 0.003*\"say\"'),\n",
      " (14,\n",
      "  '0.014*\"be\" + 0.013*\"cd\" + 0.012*\"there\" + 0.010*\"when\" + 0.006*\"love\" + '\n",
      "  '0.005*\"so\" + 0.004*\"even\" + 0.004*\"now\" + 0.003*\"do\" + 0.003*\"let\"'),\n",
      " (15,\n",
      "  '0.016*\"cd\" + 0.015*\"be\" + 0.010*\"there\" + 0.008*\"live\" + 0.006*\"when\" + '\n",
      "  '0.006*\"just\" + 0.006*\"even\" + 0.005*\"so\" + 0.004*\"love\" + 0.004*\"have\"'),\n",
      " (16,\n",
      "  '0.014*\"cd\" + 0.010*\"be\" + 0.008*\"there\" + 0.007*\"when\" + 0.004*\"so\" + '\n",
      "  '0.004*\"even\" + 0.003*\"do\" + 0.003*\"just\" + 0.003*\"get\" + 0.003*\"how\"'),\n",
      " (17,\n",
      "  '0.057*\"cd\" + 0.021*\"be\" + 0.008*\"there\" + 0.007*\"so\" + 0.006*\"have\" + '\n",
      "  '0.006*\"love\" + 0.006*\"here\" + 0.006*\"come\" + 0.006*\"great\" + 0.005*\"do\"'),\n",
      " (18,\n",
      "  '0.009*\"there\" + 0.006*\"be\" + 0.006*\"cd\" + 0.005*\"here\" + 0.005*\"so\" + '\n",
      "  '0.005*\"even\" + 0.004*\"when\" + 0.004*\"dolly\" + 0.003*\"however\" + '\n",
      "  '0.003*\"love\"'),\n",
      " (19,\n",
      "  '0.032*\"cd\" + 0.011*\"be\" + 0.009*\"get\" + 0.008*\"there\" + 0.006*\"so\" + '\n",
      "  '0.006*\"when\" + 0.006*\"do\" + 0.005*\"have\" + 0.004*\"just\" + 0.004*\"love\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(test_lda.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "15f752cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['canibus', 'sorprender', 'sorprendente', 'paquin', 'coasters', 'sorgo', 'sorcerer', 'fubert', 'parables', 'fruta', 'paradice', 'sophistipop', 'paradiso', 'paradisum', 'paradoja', 'cobalt', 'frumpy', 'cobbling', 'fuckers', 'coarseness', 'fuckery', 'papydave', 'fuese', 'papercuts', 'sostenuto', 'papered', 'cluttering', 'cmg', 'papitour', 'cnicamente', 'paragons', 'papoose', 'pappalardi', 'coaches', 'coaching', 'sorprendio', 'coalesced', 'pappou', 'coalescing', 'coalition', 'sorriso', 'fugee', 'parakato', 'cobrastyle', 'frontwomen', 'param', 'frontrunner', 'sonne', 'parameter', 'sonidito', 'soni', 'songwritter', 'codify', 'songwritery', 'frontloads', 'coe', 'frontload', 'coexisted', 'songs1', 'songon', 'coffeemaker', 'cocorosie', 'cocooned', 'frosting', 'cocoband', 'coburn', 'frugal', 'paralelas', 'froze', 'cocciante', 'coche', 'frowned', 'cocked', 'cobrasnake', 'sooooooooooooo', 'sooooooooo', 'cockney', 'cocks', 'sooooooo', 'frown', 'cocktails', 'frothing', 'frothier', 'cockeyed', 'songing', 'fugual', 'clusterfuck', 'clits', 'soweto', 'funerary', 'clobbering', 'sovereignty', 'fundamentales', 'panamanian', 'pandeiro', 'functionally', 'cloned', 'funabashi', 'clonking', 'clooney']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bottom_n_words(a3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "776f28dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bottom_n_words_tfidf(input_analyzer, n):\n",
    "    top_n = []\n",
    "    feats = input_analyzer.tfidf_vectorizer.get_feature_names()\n",
    "    w_counts = np.array(np.sum(input_analyzer.get_tfidf_matrix(), axis=0))[0]\n",
    "    sorted_indices = np.argsort(w_counts)[:n]\n",
    "    \n",
    "    for i in sorted_indices:\n",
    "        top_n.append(feats[i])\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9baf1ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['canibus', 'sorprender', 'sorprendente', 'paquin', 'coasters', 'sorgo', 'sorcerer', 'fubert', 'parables', 'fruta']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bottom_n_words_tfidf(a3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bd7417fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['canibus', 'sorprender', 'sorprendente', 'paquin', 'coasters', 'sorgo', 'sorcerer', 'fubert', 'parables', 'fruta']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bottom_n_words(a3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "54f9bc95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_n = 1000\n",
    "len(set(get_bottom_n_words_tfidf(a3, test_n)).intersection(set(get_bottom_n_words(a3, test_n))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c386cf34",
   "metadata": {},
   "source": [
    "# On just pitchfork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6844b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_file = \"../data/processed/artist_reviews_pf_only.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "655f0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfanalyzer = ArtistReviewAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "85c50658",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfanalyzer.load_data(pf_file);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "636d07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfanalyzer.build_lda_model(num_topics=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5b9734f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pf_lda_initial.pk\", \"xb\") as f:\n",
    "    pickle.dump(pfanalyzer.lda_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "caf31206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.008*\"band\" + 0.008*\"song\" + 0.006*\"record\" + 0.006*\"make\" + 0.005*\"get\" + '\n",
      "  '0.005*\"even\" + 0.004*\"go\" + 0.004*\"sound\" + 0.004*\"take\" + 0.004*\"rock\"'),\n",
      " (7,\n",
      "  '0.008*\"band\" + 0.006*\"make\" + 0.006*\"song\" + 0.006*\"get\" + 0.006*\"record\" + '\n",
      "  '0.005*\"well\" + 0.005*\"go\" + 0.004*\"pop\" + 0.004*\"good\" + 0.004*\"time\"'),\n",
      " (15,\n",
      "  '0.006*\"make\" + 0.006*\"song\" + 0.005*\"band\" + 0.005*\"record\" + 0.005*\"year\" '\n",
      "  '+ 0.005*\"even\" + 0.005*\"get\" + 0.004*\"time\" + 0.004*\"feel\" + 0.004*\"come\"'),\n",
      " (27,\n",
      "  '0.010*\"band\" + 0.007*\"make\" + 0.006*\"song\" + 0.005*\"record\" + 0.005*\"sound\" '\n",
      "  '+ 0.005*\"get\" + 0.005*\"even\" + 0.004*\"feel\" + 0.004*\"rock\" + 0.004*\"pop\"'),\n",
      " (17,\n",
      "  '0.009*\"make\" + 0.006*\"get\" + 0.006*\"record\" + 0.005*\"even\" + 0.004*\"way\" + '\n",
      "  '0.004*\"come\" + 0.004*\"time\" + 0.004*\"well\" + 0.004*\"band\" + 0.004*\"take\"'),\n",
      " (5,\n",
      "  '0.007*\"record\" + 0.005*\"band\" + 0.005*\"song\" + 0.005*\"even\" + 0.005*\"get\" + '\n",
      "  '0.004*\"year\" + 0.004*\"new\" + 0.004*\"good\" + 0.004*\"time\" + 0.004*\"feel\"'),\n",
      " (24,\n",
      "  '0.008*\"band\" + 0.007*\"make\" + 0.005*\"time\" + 0.005*\"get\" + 0.005*\"song\" + '\n",
      "  '0.005*\"take\" + 0.005*\"even\" + 0.004*\"year\" + 0.004*\"way\" + 0.004*\"good\"'),\n",
      " (34,\n",
      "  '0.007*\"make\" + 0.007*\"band\" + 0.005*\"get\" + 0.005*\"record\" + 0.004*\"rock\" + '\n",
      "  '0.004*\"song\" + 0.004*\"well\" + 0.004*\"year\" + 0.004*\"take\" + 0.004*\"even\"'),\n",
      " (2,\n",
      "  '0.006*\"record\" + 0.005*\"make\" + 0.005*\"band\" + 0.005*\"feel\" + 0.004*\"get\" + '\n",
      "  '0.004*\"song\" + 0.004*\"year\" + 0.004*\"go\" + 0.004*\"new\" + 0.004*\"first\"'),\n",
      " (6,\n",
      "  '0.008*\"band\" + 0.007*\"song\" + 0.007*\"make\" + 0.006*\"year\" + 0.006*\"get\" + '\n",
      "  '0.006*\"record\" + 0.005*\"even\" + 0.005*\"well\" + 0.005*\"time\" + 0.004*\"much\"'),\n",
      " (32,\n",
      "  '0.007*\"make\" + 0.006*\"record\" + 0.005*\"band\" + 0.005*\"get\" + 0.005*\"time\" + '\n",
      "  '0.005*\"song\" + 0.005*\"year\" + 0.005*\"even\" + 0.004*\"good\" + 0.004*\"much\"'),\n",
      " (28,\n",
      "  '0.008*\"make\" + 0.006*\"record\" + 0.006*\"even\" + 0.005*\"band\" + 0.005*\"song\" '\n",
      "  '+ 0.005*\"get\" + 0.005*\"good\" + 0.004*\"go\" + 0.004*\"sound\" + 0.004*\"time\"'),\n",
      " (23,\n",
      "  '0.007*\"make\" + 0.007*\"band\" + 0.007*\"record\" + 0.006*\"get\" + 0.004*\"even\" + '\n",
      "  '0.004*\"time\" + 0.004*\"song\" + 0.004*\"year\" + 0.004*\"go\" + 0.004*\"come\"'),\n",
      " (30,\n",
      "  '0.006*\"record\" + 0.005*\"band\" + 0.005*\"song\" + 0.004*\"make\" + 0.004*\"even\" '\n",
      "  '+ 0.004*\"come\" + 0.004*\"time\" + 0.004*\"new\" + 0.004*\"get\" + 0.004*\"first\"'),\n",
      " (31,\n",
      "  '0.008*\"make\" + 0.008*\"get\" + 0.006*\"band\" + 0.006*\"feel\" + 0.005*\"come\" + '\n",
      "  '0.005*\"even\" + 0.005*\"song\" + 0.005*\"time\" + 0.005*\"still\" + 0.005*\"pop\"'),\n",
      " (38,\n",
      "  '0.006*\"make\" + 0.006*\"record\" + 0.006*\"song\" + 0.005*\"year\" + 0.005*\"time\" '\n",
      "  '+ 0.005*\"even\" + 0.005*\"good\" + 0.005*\"band\" + 0.005*\"feel\" + 0.004*\"well\"'),\n",
      " (10,\n",
      "  '0.007*\"year\" + 0.006*\"make\" + 0.006*\"band\" + 0.005*\"song\" + 0.005*\"record\" '\n",
      "  '+ 0.005*\"rock\" + 0.004*\"good\" + 0.004*\"get\" + 0.004*\"pop\" + 0.004*\"even\"'),\n",
      " (9,\n",
      "  '0.009*\"make\" + 0.008*\"record\" + 0.007*\"song\" + 0.005*\"band\" + 0.005*\"time\" '\n",
      "  '+ 0.005*\"even\" + 0.004*\"get\" + 0.004*\"feel\" + 0.004*\"work\" + 0.004*\"take\"'),\n",
      " (20,\n",
      "  '0.006*\"make\" + 0.006*\"band\" + 0.006*\"year\" + 0.005*\"record\" + 0.005*\"time\" '\n",
      "  '+ 0.005*\"come\" + 0.005*\"much\" + 0.004*\"feel\" + 0.004*\"take\" + 0.004*\"get\"'),\n",
      " (22,\n",
      "  '0.008*\"band\" + 0.006*\"get\" + 0.006*\"record\" + 0.006*\"year\" + 0.006*\"make\" + '\n",
      "  '0.005*\"even\" + 0.005*\"good\" + 0.005*\"well\" + 0.004*\"time\" + 0.004*\"work\"')]\n"
     ]
    }
   ],
   "source": [
    "pf_lda_model = pfanalyzer.lda_model\n",
    "pprint(pf_lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a70286cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Count vectorizer has already been built.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1212467x34937 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1118590 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfanalyzer.build_count_vectorizer(min_df=30,max_df=0.8)\n",
    "pfanalyzer.build_tfidf_vectorizer(min_df=30,max_df=0.8)\n",
    "pfanalyzer.get_tfidf_matrix()\n",
    "pfanalyzer.get_count_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7ffd933f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'one', 'band', 'songs', 'even', 'new', 'time', 'rock', 'first', 'record', 'pop', 'still', 'much', 'two', 'way', 'love', 'would', 'best', 'years', 'could', 'tracks', 'sounds', 'back', 'get', 'also', 'never', 'though', 'make', 'good', 'well', 'something', 'guitar', 'work', 'year', 'last', 'live', 'little', 'made', 'might', 'life', 'rap', 'long', 'world', 'albums', 'every', 'voice', 'around', 'old', 'many', 'self', 'come', 'single', 'people', 'take', 'ever', 'since', 'know', 'man', 'three', 'better', 'great', 'enough', 'makes', 'always', 'big', 'set', 'less', 'hard', 'really', 'young', 'another', 'feel', 'go', 'thing', 'without', 'yet', 'almost', 'end', 'lyrics', 'point', 'got', 'title', 'often', 'full', 'early', 'release', 'right', 'seems', 'kind', 'sense', 'half', 'records', 'feels', 'things', 'career', 'may', 'part', 'second', 'group', 'debut']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_words(pfanalyzer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "c0113d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preme', 'resequenced', 'ledbetter', 'chocha', 'reservoirs', 'chivalry', 'chitlins', 'chitlin', 'elt', 'reshaped', 'reshapes', 'elucidate', 'reshuffled', 'residual', 'chipmunks', 'residuals', 'elusiveness', 'resiliency', 'resin', 'eluvium', 'lebanese', 'learner', 'resourcefulness', 'respecting', 'chimed', 'leaped', 'chocolates', 'resentful', 'resented', 'leeroy', 'elitism', 'chording', 'leithauser', 'chorales', 'elitists', 'chopsticks', 'chopsquad', 'republicans', 'elixir', 'elizabethtown', 'choppers', 'leigh', 'emailed', 'repulsed', 'ell', 'lego', 'choosin', 'reputed', 'ellipsis', 'rerecorded', 'rerelease', 'leftrightleftrightleft', 'chokeholds', 'chokehold', 'researched', 'resemblances', 'elkland', 'reprisal', 'leant', 'restaging', 'retailing', 'retell', 'retirements', 'retool', 'layla', 'retorts', 'retrace', 'retracing', 'lawless', 'retracting', 'lavelle', 'chesney', 'cheryl', 'retreading', 'retrieving', 'cherries', 'cherrelle', 'laureates', 'launay', 'embraceable', 'cheri', 'cherchez', 'laughin', 'retrofuturism', 'chemo', 'chewy', 'chez', 'lcds', 'resurrecting', 'leaky', 'emancipator', 'chilliness', 'restoring', 'emancipatory', 'chillaxed', 'restrains', 'restrict', 'emasculating', 'leaking', 'restriction']"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bottom_n_words(pfanalyzer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e0b4488e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34937"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pfanalyzer.count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "38c9b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from singernlp.ArtistReviewAnalyzer import ArtistReviewAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "24aff99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfanalyzer.update_stopwords(get_top_n_words(pfanalyzer, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1919105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Count vectorizer has already been built.\n"
     ]
    }
   ],
   "source": [
    "pfanalyzer.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c21b8b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10,\n",
      "  '0.005*\"be\" + 0.002*\"there\" + 0.002*\"even\" + 0.001*\"so\" + 0.001*\"when\" + '\n",
      "  '0.001*\"love\" + 0.001*\"get\" + 0.001*\"keep\" + 0.001*\"back\" + 0.001*\"rise\"'),\n",
      " (22,\n",
      "  '0.004*\"be\" + 0.002*\"there\" + 0.002*\"when\" + 0.002*\"even\" + 0.001*\"add\" + '\n",
      "  '0.001*\"love\" + 0.001*\"go\" + 0.001*\"drop\" + 0.001*\"provide\" + 0.001*\"let\"'),\n",
      " (13,\n",
      "  '0.004*\"be\" + 0.002*\"when\" + 0.002*\"let\" + 0.002*\"there\" + 0.002*\"so\" + '\n",
      "  '0.001*\"get\" + 0.001*\"here\" + 0.001*\"even\" + 0.001*\"drop\" + 0.001*\"grow\"'),\n",
      " (35,\n",
      "  '0.006*\"be\" + 0.002*\"there\" + 0.002*\"when\" + 0.002*\"even\" + 0.002*\"love\" + '\n",
      "  '0.001*\"speak\" + 0.001*\"so\" + 0.001*\"let\" + 0.001*\"include\" + 0.001*\"get\"'),\n",
      " (29,\n",
      "  '0.005*\"be\" + 0.002*\"when\" + 0.002*\"even\" + 0.001*\"get\" + 0.001*\"so\" + '\n",
      "  '0.001*\"add\" + 0.001*\"there\" + 0.001*\"speak\" + 0.001*\"keep\" + 0.001*\"tell\"'),\n",
      " (36,\n",
      "  '0.004*\"be\" + 0.003*\"there\" + 0.002*\"when\" + 0.002*\"so\" + 0.001*\"even\" + '\n",
      "  '0.001*\"let\" + 0.001*\"drop\" + 0.001*\"add\" + 0.001*\"provide\" + 0.001*\"die\"'),\n",
      " (19,\n",
      "  '0.003*\"be\" + 0.002*\"there\" + 0.002*\"when\" + 0.002*\"let\" + 0.002*\"so\" + '\n",
      "  '0.002*\"go\" + 0.002*\"love\" + 0.001*\"even\" + 0.001*\"drop\" + 0.001*\"speak\"'),\n",
      " (33,\n",
      "  '0.004*\"be\" + 0.002*\"when\" + 0.002*\"there\" + 0.002*\"even\" + 0.002*\"so\" + '\n",
      "  '0.001*\"let\" + 0.001*\"get\" + 0.001*\"manage\" + 0.001*\"promise\" + '\n",
      "  '0.001*\"keep\"'),\n",
      " (27,\n",
      "  '0.003*\"be\" + 0.003*\"there\" + 0.002*\"when\" + 0.002*\"so\" + 0.002*\"love\" + '\n",
      "  '0.002*\"let\" + 0.001*\"here\" + 0.001*\"even\" + 0.001*\"add\" + 0.001*\"ask\"'),\n",
      " (4,\n",
      "  '0.004*\"be\" + 0.003*\"there\" + 0.002*\"when\" + 0.002*\"so\" + 0.001*\"get\" + '\n",
      "  '0.001*\"even\" + 0.001*\"let\" + 0.001*\"know\" + 0.001*\"drop\" + 0.001*\"now\"'),\n",
      " (18,\n",
      "  '0.004*\"be\" + 0.002*\"there\" + 0.002*\"so\" + 0.002*\"even\" + 0.002*\"when\" + '\n",
      "  '0.001*\"go\" + 0.001*\"get\" + 0.001*\"still\" + 0.001*\"tell\" + 0.001*\"keep\"'),\n",
      " (31,\n",
      "  '0.002*\"be\" + 0.002*\"there\" + 0.002*\"even\" + 0.001*\"when\" + 0.001*\"so\" + '\n",
      "  '0.001*\"get\" + 0.001*\"serve\" + 0.001*\"happen\" + 0.001*\"drop\" + 0.001*\"cash\"'),\n",
      " (8,\n",
      "  '0.004*\"be\" + 0.002*\"when\" + 0.002*\"so\" + 0.002*\"there\" + 0.002*\"love\" + '\n",
      "  '0.002*\"even\" + 0.001*\"drop\" + 0.001*\"speak\" + 0.001*\"keep\" + 0.001*\"joke\"'),\n",
      " (26,\n",
      "  '0.004*\"be\" + 0.002*\"there\" + 0.002*\"when\" + 0.002*\"even\" + 0.002*\"love\" + '\n",
      "  '0.001*\"so\" + 0.001*\"happen\" + 0.001*\"let\" + 0.001*\"deliver\" + 0.001*\"get\"'),\n",
      " (38,\n",
      "  '0.003*\"be\" + 0.002*\"there\" + 0.002*\"so\" + 0.002*\"when\" + 0.002*\"even\" + '\n",
      "  '0.001*\"let\" + 0.001*\"offer\" + 0.001*\"do\" + 0.001*\"provide\" + 0.001*\"shift\"'),\n",
      " (39,\n",
      "  '0.003*\"be\" + 0.002*\"so\" + 0.002*\"when\" + 0.002*\"there\" + 0.002*\"let\" + '\n",
      "  '0.001*\"add\" + 0.001*\"love\" + 0.001*\"shift\" + 0.001*\"draw\" + 0.001*\"do\"'),\n",
      " (5,\n",
      "  '0.004*\"be\" + 0.002*\"when\" + 0.002*\"there\" + 0.002*\"even\" + 0.002*\"so\" + '\n",
      "  '0.002*\"love\" + 0.001*\"go\" + 0.001*\"let\" + 0.001*\"deliver\" + 0.001*\"miss\"'),\n",
      " (14,\n",
      "  '0.003*\"be\" + 0.003*\"there\" + 0.002*\"so\" + 0.002*\"when\" + 0.001*\"even\" + '\n",
      "  '0.001*\"love\" + 0.001*\"drop\" + 0.001*\"fall\" + 0.001*\"name\" + 0.001*\"get\"'),\n",
      " (30,\n",
      "  '0.004*\"be\" + 0.002*\"there\" + 0.002*\"so\" + 0.002*\"serve\" + 0.002*\"let\" + '\n",
      "  '0.002*\"when\" + 0.002*\"get\" + 0.001*\"love\" + 0.001*\"drop\" + 0.001*\"live\"'),\n",
      " (3,\n",
      "  '0.003*\"be\" + 0.003*\"there\" + 0.002*\"when\" + 0.002*\"even\" + 0.001*\"get\" + '\n",
      "  '0.001*\"so\" + 0.001*\"add\" + 0.001*\"serve\" + 0.001*\"let\" + 0.001*\"speak\"')]\n"
     ]
    }
   ],
   "source": [
    "pfanalyzer.build_lda_model(num_topics=40)\n",
    "pprint(pfanalyzer.lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "65968616",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(pfanalyzer.lemmatized_text)\n",
    "doc_lda = pfanalyzer.lda_model[[id2word.doc2bow(w) for w in pfanalyzer.lemmatized_text]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4c21e300",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pfanalyzer.lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b52ee7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = []\n",
    "for _,s in topics:\n",
    "    all_topics.append(re.findall(r'\\\"[a-z]+\\\"', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8f6b5edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['\"be\"',\n",
      "  '\"when\"',\n",
      "  '\"there\"',\n",
      "  '\"add\"',\n",
      "  '\"even\"',\n",
      "  '\"so\"',\n",
      "  '\"let\"',\n",
      "  '\"get\"',\n",
      "  '\"speak\"',\n",
      "  '\"grow\"'],\n",
      " ['\"be\"',\n",
      "  '\"when\"',\n",
      "  '\"let\"',\n",
      "  '\"there\"',\n",
      "  '\"so\"',\n",
      "  '\"get\"',\n",
      "  '\"here\"',\n",
      "  '\"even\"',\n",
      "  '\"drop\"',\n",
      "  '\"grow\"'],\n",
      " ['\"be\"',\n",
      "  '\"when\"',\n",
      "  '\"there\"',\n",
      "  '\"get\"',\n",
      "  '\"provide\"',\n",
      "  '\"love\"',\n",
      "  '\"serve\"',\n",
      "  '\"even\"',\n",
      "  '\"reveal\"',\n",
      "  '\"shift\"'],\n",
      " ['\"be\"',\n",
      "  '\"there\"',\n",
      "  '\"so\"',\n",
      "  '\"when\"',\n",
      "  '\"even\"',\n",
      "  '\"love\"',\n",
      "  '\"drop\"',\n",
      "  '\"fall\"',\n",
      "  '\"name\"',\n",
      "  '\"get\"'],\n",
      " ['\"be\"',\n",
      "  '\"there\"',\n",
      "  '\"when\"',\n",
      "  '\"love\"',\n",
      "  '\"let\"',\n",
      "  '\"even\"',\n",
      "  '\"so\"',\n",
      "  '\"drop\"',\n",
      "  '\"serve\"',\n",
      "  '\"know\"'],\n",
      " ['\"be\"',\n",
      "  '\"there\"',\n",
      "  '\"even\"',\n",
      "  '\"when\"',\n",
      "  '\"so\"',\n",
      "  '\"get\"',\n",
      "  '\"serve\"',\n",
      "  '\"happen\"',\n",
      "  '\"drop\"',\n",
      "  '\"cash\"'],\n",
      " ['\"be\"',\n",
      "  '\"there\"',\n",
      "  '\"when\"',\n",
      "  '\"even\"',\n",
      "  '\"get\"',\n",
      "  '\"so\"',\n",
      "  '\"add\"',\n",
      "  '\"serve\"',\n",
      "  '\"let\"',\n",
      "  '\"speak\"'],\n",
      " ['\"be\"',\n",
      "  '\"there\"',\n",
      "  '\"when\"',\n",
      "  '\"even\"',\n",
      "  '\"love\"',\n",
      "  '\"speak\"',\n",
      "  '\"so\"',\n",
      "  '\"let\"',\n",
      "  '\"include\"',\n",
      "  '\"get\"'],\n",
      " ['\"be\"',\n",
      "  '\"when\"',\n",
      "  '\"so\"',\n",
      "  '\"there\"',\n",
      "  '\"even\"',\n",
      "  '\"love\"',\n",
      "  '\"live\"',\n",
      "  '\"get\"',\n",
      "  '\"serve\"',\n",
      "  '\"grow\"'],\n",
      " ['\"be\"',\n",
      "  '\"there\"',\n",
      "  '\"even\"',\n",
      "  '\"when\"',\n",
      "  '\"let\"',\n",
      "  '\"love\"',\n",
      "  '\"serve\"',\n",
      "  '\"so\"',\n",
      "  '\"drop\"',\n",
      "  '\"add\"'],\n",
      " ['\"be\"',\n",
      "  '\"there\"',\n",
      "  '\"so\"',\n",
      "  '\"when\"',\n",
      "  '\"even\"',\n",
      "  '\"get\"',\n",
      "  '\"draw\"',\n",
      "  '\"manage\"',\n",
      "  '\"add\"',\n",
      "  '\"let\"'],\n",
      " ['\"be\"',\n",
      "  '\"so\"',\n",
      "  '\"even\"',\n",
      "  '\"there\"',\n",
      "  '\"when\"',\n",
      "  '\"let\"',\n",
      "  '\"still\"',\n",
      "  '\"get\"',\n",
      "  '\"keep\"',\n",
      "  '\"drop\"'],\n",
      " ['\"be\"',\n",
      "  '\"when\"',\n",
      "  '\"there\"',\n",
      "  '\"even\"',\n",
      "  '\"provide\"',\n",
      "  '\"so\"',\n",
      "  '\"let\"',\n",
      "  '\"keep\"',\n",
      "  '\"love\"',\n",
      "  '\"draw\"'],\n",
      " ['\"be\"',\n",
      "  '\"there\"',\n",
      "  '\"when\"',\n",
      "  '\"so\"',\n",
      "  '\"even\"',\n",
      "  '\"let\"',\n",
      "  '\"drop\"',\n",
      "  '\"add\"',\n",
      "  '\"provide\"',\n",
      "  '\"die\"'],\n",
      " ['\"be\"',\n",
      "  '\"when\"',\n",
      "  '\"so\"',\n",
      "  '\"there\"',\n",
      "  '\"love\"',\n",
      "  '\"even\"',\n",
      "  '\"drop\"',\n",
      "  '\"speak\"',\n",
      "  '\"keep\"',\n",
      "  '\"joke\"'],\n",
      " ['\"be\"',\n",
      "  '\"when\"',\n",
      "  '\"there\"',\n",
      "  '\"so\"',\n",
      "  '\"know\"',\n",
      "  '\"keep\"',\n",
      "  '\"live\"',\n",
      "  '\"let\"',\n",
      "  '\"here\"',\n",
      "  '\"get\"'],\n",
      " ['\"be\"',\n",
      "  '\"when\"',\n",
      "  '\"there\"',\n",
      "  '\"even\"',\n",
      "  '\"so\"',\n",
      "  '\"let\"',\n",
      "  '\"get\"',\n",
      "  '\"manage\"',\n",
      "  '\"promise\"',\n",
      "  '\"keep\"'],\n",
      " ['\"be\"',\n",
      "  '\"there\"',\n",
      "  '\"when\"',\n",
      "  '\"drop\"',\n",
      "  '\"draw\"',\n",
      "  '\"so\"',\n",
      "  '\"grow\"',\n",
      "  '\"let\"',\n",
      "  '\"deliver\"',\n",
      "  '\"add\"'],\n",
      " ['\"be\"',\n",
      "  '\"there\"',\n",
      "  '\"when\"',\n",
      "  '\"even\"',\n",
      "  '\"love\"',\n",
      "  '\"let\"',\n",
      "  '\"get\"',\n",
      "  '\"so\"',\n",
      "  '\"drop\"',\n",
      "  '\"happen\"'],\n",
      " ['\"be\"',\n",
      "  '\"there\"',\n",
      "  '\"so\"',\n",
      "  '\"love\"',\n",
      "  '\"when\"',\n",
      "  '\"get\"',\n",
      "  '\"even\"',\n",
      "  '\"add\"',\n",
      "  '\"speak\"',\n",
      "  '\"keep\"']]\n"
     ]
    }
   ],
   "source": [
    "pprint(all_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feade95",
   "metadata": {},
   "source": [
    "# Attempt 2 ... with pitchfork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "afa767c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from singernlp.ArtistReviewAnalyzer import ArtistReviewAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89a6192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "singerenv",
   "language": "python",
   "name": "singerenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
