{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f18c54e8",
   "metadata": {},
   "source": [
    "# ArtistReviewAnalyzer Class Tester\n",
    "\n",
    "Because I'm too lazy to run the file itself so I'm just forcefully doing it through jupyter lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19b38a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd6407",
   "metadata": {},
   "source": [
    "## helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd1724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "import re\n",
    "import unidecode as ud\n",
    "\n",
    "\n",
    "def clean_str(input_artist, cap_code=0):\n",
    "    \"\"\"\n",
    "    Takes in string, returns a unicode-friendly and stripped version of the string.\n",
    "    \"\"\"\n",
    "    return_artist_str = input_artist\n",
    "\n",
    "    # === REGEX REPLACE ===\n",
    "    repl_tuples = [(r'(^\\s+)|(\\s+$)', ''),  # whitespace at beg/end of string\n",
    "                   (r'\\s+', ' '),  # Remove double spaces\n",
    "                   (r'[\\n|\\r|\\t|\\0]+', ' ')\n",
    "                   ]\n",
    "    for ptn, repl_str in repl_tuples:\n",
    "        return_artist_str = re.sub(ptn, repl_str, return_artist_str)\n",
    "\n",
    "    # === UNICODE HANDLING ===\n",
    "    return_artist_str = ud.unidecode(return_artist_str)\n",
    "\n",
    "    if cap_code == -1:\n",
    "        return_artist_str = return_artist_str.lower()\n",
    "    elif cap_code == 1:\n",
    "        return_artist_str = return_artist_str.upper()\n",
    "\n",
    "    return return_artist_str\n",
    "\n",
    "\n",
    "def read_mard_json(input_filename):\n",
    "    \"\"\"\n",
    "    Takes in file name of JSON, returns a list of dictionaries in which\n",
    "    each element is a row with columns (as the keys).\n",
    "    \"\"\"\n",
    "    loaded_data_ = []\n",
    "    with open(input_filename, 'r') as file_:\n",
    "        loaded_string_ = file_.read()\n",
    "        loaded_data_ = [json.loads(s) for s in loaded_string_.split('\\n') if s is not None and len(s) > 0]\n",
    "        file_.close()\n",
    "    return loaded_data_\n",
    "\n",
    "\n",
    "def read_mard_json_as_df(input_filename):\n",
    "    \"\"\"\n",
    "    Takes in file name of JSON, returns a list of dictionaries in which\n",
    "    each element is a row with columns (as the keys).\n",
    "    \"\"\"\n",
    "    loaded_data_ = []\n",
    "    with open(input_filename, 'r') as file_:\n",
    "        loaded_string_ = file_.read()\n",
    "        loaded_data_ = [json.loads(s) for s in loaded_string_.split('\\n') if s is not None and len(s) > 0]\n",
    "        file_.close()\n",
    "    return pd.DataFrame(loaded_data_)\n",
    "\n",
    "\n",
    "def run_query_on_sqlite_db(input_query, input_filename):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns a Pandas DataFrame object containing the query results,\n",
    "    given the user's query and the filename for the sqlite database.\n",
    "\n",
    "    Input:\n",
    "     - input_query: string representation of the SQL query to run on the sqlite db\n",
    "     - input_filename: the file location of the sqlite database\n",
    "\n",
    "    \"\"\"\n",
    "    conn_ = sqlite3.connect(input_filename)\n",
    "    df_ = pd.read_sql_query(input_query, conn_)\n",
    "    conn_.close()\n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f534ec8",
   "metadata": {},
   "source": [
    "## ArtistReviewAnalyzer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de64df15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import spacy\n",
    "import itertools\n",
    "\n",
    "\n",
    "class ArtistReviewAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes artist reviews. Forms into useful data structures for NLP and topic modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Basic data\n",
    "        self.file_loc = None\n",
    "        self.raw = None\n",
    "        self.artists_list = None\n",
    "        self.all_tokens = None\n",
    "\n",
    "        # Stop-Words for tokenization\n",
    "        self.stop_words = stopwords.words('english')  # Basic stop words\n",
    "        self.update_stopwords(['album', 'music', 'cd', 'track', 'song', 'sound'])\n",
    "\n",
    "        # Vectorizers\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.count_vectorizer = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.count_matrix = None\n",
    "        self.__tokenizer = None\n",
    "        self.tokenized_reviews = None\n",
    "\n",
    "        # NLP tools\n",
    "        self.lda_model = None\n",
    "        self.bigrams = None\n",
    "        self.lemmatized_text = None\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])  # Spacy tool\n",
    "\n",
    "\n",
    "    def load_data(self, file_loc):\n",
    "        \"\"\"\n",
    "        Loads reviews data (JSON file) into a dictionary.\n",
    "        \"\"\"\n",
    "        # Save file location\n",
    "        self.file_loc = file_loc\n",
    "\n",
    "        with open(file_loc, 'r') as file:\n",
    "            d = json.load(file)\n",
    "            file.close()\n",
    "        self.raw = d  # Save raw dictionary\n",
    "\n",
    "        self.artists_list = list(self.raw.keys())  # Save artists list\n",
    "\n",
    "        self.build_count_vectorizer(2, 0.8)  # Build count vec so we have a tokenizer\n",
    "\n",
    "        # Handling tokenized list of reviews\n",
    "        self.tokenized_reviews = self.__build_reviews_list(do_tokenize=True)\n",
    "        self.remove_stopwords_from_tokenized_list()\n",
    "        self.set_all_tokens()  # Get tokenized corpus\n",
    "\n",
    "        return self.raw\n",
    "\n",
    "    def refresh(self):\n",
    "        self.build_count_vectorizer(2, 0.8)\n",
    "        self.tokenized_reviews = self.__build_reviews_list(do_tokenize=True)\n",
    "        self.remove_stopwords_from_tokenized_list()\n",
    "        self.set_all_tokens()\n",
    "\n",
    "    def build_count_vectorizer(self, min_df, max_df):\n",
    "        if self.count_vectorizer is not None:\n",
    "            print(\"WARNING: Count vectorizer has already been built.\")\n",
    "            return self.count_vectorizer\n",
    "        self.count_vectorizer = CountVectorizer(analyzer='word', stop_words=self.stop_words, min_df=min_df, max_df=max_df)\n",
    "        self.__tokenizer = self.count_vectorizer.build_tokenizer()\n",
    "        return self.count_vectorizer\n",
    "\n",
    "    def get_count_matrix(self):\n",
    "        if self.count_vectorizer is None:\n",
    "            return None\n",
    "\n",
    "        if self.count_matrix is None:\n",
    "            # TODO: self.raw is not truly reflective of the data since we clean it.\n",
    "            self.count_matrix = self.count_vectorizer.fit_transform(self.all_tokens)\n",
    "        return self.count_matrix\n",
    "\n",
    "    def build_tfidf_vectorizer(self, min_df, max_df):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words=self.stop_words, min_df=min_df, max_df=max_df)\n",
    "        return self.tfidf_vectorizer\n",
    "\n",
    "    def get_tfidf_matrix(self):\n",
    "        if self.tfidf_vectorizer is None:\n",
    "            return None\n",
    "\n",
    "        if self.tfidf_matrix is None:\n",
    "            # TODO: self.raw is not truly reflective of the data since we clean it.\n",
    "            self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.all_tokens)\n",
    "        return self.tfidf_matrix\n",
    "\n",
    "    def tokenize(self, input_string):\n",
    "        if self.__tokenizer is None:\n",
    "            print(\"WARNING: Tokenizer not set.\")\n",
    "            return None\n",
    "        return self.__tokenizer(input_string)\n",
    "\n",
    "    def update_stopwords(self, word_list):\n",
    "        self.stop_words.extend(word_list)\n",
    "        return None\n",
    "\n",
    "    def remove_stopwords_from_tokenized_list(self):\n",
    "        self.tokenized_reviews = \\\n",
    "            [[w for w in artist_review if w not in self.stop_words] for artist_review in self.tokenized_reviews]\n",
    "\n",
    "    def build_lda_model(self, num_topics=20):\n",
    "        self.bigrams = self.make_bigrams()\n",
    "        self.lemmatized_text = self.lemmatize()\n",
    "        id2word = corpora.Dictionary(self.lemmatized_text)\n",
    "        corpus = [id2word.doc2bow(w) for w in self.lemmatized_text]\n",
    "        self.lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "        return self.lda_model\n",
    "\n",
    "    def make_bigrams(self):\n",
    "        bg = gensim.models.Phrases(self.tokenized_reviews, min_count=5, threshold=100)\n",
    "        bg_mod = gensim.models.phrases.Phraser(bg)\n",
    "        return [bg_mod[d] for d in self.tokenized_reviews]\n",
    "\n",
    "    def lemmatize(self, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        texts_out = []\n",
    "        for review_words in self.tokenized_reviews:\n",
    "            joined_words = self.nlp(\" \".join(review_words))\n",
    "            texts_out.append([token.lemma_ for token in joined_words if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "\n",
    "    def set_all_tokens(self):\n",
    "        self.all_tokens = list(itertools.chain.from_iterable(self.tokenized_reviews))\n",
    "        return self.all_tokens\n",
    "\n",
    "    def get_all_tokens(self):\n",
    "        return self.all_tokens\n",
    "\n",
    "    def __build_reviews_list(self, do_tokenize=False):\n",
    "        \"\"\"\n",
    "        Builds a list of reviews for each artist.\n",
    "        \"\"\"\n",
    "        consolidated_reviews = []\n",
    "        for a in self.artists_list:\n",
    "            if do_tokenize:\n",
    "                consolidated_reviews.append(self.tokenize(clean_str(\" \".join(self.raw[a]))))\n",
    "            else:\n",
    "                consolidated_reviews.append(clean_str(\" \".join(self.raw[a])))\n",
    "        return consolidated_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66daf790",
   "metadata": {},
   "source": [
    "## Test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9c39ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_location = \"../data/processed/artist_reviews.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3c7a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ArtistReviewAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "155bc2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.load_data(json_file_location);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68db8ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.build_lda_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea59dd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*\"quot\" + 0.008*\"good\" + 0.007*\"make\" + 0.006*\"song\" + 0.006*\"well\" + '\n",
      "  '0.005*\"great\" + 0.005*\"even\" + 0.005*\"hear\" + 0.005*\"work\" + 0.004*\"first\"'),\n",
      " (1,\n",
      "  '0.008*\"get\" + 0.008*\"good\" + 0.008*\"great\" + 0.007*\"song\" + 0.006*\"well\" + '\n",
      "  '0.006*\"record\" + 0.005*\"first\" + 0.005*\"love\" + 0.005*\"make\" + '\n",
      "  '0.005*\"band\"'),\n",
      " (2,\n",
      "  '0.010*\"quot\" + 0.008*\"song\" + 0.007*\"great\" + 0.007*\"good\" + 0.006*\"well\" + '\n",
      "  '0.006*\"get\" + 0.006*\"record\" + 0.005*\"time\" + 0.005*\"make\" + 0.005*\"band\"'),\n",
      " (3,\n",
      "  '0.011*\"song\" + 0.009*\"good\" + 0.008*\"love\" + 0.008*\"make\" + 0.007*\"get\" + '\n",
      "  '0.006*\"time\" + 0.006*\"great\" + 0.005*\"cd\" + 0.005*\"well\" + 0.005*\"hear\"'),\n",
      " (4,\n",
      "  '0.008*\"make\" + 0.006*\"record\" + 0.006*\"band\" + 0.006*\"good\" + 0.005*\"well\" '\n",
      "  '+ 0.005*\"go\" + 0.005*\"time\" + 0.004*\"get\" + 0.004*\"song\" + 0.004*\"first\"'),\n",
      " (5,\n",
      "  '0.008*\"make\" + 0.007*\"song\" + 0.006*\"get\" + 0.006*\"well\" + 0.006*\"good\" + '\n",
      "  '0.006*\"quot\" + 0.006*\"band\" + 0.006*\"time\" + 0.005*\"record\" + 0.004*\"say\"'),\n",
      " (6,\n",
      "  '0.009*\"quot\" + 0.009*\"song\" + 0.007*\"good\" + 0.006*\"cd\" + 0.006*\"get\" + '\n",
      "  '0.006*\"great\" + 0.006*\"make\" + 0.005*\"time\" + 0.005*\"hear\" + 0.004*\"even\"'),\n",
      " (7,\n",
      "  '0.007*\"good\" + 0.007*\"song\" + 0.006*\"get\" + 0.006*\"well\" + 0.006*\"even\" + '\n",
      "  '0.005*\"great\" + 0.005*\"band\" + 0.005*\"time\" + 0.005*\"work\" + 0.004*\"come\"'),\n",
      " (8,\n",
      "  '0.007*\"good\" + 0.006*\"get\" + 0.006*\"record\" + 0.005*\"song\" + 0.005*\"make\" + '\n",
      "  '0.005*\"band\" + 0.005*\"play\" + 0.005*\"hear\" + 0.005*\"great\" + 0.004*\"cd\"'),\n",
      " (9,\n",
      "  '0.009*\"song\" + 0.007*\"good\" + 0.006*\"great\" + 0.006*\"record\" + 0.006*\"make\" '\n",
      "  '+ 0.006*\"get\" + 0.005*\"well\" + 0.005*\"first\" + 0.005*\"cd\" + 0.005*\"love\"'),\n",
      " (10,\n",
      "  '0.008*\"make\" + 0.007*\"band\" + 0.006*\"quot\" + 0.006*\"good\" + 0.006*\"time\" + '\n",
      "  '0.006*\"get\" + 0.005*\"record\" + 0.005*\"song\" + 0.005*\"much\" + 0.004*\"cd\"'),\n",
      " (11,\n",
      "  '0.008*\"quot\" + 0.006*\"good\" + 0.006*\"great\" + 0.005*\"get\" + 0.005*\"time\" + '\n",
      "  '0.005*\"record\" + 0.005*\"song\" + 0.005*\"make\" + 0.005*\"hear\" + 0.005*\"well\"'),\n",
      " (12,\n",
      "  '0.010*\"song\" + 0.008*\"good\" + 0.008*\"make\" + 0.008*\"quot\" + 0.007*\"well\" + '\n",
      "  '0.007*\"get\" + 0.007*\"great\" + 0.007*\"record\" + 0.005*\"go\" + 0.005*\"time\"'),\n",
      " (13,\n",
      "  '0.009*\"good\" + 0.009*\"make\" + 0.007*\"song\" + 0.007*\"great\" + 0.006*\"love\" + '\n",
      "  '0.005*\"even\" + 0.005*\"quot\" + 0.005*\"get\" + 0.005*\"time\" + 0.005*\"go\"'),\n",
      " (14,\n",
      "  '0.006*\"good\" + 0.006*\"get\" + 0.006*\"make\" + 0.006*\"great\" + 0.005*\"record\" '\n",
      "  '+ 0.005*\"band\" + 0.005*\"well\" + 0.005*\"listen\" + 0.005*\"song\" + '\n",
      "  '0.005*\"year\"'),\n",
      " (15,\n",
      "  '0.007*\"song\" + 0.006*\"make\" + 0.006*\"record\" + 0.006*\"well\" + 0.006*\"good\" '\n",
      "  '+ 0.005*\"get\" + 0.005*\"release\" + 0.005*\"time\" + 0.004*\"love\" + '\n",
      "  '0.004*\"great\"'),\n",
      " (16,\n",
      "  '0.009*\"song\" + 0.008*\"well\" + 0.007*\"love\" + 0.006*\"good\" + 0.006*\"band\" + '\n",
      "  '0.005*\"get\" + 0.005*\"quot\" + 0.005*\"great\" + 0.005*\"time\" + 0.005*\"come\"'),\n",
      " (17,\n",
      "  '0.010*\"good\" + 0.009*\"band\" + 0.007*\"song\" + 0.007*\"make\" + 0.006*\"great\" + '\n",
      "  '0.006*\"record\" + 0.006*\"get\" + 0.005*\"year\" + 0.005*\"time\" + 0.005*\"even\"'),\n",
      " (18,\n",
      "  '0.008*\"good\" + 0.008*\"song\" + 0.007*\"well\" + 0.006*\"play\" + 0.006*\"time\" + '\n",
      "  '0.006*\"cd\" + 0.005*\"great\" + 0.005*\"go\" + 0.005*\"hear\" + 0.005*\"get\"'),\n",
      " (19,\n",
      "  '0.008*\"good\" + 0.007*\"record\" + 0.007*\"get\" + 0.006*\"quot\" + 0.006*\"great\" '\n",
      "  '+ 0.006*\"first\" + 0.006*\"make\" + 0.005*\"even\" + 0.005*\"well\" + '\n",
      "  '0.005*\"song\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(analyzer.lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0095f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ArtistReviewAnalyzer()\n",
    "analyzer.update_stopwords(['good', 'band', 'record', 'make', 'great', 'hear'])\n",
    "analyzer.load_data(json_file_location);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6c3c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.build_lda_model(num_topics=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c991fbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10,\n",
      "  '0.006*\"even\" + 0.005*\"song\" + 0.005*\"get\" + 0.005*\"time\" + 0.004*\"well\" + '\n",
      "  '0.004*\"work\" + 0.004*\"make\" + 0.004*\"sound\" + 0.004*\"come\" + 0.004*\"go\"'),\n",
      " (22,\n",
      "  '0.008*\"get\" + 0.006*\"time\" + 0.006*\"song\" + 0.006*\"go\" + 0.005*\"quot\" + '\n",
      "  '0.005*\"first\" + 0.005*\"well\" + 0.004*\"love\" + 0.004*\"make\" + 0.004*\"good\"'),\n",
      " (39,\n",
      "  '0.007*\"song\" + 0.006*\"get\" + 0.005*\"come\" + 0.005*\"time\" + 0.005*\"new\" + '\n",
      "  '0.004*\"play\" + 0.004*\"first\" + 0.004*\"love\" + 0.004*\"work\" + 0.004*\"good\"'),\n",
      " (1,\n",
      "  '0.010*\"song\" + 0.008*\"quot\" + 0.007*\"well\" + 0.006*\"time\" + 0.006*\"get\" + '\n",
      "  '0.005*\"go\" + 0.005*\"love\" + 0.005*\"first\" + 0.004*\"come\" + 0.004*\"year\"'),\n",
      " (38,\n",
      "  '0.008*\"song\" + 0.006*\"go\" + 0.005*\"even\" + 0.005*\"first\" + 0.005*\"make\" + '\n",
      "  '0.004*\"well\" + 0.004*\"work\" + 0.004*\"get\" + 0.004*\"be\" + 0.004*\"voice\"'),\n",
      " (11,\n",
      "  '0.008*\"get\" + 0.007*\"song\" + 0.006*\"love\" + 0.006*\"quot\" + 0.005*\"make\" + '\n",
      "  '0.005*\"well\" + 0.005*\"time\" + 0.004*\"find\" + 0.004*\"go\" + 0.004*\"work\"'),\n",
      " (12,\n",
      "  '0.008*\"quot\" + 0.007*\"song\" + 0.007*\"get\" + 0.006*\"well\" + 0.005*\"love\" + '\n",
      "  '0.005*\"year\" + 0.005*\"come\" + 0.005*\"work\" + 0.005*\"go\" + 0.004*\"time\"'),\n",
      " (21,\n",
      "  '0.007*\"quot\" + 0.006*\"song\" + 0.006*\"cd\" + 0.006*\"time\" + 0.005*\"well\" + '\n",
      "  '0.005*\"make\" + 0.005*\"get\" + 0.005*\"love\" + 0.005*\"listen\" + 0.005*\"first\"'),\n",
      " (0,\n",
      "  '0.010*\"quot\" + 0.007*\"song\" + 0.005*\"get\" + 0.005*\"well\" + 0.005*\"love\" + '\n",
      "  '0.005*\"much\" + 0.005*\"work\" + 0.004*\"even\" + 0.004*\"good\" + 0.004*\"make\"'),\n",
      " (8,\n",
      "  '0.007*\"song\" + 0.007*\"well\" + 0.005*\"time\" + 0.005*\"get\" + 0.005*\"play\" + '\n",
      "  '0.004*\"make\" + 0.004*\"year\" + 0.004*\"take\" + 0.004*\"sound\" + 0.004*\"say\"'),\n",
      " (18,\n",
      "  '0.011*\"song\" + 0.006*\"get\" + 0.005*\"play\" + 0.005*\"well\" + 0.005*\"time\" + '\n",
      "  '0.005*\"new\" + 0.005*\"first\" + 0.005*\"come\" + 0.005*\"listen\" + 0.004*\"quot\"'),\n",
      " (24,\n",
      "  '0.007*\"song\" + 0.006*\"get\" + 0.005*\"well\" + 0.005*\"cd\" + 0.005*\"first\" + '\n",
      "  '0.005*\"year\" + 0.005*\"love\" + 0.004*\"make\" + 0.004*\"even\" + 0.004*\"work\"'),\n",
      " (33,\n",
      "  '0.008*\"song\" + 0.007*\"love\" + 0.007*\"quot\" + 0.006*\"even\" + 0.005*\"get\" + '\n",
      "  '0.005*\"cd\" + 0.005*\"well\" + 0.004*\"good\" + 0.004*\"come\" + 0.004*\"first\"'),\n",
      " (37,\n",
      "  '0.009*\"song\" + 0.008*\"well\" + 0.007*\"even\" + 0.007*\"make\" + 0.006*\"get\" + '\n",
      "  '0.006*\"love\" + 0.006*\"listen\" + 0.006*\"time\" + 0.005*\"first\" + 0.005*\"cd\"'),\n",
      " (19,\n",
      "  '0.007*\"well\" + 0.005*\"time\" + 0.005*\"recording\" + 0.005*\"make\" + '\n",
      "  '0.004*\"get\" + 0.004*\"quot\" + 0.004*\"first\" + 0.004*\"record\" + 0.004*\"also\" '\n",
      "  '+ 0.004*\"even\"'),\n",
      " (35,\n",
      "  '0.008*\"well\" + 0.007*\"quot\" + 0.007*\"song\" + 0.006*\"recording\" + 0.006*\"cd\" '\n",
      "  '+ 0.005*\"time\" + 0.005*\"get\" + 0.005*\"year\" + 0.005*\"work\" + 0.004*\"know\"'),\n",
      " (9,\n",
      "  '0.008*\"song\" + 0.007*\"quot\" + 0.006*\"first\" + 0.005*\"go\" + 0.005*\"get\" + '\n",
      "  '0.005*\"well\" + 0.004*\"much\" + 0.004*\"work\" + 0.004*\"come\" + 0.004*\"even\"'),\n",
      " (5,\n",
      "  '0.010*\"song\" + 0.008*\"get\" + 0.007*\"well\" + 0.006*\"time\" + 0.006*\"quot\" + '\n",
      "  '0.006*\"cd\" + 0.005*\"love\" + 0.005*\"first\" + 0.005*\"find\" + 0.005*\"listen\"'),\n",
      " (2,\n",
      "  '0.008*\"get\" + 0.007*\"song\" + 0.007*\"cd\" + 0.007*\"love\" + 0.006*\"time\" + '\n",
      "  '0.006*\"well\" + 0.005*\"go\" + 0.005*\"work\" + 0.005*\"make\" + 0.004*\"year\"'),\n",
      " (13,\n",
      "  '0.008*\"song\" + 0.007*\"quot\" + 0.007*\"get\" + 0.007*\"well\" + 0.005*\"love\" + '\n",
      "  '0.005*\"also\" + 0.005*\"cd\" + 0.005*\"release\" + 0.005*\"time\" + 0.005*\"even\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(analyzer.lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4cbd68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Count vectorizer has already been built.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.8, max_features=None,\n",
       "                min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 = ArtistReviewAnalyzer()\n",
    "a2.load_data(json_file_location)\n",
    "a2.build_count_vectorizer(min_df=2,max_df=0.8)\n",
    "a2.build_tfidf_vectorizer(min_df=2,max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97512463",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2_tfidf = a2.get_tfidf_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "102772e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "871.0782534277441"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a2_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d08be084",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2_count = a2.get_count_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd7b1752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1008"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(a2_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "191e79bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaron', 'adrian', 'aguilar', 'alberto', 'alejandro', 'alex', 'alexander', 'alexandra', 'alexandre', 'ali', 'alice', 'america', 'ana', 'anderson', 'andre', 'andrew', 'andrews', 'andy', 'antonio', 'band', 'banda', 'bang', 'beach', 'bear', 'beck', 'belinda', 'ben', 'benjamin', 'benny', 'big', 'bill', 'billy', 'black', 'blanca', 'blind', 'blue', 'blues', 'bob', 'bobby', 'boy', 'brian', 'bronco', 'brown', 'bruce', 'carl', 'carlos', 'cartel', 'cecilia', 'chain', 'charlie', 'chico', 'chris', 'christina', 'church', 'circle', 'city', 'claudio', 'club', 'cold', 'cole', 'collective', 'conjunto', 'country', 'crazy', 'cruz', 'culture', 'da', 'daddy', 'dan', 'daniel', 'dave', 'david', 'day', 'de', 'dead', 'death', 'del', 'di', 'die', 'diego', 'dj', 'dog', 'donna', 'dr', 'dragon', 'duncan', 'durango', 'earth', 'eddie', 'edgar', 'el', 'electric', 'elliott', 'elvis', 'emilio', 'english', 'enrique', 'eric', 'erik', 'faith', 'fernandez', 'fernando', 'ferreira', 'fiona', 'francisco', 'frankie', 'franz', 'future', 'gabriel', 'gang', 'garcia', 'garden', 'gary', 'georg', 'george', 'gilberto', 'gomez', 'gonzalez', 'grande', 'green', 'grupo', 'gustav', 'gustavo', 'guzman', 'hall', 'hernandez', 'high', 'hot', 'howard', 'iglesias', 'ismael', 'james', 'jane', 'jason', 'jay', 'jean', 'jeff', 'jerry', 'jesus', 'jim', 'jimmy', 'joan', 'joao', 'joaquin', 'joe', 'joey', 'johann', 'john', 'johnny', 'jon', 'jones', 'jorge', 'jose', 'juan', 'jules', 'julian', 'julio', 'justin', 'kamen', 'karl', 'kelly', 'kid', 'kids', 'king', 'kings', 'la', 'lady', 'lalo', 'lara', 'last', 'laura', 'lee', 'leo', 'leon', 'lewis', 'lil', 'little', 'living', 'lloyd', 'local', 'lopez', 'los', 'lucia', 'luis', 'luiz', 'mac', 'man', 'maranatha', 'marco', 'marcos', 'maria', 'mark', 'marley', 'martha', 'martin', 'marvin', 'mary', 'masta', 'matt', 'mccartney', 'medicine', 'melanie', 'michael', 'midnight', 'miguel', 'mike', 'miller', 'montanez', 'moore', 'moreno', 'morrison', 'musical', 'natalia', 'natalie', 'nick', 'nicole', 'night', 'nine', 'nino', 'noel', 'norte', 'orchestra', 'oscar', 'pablo', 'passion', 'pat', 'patti', 'paul', 'paz', 'pedro', 'perry', 'pete', 'peter', 'philharmonic', 'philharmoniker', 'phillips', 'pink', 'pires', 'plan', 'pop', 'powell', 'rae', 'raul', 'renee', 'reyes', 'ricardo', 'rich', 'richard', 'richie', 'rio', 'rivera', 'robert', 'roberto', 'rocio', 'rock', 'rodriguez', 'roger', 'romeo', 'romero', 'roots', 'rosario', 'roy', 'ruben', 'ruiz', 'ryan', 'san', 'santa', 'santiago', 'santos', 'savage', 'scott', 'sebastian', 'sergio', 'sheena', 'sierra', 'silva', 'simon', 'simpson', 'sinaloa', 'singh', 'sir', 'sky', 'slim', 'smith', 'social', 'solis', 'sonora', 'soul', 'sounds', 'st', 'stan', 'star', 'stars', 'steve', 'steven', 'stone', 'strauss', 'strings', 'symphony', 'thug', 'tito', 'tom', 'tommy', 'tony', 'toro', 'torres', 'trevor', 'tropical', 'van', 'vargas', 'vaughan', 'vladimir', 'wainwright', 'walter', 'war', 'waters', 'watson', 'wax', 'wayne', 'webber', 'williams', 'willie', 'wilson', 'wizard', 'yeah', 'york', 'young', 'youth', 'zoe']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2006f829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaron', 'adrian', 'aguilar', 'alberto', 'alejandro', 'alex', 'alexander', 'alexandra', 'alexandre', 'ali', 'alice', 'america', 'ana', 'anderson', 'andre', 'andrew', 'andrews', 'andy', 'antonio', 'band', 'banda', 'bang', 'beach', 'bear', 'beck', 'belinda', 'ben', 'benjamin', 'benny', 'big', 'bill', 'billy', 'black', 'blanca', 'blind', 'blue', 'blues', 'bob', 'bobby', 'boy', 'brian', 'bronco', 'brown', 'bruce', 'carl', 'carlos', 'cartel', 'cecilia', 'chain', 'charlie', 'chico', 'chris', 'christina', 'church', 'circle', 'city', 'claudio', 'club', 'cold', 'cole', 'collective', 'conjunto', 'country', 'crazy', 'cruz', 'culture', 'da', 'daddy', 'dan', 'daniel', 'dave', 'david', 'day', 'de', 'dead', 'death', 'del', 'di', 'die', 'diego', 'dj', 'dog', 'donna', 'dr', 'dragon', 'duncan', 'durango', 'earth', 'eddie', 'edgar', 'el', 'electric', 'elliott', 'elvis', 'emilio', 'english', 'enrique', 'eric', 'erik', 'faith', 'fernandez', 'fernando', 'ferreira', 'fiona', 'francisco', 'frankie', 'franz', 'future', 'gabriel', 'gang', 'garcia', 'garden', 'gary', 'georg', 'george', 'gilberto', 'gomez', 'gonzalez', 'grande', 'green', 'grupo', 'gustav', 'gustavo', 'guzman', 'hall', 'hernandez', 'high', 'hot', 'howard', 'iglesias', 'ismael', 'james', 'jane', 'jason', 'jay', 'jean', 'jeff', 'jerry', 'jesus', 'jim', 'jimmy', 'joan', 'joao', 'joaquin', 'joe', 'joey', 'johann', 'john', 'johnny', 'jon', 'jones', 'jorge', 'jose', 'juan', 'jules', 'julian', 'julio', 'justin', 'kamen', 'karl', 'kelly', 'kid', 'kids', 'king', 'kings', 'la', 'lady', 'lalo', 'lara', 'last', 'laura', 'lee', 'leo', 'leon', 'lewis', 'lil', 'little', 'living', 'lloyd', 'local', 'lopez', 'los', 'lucia', 'luis', 'luiz', 'mac', 'man', 'maranatha', 'marco', 'marcos', 'maria', 'mark', 'marley', 'martha', 'martin', 'marvin', 'mary', 'masta', 'matt', 'mccartney', 'medicine', 'melanie', 'michael', 'midnight', 'miguel', 'mike', 'miller', 'montanez', 'moore', 'moreno', 'morrison', 'musical', 'natalia', 'natalie', 'nick', 'nicole', 'night', 'nine', 'nino', 'noel', 'norte', 'orchestra', 'oscar', 'pablo', 'passion', 'pat', 'patti', 'paul', 'paz', 'pedro', 'perry', 'pete', 'peter', 'philharmonic', 'philharmoniker', 'phillips', 'pink', 'pires', 'plan', 'pop', 'powell', 'rae', 'raul', 'renee', 'reyes', 'ricardo', 'rich', 'richard', 'richie', 'rio', 'rivera', 'robert', 'roberto', 'rocio', 'rock', 'rodriguez', 'roger', 'romeo', 'romero', 'roots', 'rosario', 'roy', 'ruben', 'ruiz', 'ryan', 'san', 'santa', 'santiago', 'santos', 'savage', 'scott', 'sebastian', 'sergio', 'sheena', 'sierra', 'silva', 'simon', 'simpson', 'sinaloa', 'singh', 'sir', 'sky', 'slim', 'smith', 'social', 'solis', 'sonora', 'soul', 'sounds', 'st', 'stan', 'star', 'stars', 'steve', 'steven', 'stone', 'strauss', 'strings', 'symphony', 'thug', 'tito', 'tom', 'tommy', 'tony', 'toro', 'torres', 'trevor', 'tropical', 'van', 'vargas', 'vaughan', 'vladimir', 'wainwright', 'walter', 'war', 'waters', 'watson', 'wax', 'wayne', 'webber', 'williams', 'willie', 'wilson', 'wizard', 'yeah', 'york', 'young', 'youth', 'zoe']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e21bfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329\n"
     ]
    }
   ],
   "source": [
    "print(len(a2.count_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "baaf7650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1784, 329)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d4d3dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_freq = np.array(np.sum(a2_count, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1500ef85",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_to_w = a2.count_vectorizer.get_feature_names()\n",
    "w_to_i = {w:i for i,w in enumerate(a2.count_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7c9e4739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'banda'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_to_w[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66332b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Count vectorizer has already been built.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.8, max_features=None,\n",
       "                min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a3 = ArtistReviewAnalyzer()\n",
    "a3.load_data(json_file_location)\n",
    "a3.build_count_vectorizer(min_df=2,max_df=0.8)\n",
    "a3.build_tfidf_vectorizer(min_df=2,max_df=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "10a5a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3_tfidf = a3.get_tfidf_matrix()\n",
    "a3_count = a3.get_count_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "82641479",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3.count_vectorizer.get_feature_names();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dcba721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(input_analyzer, n):\n",
    "    top_n = []\n",
    "    feats = input_analyzer.count_vectorizer.get_feature_names()\n",
    "    w_counts = np.array(np.sum(input_analyzer.get_count_matrix(), axis=0))[0]\n",
    "    sorted_indices = np.argsort(w_counts)[::-1][:n]\n",
    "    \n",
    "    for i in sorted_indices:\n",
    "        top_n.append((feats[i], w_counts[i]))\n",
    "    return top_n\n",
    "\n",
    "def get_top_n_words(input_analyzer, n):\n",
    "    top_n = []\n",
    "    feats = input_analyzer.count_vectorizer.get_feature_names()\n",
    "    w_counts = np.array(np.sum(input_analyzer.get_count_matrix(), axis=0))[0]\n",
    "    sorted_indices = np.argsort(w_counts)[::-1][:n]\n",
    "    \n",
    "    for i in sorted_indices:\n",
    "        top_n.append(feats[i])\n",
    "    return top_n\n",
    "\n",
    "def get_bottom_n_words(input_analyzer, n):\n",
    "    top_n = []\n",
    "    feats = input_analyzer.count_vectorizer.get_feature_names()\n",
    "    w_counts = np.array(np.sum(input_analyzer.get_count_matrix(), axis=0))[0]\n",
    "    sorted_indices = np.argsort(w_counts)[:n]\n",
    "    \n",
    "    for i in sorted_indices:\n",
    "        top_n.append(feats[i])\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5eec582e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('like', 13878),\n",
      " ('one', 12812),\n",
      " ('songs', 9463),\n",
      " ('quot', 7580),\n",
      " ('great', 6798),\n",
      " ('love', 6094),\n",
      " ('band', 5880),\n",
      " ('good', 5873),\n",
      " ('first', 5803),\n",
      " ('time', 5604),\n",
      " ('best', 5572),\n",
      " ('even', 5272),\n",
      " ('de', 4862),\n",
      " ('new', 4819),\n",
      " ('much', 4652),\n",
      " ('would', 4491),\n",
      " ('well', 4426),\n",
      " ('get', 4336),\n",
      " ('also', 4168),\n",
      " ('still', 4035),\n",
      " ('two', 3994),\n",
      " ('rock', 3792),\n",
      " ('tracks', 3716),\n",
      " ('really', 3672),\n",
      " ('way', 3529),\n",
      " ('record', 3344),\n",
      " ('pop', 3328),\n",
      " ('years', 3326),\n",
      " ('voice', 3256),\n",
      " ('could', 3192),\n",
      " ('many', 3064),\n",
      " ('recording', 3057),\n",
      " ('work', 3052),\n",
      " ('back', 3039),\n",
      " ('never', 3024),\n",
      " ('sounds', 2974),\n",
      " ('la', 2930),\n",
      " ('live', 2918),\n",
      " ('better', 2876),\n",
      " ('make', 2762),\n",
      " ('albums', 2677),\n",
      " ('though', 2553),\n",
      " ('ever', 2522),\n",
      " ('guitar', 2521),\n",
      " ('little', 2511),\n",
      " ('version', 2472),\n",
      " ('heard', 2470),\n",
      " ('know', 2464),\n",
      " ('listen', 2457),\n",
      " ('every', 2443)]\n"
     ]
    }
   ],
   "source": [
    "pprint(get_top_n(a3, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e8978edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3.update_stopwords(get_top_n_words(a3, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "df9a5ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Count vectorizer has already been built.\n"
     ]
    }
   ],
   "source": [
    "a3.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "43417e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3.build_lda_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "205b9019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.005*\"come\" + 0.004*\"take\" + 0.004*\"play\" + 0.004*\"say\" + 0.004*\"make\" + '\n",
      "  '0.004*\"go\" + 0.004*\"give\" + 0.004*\"get\" + 0.003*\"feel\" + 0.003*\"seem\"'),\n",
      " (1,\n",
      "  '0.004*\"be\" + 0.004*\"come\" + 0.003*\"release\" + 0.003*\"think\" + 0.003*\"make\" '\n",
      "  '+ 0.003*\"feel\" + 0.003*\"cd\" + 0.003*\"sing\" + 0.003*\"go\" + 0.003*\"find\"'),\n",
      " (2,\n",
      "  '0.007*\"go\" + 0.006*\"cd\" + 0.006*\"come\" + 0.005*\"make\" + 0.005*\"play\" + '\n",
      "  '0.004*\"find\" + 0.004*\"say\" + 0.004*\"fan\" + 0.003*\"want\" + 0.003*\"single\"'),\n",
      " (3,\n",
      "  '0.006*\"make\" + 0.006*\"come\" + 0.006*\"go\" + 0.005*\"feel\" + 0.005*\"play\" + '\n",
      "  '0.005*\"take\" + 0.004*\"cd\" + 0.004*\"get\" + 0.004*\"say\" + 0.004*\"be\"'),\n",
      " (4,\n",
      "  '0.005*\"make\" + 0.005*\"play\" + 0.005*\"cd\" + 0.004*\"go\" + 0.004*\"say\" + '\n",
      "  '0.004*\"get\" + 0.003*\"feel\" + 0.003*\"release\" + 0.003*\"see\" + 0.003*\"come\"'),\n",
      " (5,\n",
      "  '0.006*\"make\" + 0.005*\"go\" + 0.004*\"take\" + 0.004*\"release\" + 0.004*\"say\" + '\n",
      "  '0.004*\"get\" + 0.004*\"come\" + 0.004*\"give\" + 0.003*\"feel\" + 0.003*\"cd\"'),\n",
      " (6,\n",
      "  '0.005*\"make\" + 0.004*\"cd\" + 0.004*\"come\" + 0.004*\"be\" + 0.004*\"go\" + '\n",
      "  '0.004*\"say\" + 0.003*\"play\" + 0.003*\"think\" + 0.003*\"get\" + 0.003*\"feel\"'),\n",
      " (7,\n",
      "  '0.005*\"make\" + 0.004*\"come\" + 0.004*\"get\" + 0.004*\"go\" + 0.004*\"take\" + '\n",
      "  '0.003*\"release\" + 0.003*\"que\" + 0.003*\"play\" + 0.003*\"find\" + 0.003*\"cd\"'),\n",
      " (8,\n",
      "  '0.006*\"cd\" + 0.005*\"make\" + 0.005*\"find\" + 0.005*\"come\" + 0.004*\"go\" + '\n",
      "  '0.004*\"be\" + 0.004*\"feel\" + 0.003*\"fan\" + 0.003*\"get\" + 0.003*\"release\"'),\n",
      " (9,\n",
      "  '0.005*\"be\" + 0.005*\"go\" + 0.005*\"cd\" + 0.005*\"hit\" + 0.005*\"make\" + '\n",
      "  '0.005*\"release\" + 0.004*\"come\" + 0.004*\"get\" + 0.004*\"take\" + '\n",
      "  '0.004*\"single\"'),\n",
      " (10,\n",
      "  '0.006*\"cd\" + 0.005*\"make\" + 0.004*\"get\" + 0.004*\"go\" + 0.004*\"find\" + '\n",
      "  '0.004*\"play\" + 0.003*\"take\" + 0.003*\"come\" + 0.003*\"say\" + 0.003*\"vocal\"'),\n",
      " (11,\n",
      "  '0.005*\"go\" + 0.004*\"make\" + 0.004*\"feel\" + 0.004*\"play\" + 0.004*\"give\" + '\n",
      "  '0.004*\"movie\" + 0.003*\"find\" + 0.003*\"take\" + 0.003*\"think\" + 0.003*\"get\"'),\n",
      " (12,\n",
      "  '0.006*\"cd\" + 0.005*\"go\" + 0.004*\"play\" + 0.004*\"say\" + 0.004*\"come\" + '\n",
      "  '0.004*\"get\" + 0.004*\"take\" + 0.004*\"make\" + 0.003*\"release\" + 0.003*\"fan\"'),\n",
      " (13,\n",
      "  '0.006*\"make\" + 0.005*\"come\" + 0.005*\"cd\" + 0.005*\"release\" + 0.004*\"take\" + '\n",
      "  '0.004*\"find\" + 0.004*\"say\" + 0.004*\"play\" + 0.003*\"go\" + 0.003*\"vocal\"'),\n",
      " (14,\n",
      "  '0.005*\"come\" + 0.004*\"make\" + 0.004*\"cd\" + 0.004*\"play\" + 0.004*\"take\" + '\n",
      "  '0.004*\"release\" + 0.003*\"go\" + 0.003*\"give\" + 0.003*\"performance\" + '\n",
      "  '0.003*\"find\"'),\n",
      " (15,\n",
      "  '0.005*\"make\" + 0.004*\"feel\" + 0.004*\"come\" + 0.004*\"cd\" + 0.003*\"play\" + '\n",
      "  '0.003*\"find\" + 0.003*\"release\" + 0.003*\"take\" + 0.003*\"say\" + 0.003*\"give\"'),\n",
      " (16,\n",
      "  '0.004*\"make\" + 0.004*\"cd\" + 0.004*\"take\" + 0.004*\"come\" + 0.003*\"go\" + '\n",
      "  '0.003*\"fan\" + 0.003*\"give\" + 0.003*\"say\" + 0.003*\"feel\" + 0.003*\"que\"'),\n",
      " (17,\n",
      "  '0.004*\"feel\" + 0.004*\"cd\" + 0.004*\"make\" + 0.004*\"give\" + 0.003*\"take\" + '\n",
      "  '0.003*\"find\" + 0.003*\"seem\" + 0.003*\"get\" + 0.003*\"go\" + 0.003*\"play\"'),\n",
      " (18,\n",
      "  '0.006*\"cd\" + 0.005*\"make\" + 0.004*\"release\" + 0.004*\"come\" + 0.004*\"play\" + '\n",
      "  '0.004*\"go\" + 0.004*\"feel\" + 0.003*\"be\" + 0.003*\"think\" + 0.003*\"fan\"'),\n",
      " (19,\n",
      "  '0.005*\"cd\" + 0.005*\"go\" + 0.005*\"make\" + 0.004*\"play\" + 0.004*\"take\" + '\n",
      "  '0.003*\"feel\" + 0.003*\"say\" + 0.003*\"buy\" + 0.003*\"get\" + 0.003*\"come\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(a3.lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5bc1234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3.update_stopwords(get_top_n_words(a3, int(0.3*len(a3.count_vectorizer.get_feature_names()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "83d6491f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Count vectorizer has already been built.\n"
     ]
    }
   ],
   "source": [
    "a3.refresh()\n",
    "a3.build_lda_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4279e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8c05b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_data.pk\", \"wb\") as f:\n",
    "    pickle.dump(a3.lda_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c27a1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lda = pickle.load(open(\"test_data.pk\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e63e5888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.019*\"cd\" + 0.008*\"be\" + 0.006*\"here\" + 0.006*\"there\" + 0.005*\"have\" + '\n",
      "  '0.004*\"now\" + 0.004*\"so\" + 0.004*\"do\" + 0.004*\"even\" + 0.003*\"make\"'),\n",
      " (1,\n",
      "  '0.020*\"be\" + 0.014*\"cd\" + 0.009*\"there\" + 0.006*\"so\" + 0.004*\"when\" + '\n",
      "  '0.004*\"have\" + 0.003*\"hit\" + 0.003*\"do\" + 0.003*\"let\" + 0.003*\"go\"'),\n",
      " (2,\n",
      "  '0.018*\"be\" + 0.010*\"cd\" + 0.006*\"there\" + 0.005*\"when\" + 0.005*\"where\" + '\n",
      "  '0.004*\"even\" + 0.004*\"so\" + 0.003*\"have\" + 0.003*\"just\" + 0.003*\"find\"'),\n",
      " (3,\n",
      "  '0.015*\"there\" + 0.013*\"cd\" + 0.009*\"be\" + 0.007*\"when\" + 0.005*\"so\" + '\n",
      "  '0.004*\"get\" + 0.004*\"even\" + 0.004*\"now\" + 0.003*\"have\" + 0.003*\"make\"'),\n",
      " (4,\n",
      "  '0.012*\"cd\" + 0.010*\"there\" + 0.008*\"be\" + 0.005*\"even\" + 0.004*\"so\" + '\n",
      "  '0.004*\"when\" + 0.003*\"come\" + 0.003*\"still\" + 0.003*\"now\" + 0.003*\"have\"'),\n",
      " (5,\n",
      "  '0.020*\"be\" + 0.015*\"cd\" + 0.015*\"there\" + 0.007*\"when\" + 0.005*\"go\" + '\n",
      "  '0.005*\"so\" + 0.004*\"here\" + 0.004*\"even\" + 0.004*\"live\" + 0.003*\"however\"'),\n",
      " (6,\n",
      "  '0.013*\"be\" + 0.009*\"cd\" + 0.006*\"there\" + 0.004*\"when\" + 0.004*\"so\" + '\n",
      "  '0.003*\"come\" + 0.003*\"just\" + 0.003*\"even\" + 0.003*\"here\" + 0.003*\"love\"'),\n",
      " (7,\n",
      "  '0.028*\"be\" + 0.009*\"cd\" + 0.009*\"there\" + 0.007*\"go\" + 0.005*\"do\" + '\n",
      "  '0.005*\"only\" + 0.005*\"so\" + 0.004*\"make\" + 0.004*\"love\" + 0.004*\"even\"'),\n",
      " (8,\n",
      "  '0.015*\"cd\" + 0.008*\"so\" + 0.008*\"there\" + 0.008*\"when\" + 0.006*\"be\" + '\n",
      "  '0.004*\"do\" + 0.004*\"even\" + 0.003*\"take\" + 0.003*\"die\" + 0.003*\"how\"'),\n",
      " (9,\n",
      "  '0.009*\"be\" + 0.008*\"there\" + 0.006*\"cd\" + 0.004*\"when\" + 0.004*\"even\" + '\n",
      "  '0.003*\"do\" + 0.003*\"have\" + 0.003*\"love\" + 0.003*\"more\" + 0.002*\"More\"'),\n",
      " (10,\n",
      "  '0.023*\"cd\" + 0.015*\"be\" + 0.011*\"there\" + 0.007*\"love\" + 0.006*\"when\" + '\n",
      "  '0.006*\"so\" + 0.005*\"take\" + 0.004*\"do\" + 0.004*\"just\" + 0.004*\"now\"'),\n",
      " (11,\n",
      "  '0.008*\"be\" + 0.008*\"there\" + 0.006*\"even\" + 0.005*\"live\" + 0.005*\"cd\" + '\n",
      "  '0.004*\"so\" + 0.003*\"come\" + 0.003*\"here\" + 0.003*\"when\" + 0.003*\"do\"'),\n",
      " (12,\n",
      "  '0.010*\"cd\" + 0.007*\"love\" + 0.007*\"be\" + 0.005*\"even\" + 0.005*\"there\" + '\n",
      "  '0.004*\"so\" + 0.003*\"know\" + 0.003*\"when\" + 0.002*\"however\" + 0.002*\"get\"'),\n",
      " (13,\n",
      "  '0.021*\"be\" + 0.016*\"cd\" + 0.009*\"there\" + 0.004*\"even\" + 0.004*\"just\" + '\n",
      "  '0.004*\"so\" + 0.003*\"have\" + 0.003*\"when\" + 0.003*\"do\" + 0.003*\"say\"'),\n",
      " (14,\n",
      "  '0.014*\"be\" + 0.013*\"cd\" + 0.012*\"there\" + 0.010*\"when\" + 0.006*\"love\" + '\n",
      "  '0.005*\"so\" + 0.004*\"even\" + 0.004*\"now\" + 0.003*\"do\" + 0.003*\"let\"'),\n",
      " (15,\n",
      "  '0.016*\"cd\" + 0.015*\"be\" + 0.010*\"there\" + 0.008*\"live\" + 0.006*\"when\" + '\n",
      "  '0.006*\"just\" + 0.006*\"even\" + 0.005*\"so\" + 0.004*\"love\" + 0.004*\"have\"'),\n",
      " (16,\n",
      "  '0.014*\"cd\" + 0.010*\"be\" + 0.008*\"there\" + 0.007*\"when\" + 0.004*\"so\" + '\n",
      "  '0.004*\"even\" + 0.003*\"do\" + 0.003*\"just\" + 0.003*\"get\" + 0.003*\"how\"'),\n",
      " (17,\n",
      "  '0.057*\"cd\" + 0.021*\"be\" + 0.008*\"there\" + 0.007*\"so\" + 0.006*\"have\" + '\n",
      "  '0.006*\"love\" + 0.006*\"here\" + 0.006*\"come\" + 0.006*\"great\" + 0.005*\"do\"'),\n",
      " (18,\n",
      "  '0.009*\"there\" + 0.006*\"be\" + 0.006*\"cd\" + 0.005*\"here\" + 0.005*\"so\" + '\n",
      "  '0.005*\"even\" + 0.004*\"when\" + 0.004*\"dolly\" + 0.003*\"however\" + '\n",
      "  '0.003*\"love\"'),\n",
      " (19,\n",
      "  '0.032*\"cd\" + 0.011*\"be\" + 0.009*\"get\" + 0.008*\"there\" + 0.006*\"so\" + '\n",
      "  '0.006*\"when\" + 0.006*\"do\" + 0.005*\"have\" + 0.004*\"just\" + 0.004*\"love\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(test_lda.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "78455813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['canibus', 'sorprender', 'sorprendente', 'paquin', 'coasters', 'sorgo', 'sorcerer', 'fubert', 'parables', 'fruta', 'paradice', 'sophistipop', 'paradiso', 'paradisum', 'paradoja', 'cobalt', 'frumpy', 'cobbling', 'fuckers', 'coarseness', 'fuckery', 'papydave', 'fuese', 'papercuts', 'sostenuto', 'papered', 'cluttering', 'cmg', 'papitour', 'cnicamente', 'paragons', 'papoose', 'pappalardi', 'coaches', 'coaching', 'sorprendio', 'coalesced', 'pappou', 'coalescing', 'coalition', 'sorriso', 'fugee', 'parakato', 'cobrastyle', 'frontwomen', 'param', 'frontrunner', 'sonne', 'parameter', 'sonidito', 'soni', 'songwritter', 'codify', 'songwritery', 'frontloads', 'coe', 'frontload', 'coexisted', 'songs1', 'songon', 'coffeemaker', 'cocorosie', 'cocooned', 'frosting', 'cocoband', 'coburn', 'frugal', 'paralelas', 'froze', 'cocciante', 'coche', 'frowned', 'cocked', 'cobrasnake', 'sooooooooooooo', 'sooooooooo', 'cockney', 'cocks', 'sooooooo', 'frown', 'cocktails', 'frothing', 'frothier', 'cockeyed', 'songing', 'fugual', 'clusterfuck', 'clits', 'soweto', 'funerary', 'clobbering', 'sovereignty', 'fundamentales', 'panamanian', 'pandeiro', 'functionally', 'cloned', 'funabashi', 'clonking', 'clooney']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bottom_n_words(a3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5a6f232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bottom_n_words_tfidf(input_analyzer, n):\n",
    "    top_n = []\n",
    "    feats = input_analyzer.tfidf_vectorizer.get_feature_names()\n",
    "    w_counts = np.array(np.sum(input_analyzer.get_tfidf_matrix(), axis=0))[0]\n",
    "    sorted_indices = np.argsort(w_counts)[:n]\n",
    "    \n",
    "    for i in sorted_indices:\n",
    "        top_n.append(feats[i])\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "59b9a9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['canibus', 'sorprender', 'sorprendente', 'paquin', 'coasters', 'sorgo', 'sorcerer', 'fubert', 'parables', 'fruta']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bottom_n_words_tfidf(a3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5eb8ff9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['canibus', 'sorprender', 'sorprendente', 'paquin', 'coasters', 'sorgo', 'sorcerer', 'fubert', 'parables', 'fruta']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bottom_n_words(a3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "685648c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_n = 1000\n",
    "len(set(get_bottom_n_words_tfidf(a3, test_n)).intersection(set(get_bottom_n_words(a3, test_n))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f17598f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['audio_properties', 'tags', 'version']\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/raw/mard_reviews/acousticbrainz_descriptors/B000008I2U_53a4307e-9331-4489-9874-cb5acf818a0b.json\", \"r\") as f:\n",
    "    d = json.load(f)\n",
    "    f.close()\n",
    "pprint(sorted(list(d['metadata'].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7c2d435e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lowlevel', 'metadata', 'rhythm', 'tonal']\n"
     ]
    }
   ],
   "source": [
    "pprint(sorted(list(d.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5423228a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beats_count',\n",
      " 'beats_loudness',\n",
      " 'beats_loudness_band_ratio',\n",
      " 'beats_position',\n",
      " 'bpm',\n",
      " 'bpm_histogram_first_peak_bpm',\n",
      " 'bpm_histogram_first_peak_spread',\n",
      " 'bpm_histogram_first_peak_weight',\n",
      " 'bpm_histogram_second_peak_bpm',\n",
      " 'bpm_histogram_second_peak_spread',\n",
      " 'bpm_histogram_second_peak_weight',\n",
      " 'danceability',\n",
      " 'onset_rate']\n"
     ]
    }
   ],
   "source": [
    "pprint(sorted(list(d['rhythm'].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7541bcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average_loudness',\n",
      " 'barkbands',\n",
      " 'barkbands_crest',\n",
      " 'barkbands_flatness_db',\n",
      " 'barkbands_kurtosis',\n",
      " 'barkbands_skewness',\n",
      " 'barkbands_spread',\n",
      " 'dissonance',\n",
      " 'dynamic_complexity',\n",
      " 'erbbands',\n",
      " 'erbbands_crest',\n",
      " 'erbbands_flatness_db',\n",
      " 'erbbands_kurtosis',\n",
      " 'erbbands_skewness',\n",
      " 'erbbands_spread',\n",
      " 'gfcc',\n",
      " 'hfc',\n",
      " 'melbands',\n",
      " 'melbands_crest',\n",
      " 'melbands_flatness_db',\n",
      " 'melbands_kurtosis',\n",
      " 'melbands_skewness',\n",
      " 'melbands_spread',\n",
      " 'mfcc',\n",
      " 'pitch_salience',\n",
      " 'silence_rate_20dB',\n",
      " 'silence_rate_30dB',\n",
      " 'silence_rate_60dB',\n",
      " 'spectral_centroid',\n",
      " 'spectral_complexity',\n",
      " 'spectral_contrast_coeffs',\n",
      " 'spectral_contrast_valleys',\n",
      " 'spectral_decrease',\n",
      " 'spectral_energy',\n",
      " 'spectral_energyband_high',\n",
      " 'spectral_energyband_low',\n",
      " 'spectral_energyband_middle_high',\n",
      " 'spectral_energyband_middle_low',\n",
      " 'spectral_entropy',\n",
      " 'spectral_flux',\n",
      " 'spectral_kurtosis',\n",
      " 'spectral_rms',\n",
      " 'spectral_rolloff',\n",
      " 'spectral_skewness',\n",
      " 'spectral_spread',\n",
      " 'spectral_strongpeak',\n",
      " 'zerocrossingrate']\n"
     ]
    }
   ],
   "source": [
    "pprint(sorted(list(d['lowlevel'].keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a1d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "singerenv",
   "language": "python",
   "name": "singerenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
