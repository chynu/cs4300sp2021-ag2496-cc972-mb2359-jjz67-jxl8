{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dc602c4",
   "metadata": {},
   "source": [
    "# ArtistReviewAnalyzer Class Tester\n",
    "\n",
    "Because I'm too lazy to run the file itself so I'm just forcefully doing it through jupyter lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15b40260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d368b",
   "metadata": {},
   "source": [
    "## helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76cf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "import re\n",
    "import unidecode as ud\n",
    "\n",
    "\n",
    "def clean_str(input_artist, cap_code=0):\n",
    "    \"\"\"\n",
    "    Takes in string, returns a unicode-friendly and stripped version of the string.\n",
    "    \"\"\"\n",
    "    return_artist_str = input_artist\n",
    "\n",
    "    # === REGEX REPLACE ===\n",
    "    repl_tuples = [(r'(^\\s+)|(\\s+$)', ''),  # whitespace at beg/end of string\n",
    "                   (r'\\s+', ' '),  # Remove double spaces\n",
    "                   (r'[\\n|\\r|\\t|\\0]+', ' ')\n",
    "                   ]\n",
    "    for ptn, repl_str in repl_tuples:\n",
    "        return_artist_str = re.sub(ptn, repl_str, return_artist_str)\n",
    "\n",
    "    # === UNICODE HANDLING ===\n",
    "    return_artist_str = ud.unidecode(return_artist_str)\n",
    "\n",
    "    if cap_code == -1:\n",
    "        return_artist_str = return_artist_str.lower()\n",
    "    elif cap_code == 1:\n",
    "        return_artist_str = return_artist_str.upper()\n",
    "\n",
    "    return return_artist_str\n",
    "\n",
    "\n",
    "def read_mard_json(input_filename):\n",
    "    \"\"\"\n",
    "    Takes in file name of JSON, returns a list of dictionaries in which\n",
    "    each element is a row with columns (as the keys).\n",
    "    \"\"\"\n",
    "    loaded_data_ = []\n",
    "    with open(input_filename, 'r') as file_:\n",
    "        loaded_string_ = file_.read()\n",
    "        loaded_data_ = [json.loads(s) for s in loaded_string_.split('\\n') if s is not None and len(s) > 0]\n",
    "        file_.close()\n",
    "    return loaded_data_\n",
    "\n",
    "\n",
    "def read_mard_json_as_df(input_filename):\n",
    "    \"\"\"\n",
    "    Takes in file name of JSON, returns a list of dictionaries in which\n",
    "    each element is a row with columns (as the keys).\n",
    "    \"\"\"\n",
    "    loaded_data_ = []\n",
    "    with open(input_filename, 'r') as file_:\n",
    "        loaded_string_ = file_.read()\n",
    "        loaded_data_ = [json.loads(s) for s in loaded_string_.split('\\n') if s is not None and len(s) > 0]\n",
    "        file_.close()\n",
    "    return pd.DataFrame(loaded_data_)\n",
    "\n",
    "\n",
    "def run_query_on_sqlite_db(input_query, input_filename):\n",
    "    \"\"\"\n",
    "\n",
    "    Returns a Pandas DataFrame object containing the query results,\n",
    "    given the user's query and the filename for the sqlite database.\n",
    "\n",
    "    Input:\n",
    "     - input_query: string representation of the SQL query to run on the sqlite db\n",
    "     - input_filename: the file location of the sqlite database\n",
    "\n",
    "    \"\"\"\n",
    "    conn_ = sqlite3.connect(input_filename)\n",
    "    df_ = pd.read_sql_query(input_query, conn_)\n",
    "    conn_.close()\n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac95e2",
   "metadata": {},
   "source": [
    "## ArtistReviewAnalyzer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b33e3119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import spacy\n",
    "\n",
    "\n",
    "class ArtistReviewAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes artist reviews. Forms into useful data structures for NLP and topic modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Basic data\n",
    "        self.file_loc = None\n",
    "        self.raw = None\n",
    "        self.artists_list = None\n",
    "\n",
    "        # Stop-Words for tokenization\n",
    "        self.stop_words = stopwords.words('english')  # Basic stop words\n",
    "        self.update_stopwords(['album', 'music', 'cd', 'track', 'song', 'sound'])\n",
    "\n",
    "        # Vectorizers\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.count_vectorizer = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.count_matrix = None\n",
    "        self.__tokenizer = None\n",
    "        self.tokenized_reviews = None\n",
    "\n",
    "        # NLP tools\n",
    "        self.lda_model = None\n",
    "        self.bigrams = None\n",
    "        self.lemmatized_text = None\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])  # Spacy tool\n",
    "\n",
    "\n",
    "    def load_data(self, file_loc):\n",
    "        \"\"\"\n",
    "        Loads reviews data (JSON file) into a dictionary.\n",
    "        \"\"\"\n",
    "        # Save file location\n",
    "        self.file_loc = file_loc\n",
    "\n",
    "        with open(file_loc, 'r') as file:\n",
    "            d = json.load(file)\n",
    "            file.close()\n",
    "        self.raw = d  # Save raw dictionary\n",
    "\n",
    "        self.artists_list = list(self.raw.keys())  # Save artists list\n",
    "\n",
    "        self.build_count_vectorizer(2, 0.8)  # Build count vec so we have a tokenizer\n",
    "\n",
    "        # Handling tokenized list of reviews\n",
    "        self.tokenized_reviews = self.__build_reviews_list(do_tokenize=True)\n",
    "        self.remove_stopwords_from_tokenized_list()\n",
    "\n",
    "        return self.raw\n",
    "\n",
    "    def refresh(self):\n",
    "        self.build_count_vectorizer(2, 0.8)\n",
    "        self.tokenized_reviews = self.__build_reviews_list(do_tokenize=True)\n",
    "        self.remove_stopwords_from_tokenized_list()\n",
    "\n",
    "    def build_count_vectorizer(self, min_df, max_df):\n",
    "        if self.count_vectorizer is not None:\n",
    "            print(\"WARNING: Count vectorizer has already been built.\")\n",
    "            return self.count_vectorizer\n",
    "        self.count_vectorizer = CountVectorizer(analyzer='word', stop_words=self.stop_words, min_df=min_df, max_df=max_df)\n",
    "        self.__tokenizer = self.count_vectorizer.build_tokenizer()\n",
    "        return self.count_vectorizer\n",
    "\n",
    "    def get_count_matrix(self):\n",
    "        if self.count_vectorizer is None:\n",
    "            return None\n",
    "\n",
    "        if self.count_matrix is None:\n",
    "            # TODO: self.raw is not truly reflective of the data since we clean it.\n",
    "            self.count_matrix = self.count_vectorizer.fit_transform(self.raw)\n",
    "\n",
    "    def build_tfidf_vectorizer(self, min_df, max_df):\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words=self.stop_words, min_df=min_df, max_df=max_df)\n",
    "        return self.tfidf_vectorizer\n",
    "\n",
    "    def get_tfidf_matrix(self):\n",
    "        if self.tfidf_vectorizer is None:\n",
    "            return None\n",
    "\n",
    "        if self.tfidf_matrix is None:\n",
    "            # TODO: self.raw is not truly reflective of the data since we clean it.\n",
    "            self.count_matrix = self.tfidf_vectorizer.fit_transform(self.raw)\n",
    "\n",
    "    def tokenize(self, input_string):\n",
    "        if self.__tokenizer is None:\n",
    "            print(\"WARNING: Tokenizer not set.\")\n",
    "            return None\n",
    "        return self.__tokenizer(input_string)\n",
    "\n",
    "    def update_stopwords(self, word_list):\n",
    "        self.stop_words.extend(word_list)\n",
    "        return None\n",
    "\n",
    "    def remove_stopwords_from_tokenized_list(self):\n",
    "        self.tokenized_reviews = \\\n",
    "            [[w for w in artist_review if w not in self.stop_words] for artist_review in self.tokenized_reviews]\n",
    "\n",
    "    def build_lda_model(self, num_topics=20):\n",
    "        self.bigrams = self.make_bigrams()\n",
    "        self.lemmatized_text = self.lemmatize()\n",
    "        id2word = corpora.Dictionary(self.lemmatized_text)\n",
    "        corpus = [id2word.doc2bow(w) for w in self.lemmatized_text]\n",
    "        self.lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "        return self.lda_model\n",
    "\n",
    "    def make_bigrams(self):\n",
    "        bg = gensim.models.Phrases(self.tokenized_reviews, min_count=5, threshold=100)\n",
    "        bg_mod = gensim.models.phrases.Phraser(bg)\n",
    "        return [bg_mod[d] for d in self.tokenized_reviews]\n",
    "\n",
    "    def lemmatize(self, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        texts_out = []\n",
    "        for review_words in self.tokenized_reviews:\n",
    "            joined_words = self.nlp(\" \".join(review_words))\n",
    "            texts_out.append([token.lemma_ for token in joined_words if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "\n",
    "    def get_all_words(self, do_tokenize=False):\n",
    "        return self.tokenize(\" \".join(self.raw)) if do_tokenize else \" \".join(self.raw)\n",
    "\n",
    "    def __build_reviews_list(self, do_tokenize=False):\n",
    "        \"\"\"\n",
    "        Builds a list of reviews for each artist.\n",
    "        \"\"\"\n",
    "        consolidated_reviews = []\n",
    "        for a in self.artists_list:\n",
    "            if do_tokenize:\n",
    "                consolidated_reviews.append(self.tokenize(clean_str(\" \".join(self.raw[a]))))\n",
    "            else:\n",
    "                consolidated_reviews.append(clean_str(\" \".join(self.raw[a])))\n",
    "        return consolidated_reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285ce17",
   "metadata": {},
   "source": [
    "## Test runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cade9669",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_location = \"../data/processed/artist_reviews.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c955835",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ArtistReviewAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a743968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.load_data(json_file_location);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea74691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.build_lda_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a79e70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*\"quot\" + 0.008*\"good\" + 0.007*\"make\" + 0.006*\"song\" + 0.006*\"well\" + '\n",
      "  '0.005*\"great\" + 0.005*\"even\" + 0.005*\"hear\" + 0.005*\"work\" + 0.004*\"first\"'),\n",
      " (1,\n",
      "  '0.008*\"get\" + 0.008*\"good\" + 0.008*\"great\" + 0.007*\"song\" + 0.006*\"well\" + '\n",
      "  '0.006*\"record\" + 0.005*\"first\" + 0.005*\"love\" + 0.005*\"make\" + '\n",
      "  '0.005*\"band\"'),\n",
      " (2,\n",
      "  '0.010*\"quot\" + 0.008*\"song\" + 0.007*\"great\" + 0.007*\"good\" + 0.006*\"well\" + '\n",
      "  '0.006*\"get\" + 0.006*\"record\" + 0.005*\"time\" + 0.005*\"make\" + 0.005*\"band\"'),\n",
      " (3,\n",
      "  '0.011*\"song\" + 0.009*\"good\" + 0.008*\"love\" + 0.008*\"make\" + 0.007*\"get\" + '\n",
      "  '0.006*\"time\" + 0.006*\"great\" + 0.005*\"cd\" + 0.005*\"well\" + 0.005*\"hear\"'),\n",
      " (4,\n",
      "  '0.008*\"make\" + 0.006*\"record\" + 0.006*\"band\" + 0.006*\"good\" + 0.005*\"well\" '\n",
      "  '+ 0.005*\"go\" + 0.005*\"time\" + 0.004*\"get\" + 0.004*\"song\" + 0.004*\"first\"'),\n",
      " (5,\n",
      "  '0.008*\"make\" + 0.007*\"song\" + 0.006*\"get\" + 0.006*\"well\" + 0.006*\"good\" + '\n",
      "  '0.006*\"quot\" + 0.006*\"band\" + 0.006*\"time\" + 0.005*\"record\" + 0.004*\"say\"'),\n",
      " (6,\n",
      "  '0.009*\"quot\" + 0.009*\"song\" + 0.007*\"good\" + 0.006*\"cd\" + 0.006*\"get\" + '\n",
      "  '0.006*\"great\" + 0.006*\"make\" + 0.005*\"time\" + 0.005*\"hear\" + 0.004*\"even\"'),\n",
      " (7,\n",
      "  '0.007*\"good\" + 0.007*\"song\" + 0.006*\"get\" + 0.006*\"well\" + 0.006*\"even\" + '\n",
      "  '0.005*\"great\" + 0.005*\"band\" + 0.005*\"time\" + 0.005*\"work\" + 0.004*\"come\"'),\n",
      " (8,\n",
      "  '0.007*\"good\" + 0.006*\"get\" + 0.006*\"record\" + 0.005*\"song\" + 0.005*\"make\" + '\n",
      "  '0.005*\"band\" + 0.005*\"play\" + 0.005*\"hear\" + 0.005*\"great\" + 0.004*\"cd\"'),\n",
      " (9,\n",
      "  '0.009*\"song\" + 0.007*\"good\" + 0.006*\"great\" + 0.006*\"record\" + 0.006*\"make\" '\n",
      "  '+ 0.006*\"get\" + 0.005*\"well\" + 0.005*\"first\" + 0.005*\"cd\" + 0.005*\"love\"'),\n",
      " (10,\n",
      "  '0.008*\"make\" + 0.007*\"band\" + 0.006*\"quot\" + 0.006*\"good\" + 0.006*\"time\" + '\n",
      "  '0.006*\"get\" + 0.005*\"record\" + 0.005*\"song\" + 0.005*\"much\" + 0.004*\"cd\"'),\n",
      " (11,\n",
      "  '0.008*\"quot\" + 0.006*\"good\" + 0.006*\"great\" + 0.005*\"get\" + 0.005*\"time\" + '\n",
      "  '0.005*\"record\" + 0.005*\"song\" + 0.005*\"make\" + 0.005*\"hear\" + 0.005*\"well\"'),\n",
      " (12,\n",
      "  '0.010*\"song\" + 0.008*\"good\" + 0.008*\"make\" + 0.008*\"quot\" + 0.007*\"well\" + '\n",
      "  '0.007*\"get\" + 0.007*\"great\" + 0.007*\"record\" + 0.005*\"go\" + 0.005*\"time\"'),\n",
      " (13,\n",
      "  '0.009*\"good\" + 0.009*\"make\" + 0.007*\"song\" + 0.007*\"great\" + 0.006*\"love\" + '\n",
      "  '0.005*\"even\" + 0.005*\"quot\" + 0.005*\"get\" + 0.005*\"time\" + 0.005*\"go\"'),\n",
      " (14,\n",
      "  '0.006*\"good\" + 0.006*\"get\" + 0.006*\"make\" + 0.006*\"great\" + 0.005*\"record\" '\n",
      "  '+ 0.005*\"band\" + 0.005*\"well\" + 0.005*\"listen\" + 0.005*\"song\" + '\n",
      "  '0.005*\"year\"'),\n",
      " (15,\n",
      "  '0.007*\"song\" + 0.006*\"make\" + 0.006*\"record\" + 0.006*\"well\" + 0.006*\"good\" '\n",
      "  '+ 0.005*\"get\" + 0.005*\"release\" + 0.005*\"time\" + 0.004*\"love\" + '\n",
      "  '0.004*\"great\"'),\n",
      " (16,\n",
      "  '0.009*\"song\" + 0.008*\"well\" + 0.007*\"love\" + 0.006*\"good\" + 0.006*\"band\" + '\n",
      "  '0.005*\"get\" + 0.005*\"quot\" + 0.005*\"great\" + 0.005*\"time\" + 0.005*\"come\"'),\n",
      " (17,\n",
      "  '0.010*\"good\" + 0.009*\"band\" + 0.007*\"song\" + 0.007*\"make\" + 0.006*\"great\" + '\n",
      "  '0.006*\"record\" + 0.006*\"get\" + 0.005*\"year\" + 0.005*\"time\" + 0.005*\"even\"'),\n",
      " (18,\n",
      "  '0.008*\"good\" + 0.008*\"song\" + 0.007*\"well\" + 0.006*\"play\" + 0.006*\"time\" + '\n",
      "  '0.006*\"cd\" + 0.005*\"great\" + 0.005*\"go\" + 0.005*\"hear\" + 0.005*\"get\"'),\n",
      " (19,\n",
      "  '0.008*\"good\" + 0.007*\"record\" + 0.007*\"get\" + 0.006*\"quot\" + 0.006*\"great\" '\n",
      "  '+ 0.006*\"first\" + 0.006*\"make\" + 0.005*\"even\" + 0.005*\"well\" + '\n",
      "  '0.005*\"song\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(analyzer.lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9828db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ArtistReviewAnalyzer()\n",
    "analyzer.update_stopwords(['good', 'band', 'record', 'make', 'great', 'hear'])\n",
    "analyzer.load_data(json_file_location);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b446186",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.build_lda_model(num_topics=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6f19e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10,\n",
      "  '0.006*\"even\" + 0.005*\"song\" + 0.005*\"get\" + 0.005*\"time\" + 0.004*\"well\" + '\n",
      "  '0.004*\"work\" + 0.004*\"make\" + 0.004*\"sound\" + 0.004*\"come\" + 0.004*\"go\"'),\n",
      " (22,\n",
      "  '0.008*\"get\" + 0.006*\"time\" + 0.006*\"song\" + 0.006*\"go\" + 0.005*\"quot\" + '\n",
      "  '0.005*\"first\" + 0.005*\"well\" + 0.004*\"love\" + 0.004*\"make\" + 0.004*\"good\"'),\n",
      " (39,\n",
      "  '0.007*\"song\" + 0.006*\"get\" + 0.005*\"come\" + 0.005*\"time\" + 0.005*\"new\" + '\n",
      "  '0.004*\"play\" + 0.004*\"first\" + 0.004*\"love\" + 0.004*\"work\" + 0.004*\"good\"'),\n",
      " (1,\n",
      "  '0.010*\"song\" + 0.008*\"quot\" + 0.007*\"well\" + 0.006*\"time\" + 0.006*\"get\" + '\n",
      "  '0.005*\"go\" + 0.005*\"love\" + 0.005*\"first\" + 0.004*\"come\" + 0.004*\"year\"'),\n",
      " (38,\n",
      "  '0.008*\"song\" + 0.006*\"go\" + 0.005*\"even\" + 0.005*\"first\" + 0.005*\"make\" + '\n",
      "  '0.004*\"well\" + 0.004*\"work\" + 0.004*\"get\" + 0.004*\"be\" + 0.004*\"voice\"'),\n",
      " (11,\n",
      "  '0.008*\"get\" + 0.007*\"song\" + 0.006*\"love\" + 0.006*\"quot\" + 0.005*\"make\" + '\n",
      "  '0.005*\"well\" + 0.005*\"time\" + 0.004*\"find\" + 0.004*\"go\" + 0.004*\"work\"'),\n",
      " (12,\n",
      "  '0.008*\"quot\" + 0.007*\"song\" + 0.007*\"get\" + 0.006*\"well\" + 0.005*\"love\" + '\n",
      "  '0.005*\"year\" + 0.005*\"come\" + 0.005*\"work\" + 0.005*\"go\" + 0.004*\"time\"'),\n",
      " (21,\n",
      "  '0.007*\"quot\" + 0.006*\"song\" + 0.006*\"cd\" + 0.006*\"time\" + 0.005*\"well\" + '\n",
      "  '0.005*\"make\" + 0.005*\"get\" + 0.005*\"love\" + 0.005*\"listen\" + 0.005*\"first\"'),\n",
      " (0,\n",
      "  '0.010*\"quot\" + 0.007*\"song\" + 0.005*\"get\" + 0.005*\"well\" + 0.005*\"love\" + '\n",
      "  '0.005*\"much\" + 0.005*\"work\" + 0.004*\"even\" + 0.004*\"good\" + 0.004*\"make\"'),\n",
      " (8,\n",
      "  '0.007*\"song\" + 0.007*\"well\" + 0.005*\"time\" + 0.005*\"get\" + 0.005*\"play\" + '\n",
      "  '0.004*\"make\" + 0.004*\"year\" + 0.004*\"take\" + 0.004*\"sound\" + 0.004*\"say\"'),\n",
      " (18,\n",
      "  '0.011*\"song\" + 0.006*\"get\" + 0.005*\"play\" + 0.005*\"well\" + 0.005*\"time\" + '\n",
      "  '0.005*\"new\" + 0.005*\"first\" + 0.005*\"come\" + 0.005*\"listen\" + 0.004*\"quot\"'),\n",
      " (24,\n",
      "  '0.007*\"song\" + 0.006*\"get\" + 0.005*\"well\" + 0.005*\"cd\" + 0.005*\"first\" + '\n",
      "  '0.005*\"year\" + 0.005*\"love\" + 0.004*\"make\" + 0.004*\"even\" + 0.004*\"work\"'),\n",
      " (33,\n",
      "  '0.008*\"song\" + 0.007*\"love\" + 0.007*\"quot\" + 0.006*\"even\" + 0.005*\"get\" + '\n",
      "  '0.005*\"cd\" + 0.005*\"well\" + 0.004*\"good\" + 0.004*\"come\" + 0.004*\"first\"'),\n",
      " (37,\n",
      "  '0.009*\"song\" + 0.008*\"well\" + 0.007*\"even\" + 0.007*\"make\" + 0.006*\"get\" + '\n",
      "  '0.006*\"love\" + 0.006*\"listen\" + 0.006*\"time\" + 0.005*\"first\" + 0.005*\"cd\"'),\n",
      " (19,\n",
      "  '0.007*\"well\" + 0.005*\"time\" + 0.005*\"recording\" + 0.005*\"make\" + '\n",
      "  '0.004*\"get\" + 0.004*\"quot\" + 0.004*\"first\" + 0.004*\"record\" + 0.004*\"also\" '\n",
      "  '+ 0.004*\"even\"'),\n",
      " (35,\n",
      "  '0.008*\"well\" + 0.007*\"quot\" + 0.007*\"song\" + 0.006*\"recording\" + 0.006*\"cd\" '\n",
      "  '+ 0.005*\"time\" + 0.005*\"get\" + 0.005*\"year\" + 0.005*\"work\" + 0.004*\"know\"'),\n",
      " (9,\n",
      "  '0.008*\"song\" + 0.007*\"quot\" + 0.006*\"first\" + 0.005*\"go\" + 0.005*\"get\" + '\n",
      "  '0.005*\"well\" + 0.004*\"much\" + 0.004*\"work\" + 0.004*\"come\" + 0.004*\"even\"'),\n",
      " (5,\n",
      "  '0.010*\"song\" + 0.008*\"get\" + 0.007*\"well\" + 0.006*\"time\" + 0.006*\"quot\" + '\n",
      "  '0.006*\"cd\" + 0.005*\"love\" + 0.005*\"first\" + 0.005*\"find\" + 0.005*\"listen\"'),\n",
      " (2,\n",
      "  '0.008*\"get\" + 0.007*\"song\" + 0.007*\"cd\" + 0.007*\"love\" + 0.006*\"time\" + '\n",
      "  '0.006*\"well\" + 0.005*\"go\" + 0.005*\"work\" + 0.005*\"make\" + 0.004*\"year\"'),\n",
      " (13,\n",
      "  '0.008*\"song\" + 0.007*\"quot\" + 0.007*\"get\" + 0.007*\"well\" + 0.005*\"love\" + '\n",
      "  '0.005*\"also\" + 0.005*\"cd\" + 0.005*\"release\" + 0.005*\"time\" + 0.005*\"even\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(analyzer.lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafae830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "singerenv",
   "language": "python",
   "name": "singerenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
