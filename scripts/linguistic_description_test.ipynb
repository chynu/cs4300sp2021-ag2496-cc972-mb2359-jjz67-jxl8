{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71463ac",
   "metadata": {},
   "source": [
    "# Linguistic Description of Songs\n",
    "\n",
    "Exploring how we can deduce song genres or narrow down artists from a user-inputted linguistic description.\n",
    "\n",
    "Columns afforded by the Kaggle dataset:\n",
    " - acousticness\n",
    " - artists\n",
    " - danceability\n",
    " - duration_ms\n",
    " - energy\n",
    " - explicit\n",
    " - id\n",
    " - instrumentalness\n",
    " - key\n",
    " - liveness\n",
    " - loudness\n",
    " - mode\n",
    " - name\n",
    " - popularity\n",
    " - release_date\n",
    " - speechiness\n",
    " - tempo\n",
    " - valence\n",
    " - year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a270a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "import re\n",
    "import unidecode as ud\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba8e02",
   "metadata": {},
   "source": [
    "## Standardized Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e58d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(input_artist, cap_code=0):\n",
    "    \"\"\"\n",
    "    Takes in string, returns a unicode-friendly and stripped version of the string.\n",
    "    \"\"\"\n",
    "    return_artist_str = input_artist\n",
    "    \n",
    "    # === REGEX REPLACE ===\n",
    "    repl_tuples = [(r'(^\\s+)|(\\s+$)', ''),           # whitespace at beg/end of string\n",
    "                   (r'\\s+', ' '),                    # Remove double spaces\n",
    "                   (r'[\\n|\\r|\\t|\\0]+', ' ')\n",
    "                  ]\n",
    "    for ptn, repl_str in repl_tuples:\n",
    "        return_artist_str = re.sub(ptn, repl_str, return_artist_str)\n",
    "    \n",
    "    # === UNICODE HANDLING ===\n",
    "    return_artist_str = ud.unidecode(return_artist_str)\n",
    "    \n",
    "    if cap_code == -1: return_artist_str = return_artist_str.lower()\n",
    "    elif cap_code == 1: return_artist_str = return_artist_str.upper()\n",
    "    \n",
    "    return return_artist_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b83676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mard_json(input_filename):\n",
    "    \"\"\"\n",
    "    \n",
    "    Takes in file name of JSON, returns a list of dictionaries in which\n",
    "    each element is a row with columns (as the keys).\n",
    "    \n",
    "    \"\"\"\n",
    "    loaded_data_ = []\n",
    "    with open(input_filename, 'r') as file_:\n",
    "        loaded_string_ = file_.read()\n",
    "        loaded_data_ = [json.loads(s) for s in loaded_string_.split('\\n') if s is not None and len(s) > 0]\n",
    "        file_.close()\n",
    "    return loaded_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "13aeef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mard_json_as_df(input_filename):\n",
    "    \"\"\"\n",
    "    \n",
    "    Takes in file name of JSON, returns a list of dictionaries in which\n",
    "    each element is a row with columns (as the keys).\n",
    "    \n",
    "    \"\"\"\n",
    "    loaded_data_ = []\n",
    "    with open(input_filename, 'r') as file_:\n",
    "        loaded_string_ = file_.read()\n",
    "        loaded_data_ = [json.loads(s) for s in loaded_string_.split('\\n') if s is not None and len(s) > 0]\n",
    "        file_.close()\n",
    "    return pd.DataFrame(loaded_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a340410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query_on_sqlite_db(input_query, input_filename):\n",
    "    \"\"\"\n",
    "    \n",
    "    Returns a Pandas DataFrame object containing the query results,\n",
    "    given the user's query and the filename for the sqlite database.\n",
    "    \n",
    "    Input:\n",
    "     - input_query: string representation of the SQL query to run on the sqlite db\n",
    "     - input_filename: the file location of the sqlite database\n",
    "     \n",
    "    \"\"\"\n",
    "    conn_ = sqlite3.connect(input_filename)\n",
    "    df_ = pd.read_sql_query(input_query, conn_)\n",
    "    conn_.close()\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9902d",
   "metadata": {},
   "source": [
    "## Mard Reviews Dataset\n",
    "\n",
    "### Loading & Understanding Data\n",
    "\n",
    "First we load in the data. Data source can be found [here](https://www.upf.edu/web/mtg/mard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff638a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Metadata\n",
    "mard_metadata = read_mard_json('../data/raw/mard_reviews/mard_metadata.json')\n",
    "    \n",
    "# Loading Reviews\n",
    "mard_reviews = read_mard_json('../data/raw/mard_reviews/mard_reviews.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31788717",
   "metadata": {},
   "source": [
    "Then we print some metadata to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eef68799",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_md_keys = set([])\n",
    "all_rev_keys = set([])\n",
    "for row in mard_metadata:\n",
    "    all_md_keys.update(list(row.keys()))\n",
    "for row in mard_reviews:\n",
    "    all_rev_keys.update(list(row.keys()))\n",
    "\n",
    "# Gather all metadata and review keys to see what is going on\n",
    "all_md_keys = list(all_md_keys)\n",
    "all_rev_keys = list(all_rev_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669a35ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METADATA KEYS\n",
      "['first-release-year', 'categories', 'imUrl', 'artist-mbid', 'price', 'title', 'artist', 'release-group-mbid', 'root-genre', 'related', 'amazon-id', 'salesRank', 'brand', 'confidence', 'artist_url', 'release-mbid', 'label', 'songs']\n",
      "\n",
      "REVIEWS KEYS\n",
      "['summary', 'reviewerID', 'reviewerName', 'reviewText', 'amazon-id', 'unixReviewTime', 'helpful', 'reviewTime', 'overall']\n"
     ]
    }
   ],
   "source": [
    "print('METADATA KEYS')\n",
    "print(all_md_keys)\n",
    "print()\n",
    "print('REVIEWS KEYS')\n",
    "print(all_rev_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb142f4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buy this album. Now.  Don't worry about the reviews.  If you love pure, honest music buy this album....you will not be let down.\n",
      "\n",
      "The Sudden Passion did a great job with this one. The lyrics are witty, with just the right amount of twang.  This is a whiskey-soaked indie rock jam that captures the soul of modern americana!\n",
      "\n",
      "I received this CD as a gift a few weeks ago from a friend. I was a bit skeptical at first but decided to try it anyway. This CD is great! The first two tracks talk you through how to use the CD and give you great techniques for breathing. I like track 3 because it's only 10 minutes which helps a lot with my busy schedule. It's already helped me sleep better and feel better during the day. Highly recommended!\n",
      "\n",
      "I am a beginner and have tried a couple of meditation CDs on the market but have disliked them due to poor audio quality and all the Far East philosophy they try to get you to buy into. I simply wanted a quick way to get started meditating so that I could relieve my stress after a long day. This CD has been a blessing as it is easy to understand and benefit from. The music and the sound effects help me regulate my breathing and synchronize to the guided meditations. I highly recommend this for anybody looking to improve their quality of life and mental focus and don't want to spend a lot of money or have to subscribe to complicated spiritual values from India or China.  I had my wife try it and she loves it. Now we meditate together and it has brought us closer and even increased passion in our relationship. I will give this as gifts as the price is great for the life-improving benefits it provides. This is a CD for everyone!\n",
      "\n",
      "This is coming from a person that didn't believe that mediation helped anyone. I was given this tape from a friend, and it took me a week before I even opened it. From the very first time I listened to it, I was like \"WOW\"!! I have lived with anxiety and panics for 20 years and now anytime I need to just calm my mind and be in total relaxation I do the 10 min. part of the CD. It is truly amazing how it helps me be relaxed, confident, and in total peace. I have been truly blessed by the CD, and by the friend that introduced it to me! This is one of the greatest life changing gifts I have ever received.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first five reviews just to get a sense of the data\n",
    "for row in mard_reviews[:5]:\n",
    "    print(row['reviewText'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd91dc",
   "metadata": {},
   "source": [
    "### Vectorize Dataset\n",
    "\n",
    "Now we turn it into a usable data structure so that we can start analyzing it. First we turn it into a TF-IDF matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f331a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_list = []\n",
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words={'english'})\n",
    "\n",
    "for i,row in enumerate(mard_reviews):\n",
    "    reviews_list.append(row['reviewText'])\n",
    "\n",
    "mard_tfidf = vectorizer.fit_transform(reviews_list)\n",
    "print(mard_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5238f",
   "metadata": {},
   "source": [
    "### Get Unique Artists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc192e9e",
   "metadata": {},
   "source": [
    "Getting a list of all unique artists in the MARD dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06b85213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique artists: 36676\n"
     ]
    }
   ],
   "source": [
    "mard_artists = set([])\n",
    "\n",
    "for row in mard_metadata:\n",
    "    if 'artist' in row:\n",
    "        mard_artists.add(clean_str(row['artist'], cap_code=1))\n",
    "\n",
    "mard_artists = sorted(list(mard_artists))\n",
    "        \n",
    "print(\"Number of unique artists:\", len(mard_artists))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad1bdf3",
   "metadata": {},
   "source": [
    "## Pitchfork Reviews Dataset\n",
    "\n",
    "### Loading and Understanding the Data\n",
    "\n",
    "Loading in sqlite db. Source: [link](https://www.kaggle.com/nolanbconaway/pitchfork-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcec570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pf_tables = ['artists', 'content', 'genres', 'labels', 'reviews', 'years']\n",
    "pitchfork_db = {}\n",
    "\n",
    "for table in all_pf_tables:\n",
    "    pitchfork_db[table] = run_query_on_sqlite_db(\"SELECT * FROM \" + table, \"../data/raw/pitchfork_reviews.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e59861ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique genres in db:\n",
      "['electronic' 'metal' 'rock' None 'rap' 'experimental' 'pop/r&b'\n",
      " 'folk/country' 'jazz' 'global']\n"
     ]
    }
   ],
   "source": [
    "# Printing all unique genres in db\n",
    "print(\"Unique genres in db:\")\n",
    "print(pd.unique(pitchfork_db['genres']['genre']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "819e10ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE OF REVIEW TITLES\n",
      "0                                             mezzanine\n",
      "1                                          prelapsarian\n",
      "2                                  all of them naturals\n",
      "3                                           first songs\n",
      "4                                             new start\n",
      "5         insecure (music from the hbo original series)\n",
      "6                               stillness in wonderland\n",
      "7                                              tehillim\n",
      "8                                            reflection\n",
      "9                          filthy america its beautiful\n",
      "10                                clear sounds/perfetta\n",
      "11                                     run the jewels 3\n",
      "12                                                nadir\n",
      "13                                        december 99th\n",
      "14                                     don't smoke rock\n",
      "15    punk45: les punks: the french connection (the ...\n",
      "16                                      brnshj (puncak)\n",
      "17                             merry christmas lil mama\n",
      "18                                      ///// effectual\n",
      "19                                    love you to death\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"SAMPLE OF REVIEW TITLES\")\n",
    "print(pitchfork_db['reviews']['title'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45440a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE OF REVIEW CONTENT\n",
      "0     “Trip-hop” eventually became a ’90s punchline,...\n",
      "1     Eight years, five albums, and two EPs in, the ...\n",
      "2     Minneapolis’ Uranium Club seem to revel in bei...\n",
      "3     Kleenex began with a crash. It transpired one ...\n",
      "4     It is impossible to consider a given release b...\n",
      "5     In the pilot episode of “Insecure,” the critic...\n",
      "6     Rapper Simbi Ajikawo, who records as Little Si...\n",
      "7     For the last thirty years, Israel’s electronic...\n",
      "8     Ambient music is a funny thing. As innocuous a...\n",
      "9     There were innumerable cameos at the Bad Boy F...\n",
      "10    Lots of drone musicians have been called sound...\n",
      "11    On 2006’s “That’s Life,” Killer Mike boasted “...\n",
      "12    “Why so sad?/Don’t feel so bad/Get out of bed,...\n",
      "13    In January 2016, rapper/actor Yasiin Bey annou...\n",
      "14    Don’t take your eyes off Pete Rock. The early-...\n",
      "15    Soul Jazz’s Punk 45 series has made it its mis...\n",
      "16    It’s safe to say there is no other band on the...\n",
      "17    When Chance the Rapper performed “Sunday Candy...\n",
      "18    It’s not easy for drummers to get the spotligh...\n",
      "19    For over a decade now, The-Dream has demonstra...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"SAMPLE OF REVIEW CONTENT\")\n",
    "print(pitchfork_db['content']['content'][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d84dc1",
   "metadata": {},
   "source": [
    "### Get Unique Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8e76497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num unique artists: 8633\n",
      "\n",
      "First twenty artists (alphabetic order):\n",
      "['', '!!!', '+/-', '-ZIQ', '...AND YOU WILL KNOW US BY THE TRAIL OF DEAD', '1,2,3', '10 IN THE SWEAR JAR', '11:11', '12 RODS', '120 DAYS', '13 & GOD', '13GHOSTS', '13TH FLOOR ELEVATORS', '1990S', '2 CHAINZ', '2 MANY DJS', '20 MILES', '20 MINUTE LOOP', '200 YEARS', '21 SAVAGE']\n"
     ]
    }
   ],
   "source": [
    "pitchfork_artists = sorted([clean_str(a, cap_code=1) for a in pd.unique(pitchfork_db['artists']['artist'])])\n",
    "\n",
    "print(\"Num unique artists:\", len(pitchfork_artists))\n",
    "print()\n",
    "print(\"First twenty artists (alphabetic order):\")\n",
    "print(sorted(pitchfork_artists)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb7b3f",
   "metadata": {},
   "source": [
    "## Comparing Artist Lists\n",
    "\n",
    "Comparing against the list of artists from our baseline TFIDF matrix for artist search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bf543af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"CATS\" 1981 ORIGINAL LONDON CAST', '\"WEIRD AL\" YANKOVIC', '$ATORI ZOOM', '$NOT', \"'IN THE HEIGHTS' ORIGINAL BROADWAY COMPANY\", '((( O )))', '(EM)', '*SPANDREL', '03 GREEDO', '070 SHAKE', '10 YEARS', '100 GECS', '1927', '2 CHAINZ', '20/20', '21 SAVAGE', '24HRS', '24KGOLDN', '27CLUB', '2KBABY']\n"
     ]
    }
   ],
   "source": [
    "kaggle_tfidf = pd.read_csv(\"../data/processed/tfidf_mat_compressed.csv\")\n",
    "kaggle_artists = sorted([ clean_str(s, cap_code=1) for s in kaggle_tfidf.values[:,0] ])\n",
    "\n",
    "print(kaggle_artists[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08361a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store artist sets from different databases in a single dictionary\n",
    "artist_sets = {}\n",
    "artist_sets['tfidf'] = set(kaggle_artists)\n",
    "artist_sets['pitchfork'] = set(pitchfork_artists)\n",
    "artist_sets['mard'] = set(mard_artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c96c85",
   "metadata": {},
   "source": [
    "Basic intersection of artist strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07365d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF && pitchfork: 497\n",
      "TFIDF && mard: 1220\n",
      "pitchfork && mard: 1216\n",
      "TFIDF && (pitchfork || mard): 1509\n"
     ]
    }
   ],
   "source": [
    "# Store intersections into variables for easy access\n",
    "tf_pf = list(artist_sets['tfidf'].intersection(artist_sets['pitchfork']))\n",
    "tf_mard = list(artist_sets['tfidf'].intersection(artist_sets['mard']))\n",
    "pf_mard = list(artist_sets['pitchfork'].intersection(artist_sets['mard']))\n",
    "tf_pfmard = list(artist_sets['tfidf'].intersection(artist_sets['pitchfork'].union(artist_sets['mard'])))\n",
    "                                          \n",
    "print(\"TFIDF && pitchfork:\", len(tf_pf))\n",
    "print(\"TFIDF && mard:\", len(tf_mard))\n",
    "print(\"pitchfork && mard:\", len(pf_mard))\n",
    "print(\"TFIDF && (pitchfork || mard):\", len(tf_pfmard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e1806b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST 30 ARTISTS IN (TFIDF && (pitchfork || mard)):\n",
      "\n",
      "['MAELO RUIZ', 'KEHLANI', 'PAUL BADURA-SKODA', 'WET WET WET', 'CECILIA', 'EMANUEL AX', 'JON NAKAMATSU', 'COMMON', 'THE PRESIDENTS OF THE UNITED STATES OF AMERICA', 'MANCHESTER ORCHESTRA', 'TWISTA', 'JOSS STONE', 'NAT KING COLE', 'RIDE', 'MIX MASTER MIKE', 'SAVAGE', 'TIM MCGRAW', 'EL TRONO DE MEXICO', 'CULTURE BEAT', 'EVE', 'BUJU BANTON', 'GUAYACAN ORQUESTA', 'PHISH', 'JOHN LEGEND', 'BTS', 'ELECTRIC GUEST', 'ELLIE GOULDING', 'TALK TALK', 'DUNCAN DHU', 'DESIIGNER']\n"
     ]
    }
   ],
   "source": [
    "print(\"FIRST 30 ARTISTS IN (TFIDF && (pitchfork || mard)):\\n\")\n",
    "print(tf_pfmard[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b94326",
   "metadata": {},
   "source": [
    "## Mapping Artist to Review Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25a18ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assign which list of artists will be our finalized\n",
    "# list of artists to consult for reviews.\n",
    "ALL_ARTISTS = tf_pfmard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b385895",
   "metadata": {},
   "source": [
    "### Artist to Index Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff25df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_to_i = {a:i for i,a in enumerate(ALL_ARTISTS)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37db925d",
   "metadata": {},
   "source": [
    "### Review Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98447f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_reviews_df = run_query_on_sqlite_db(\n",
    "    \"\"\"\n",
    "    SELECT reviews.title, reviews.artist, reviews.score, content.content\n",
    "      FROM content\n",
    "      LEFT JOIN reviews\n",
    "      ON content.reviewid = reviews.reviewid\n",
    "    \"\"\", \"../data/raw/pitchfork_reviews.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e51f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'artist', 'score', 'content'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(pf_reviews_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78b363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Delete cell\n",
    "reviews_list = []\n",
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words={'english'})\n",
    "\n",
    "for i,row in enumerate(mard_reviews):\n",
    "    reviews_list.append(row['reviewText'])\n",
    "\n",
    "mard_tfidf = vectorizer.fit_transform(reviews_list)\n",
    "print(mard_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c56e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words={'english'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7d3cb833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews(input_artist_list, input_dataframe, max_reviews=30):\n",
    "    \"\"\"\n",
    "    \n",
    "    Maps artist name to a list of reviews written about that artist.\n",
    "    \n",
    "    \"\"\"\n",
    "    artist_to_review_dict = {}\n",
    "\n",
    "    # Initialize dictionary of all artists\n",
    "    for artist in input_artist_list:\n",
    "        artist_to_review_dict[artist] = []\n",
    "\n",
    "    # Iterate through DataFrame to sanitize strings \n",
    "    # and add reviews to dict\n",
    "    for i,row in input_dataframe.iterrows():\n",
    "        if type(row['artist']) != str or len(row['artist']) == 0:\n",
    "            continue\n",
    "        temp_artist = clean_str(row['artist'], cap_code=1)\n",
    "        if temp_artist in artist_to_review_dict and len(artist_to_review_dict[temp_artist]) <= max_reviews:\n",
    "            temp_review_string = \"\"\n",
    "            if type(row['title']) == str and len(row['title']) > 0:\n",
    "                temp_review_string += clean_str(row['title'])\n",
    "            if type(row['content']) == str and len(row['content']) > 0:\n",
    "                temp_review_string += clean_str(row['content']) if len(temp_review_string)==0 else \" - \" + clean_str(row['content'])\n",
    "            \n",
    "            artist_to_review_dict[temp_artist].append(temp_review_string)\n",
    "    \n",
    "    return artist_to_review_dict\n",
    "\n",
    "pf_atr_dict = process_reviews(ALL_ARTISTS, pf_reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cb298ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data as DataFrames\n",
    "mard_md_df = read_mard_json_as_df('../data/raw/mard_reviews/mard_metadata.json')\n",
    "mard_rev_df = read_mard_json_as_df('../data/raw/mard_reviews/mard_reviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35698390",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['price', 'artist-mbid', 'imUrl', 'confidence', 'categories',\n",
      "       'release-group-mbid', 'amazon-id', 'root-genre', 'title', 'artist',\n",
      "       'label', 'artist_url', 'first-release-year', 'release-mbid', 'songs',\n",
      "       'salesRank', 'related', 'brand'],\n",
      "      dtype='object')\n",
      "Index(['reviewerID', 'amazon-id', 'reviewerName', 'helpful', 'unixReviewTime',\n",
      "       'reviewText', 'overall', 'reviewTime', 'summary'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create a joined DataFrame that has 'artist', 'title', and 'content' fields.\n",
    "print(mard_md_df.keys())\n",
    "print(mard_rev_df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31d596fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mard_combined_dataframe = mard_md_df.merge(mard_rev_df, on='amazon-id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "869b194e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artist-mbid', 'amazon-id', 'title', 'artist', 'label', 'artist_url', 'first-release-year', 'songs', 'salesRank', 'related', 'brand', 'reviewerID', 'reviewerName', 'helpful', 'unixReviewTime', 'content', 'overall', 'reviewTime', 'summary']\n"
     ]
    }
   ],
   "source": [
    "temp_columns_to_drop = ['artist-mbid', 'amazon-id', \n",
    "                        'label', 'artist_url', 'first-release-year', \n",
    "                        'songs', 'salesRank', 'related', 'brand', \n",
    "                        'reviewerID', 'reviewerName', 'helpful', 'unixReviewTime', \n",
    "                        'overall', 'reviewTime', 'summary']\n",
    "\n",
    "mard_combined_dataframe = mard_combined_dataframe.rename(columns={'reviewText': 'content'})\n",
    "mard_combined_dataframe = mard_combined_dataframe.drop(columns=temp_columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "276af0ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mard_atr_dict = process_reviews(ALL_ARTISTS, mard_combined_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cb094caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pf_review(input_artist):\n",
    "    \"\"\"\n",
    "    Gets all Pitchfork reviews for the given artist.\n",
    "    \n",
    "    Input: \n",
    "    - input_artist (string)\n",
    "    \n",
    "    Output:\n",
    "    - list of reviews for given artist. returns empty list if no reviews\n",
    "      exist for the artist, or if the artist is not in the db.\n",
    "    \n",
    "    \"\"\"\n",
    "    return pf_atr_dict[input_artist] if len(pf_atr_dict[input_artist]) > 0 else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7c4ad698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mard_review(input_artist):\n",
    "    \"\"\"\n",
    "    Gets all MARD reviews for the given artist.\n",
    "    \n",
    "    Input: \n",
    "    - input_artist (string)\n",
    "    \n",
    "    Output:\n",
    "    - list of reviews for given artist. returns empty list if no reviews\n",
    "      exist for the artist, or if the artist is not in the db.\n",
    "    \n",
    "    \"\"\"\n",
    "    return mard_atr_dict[input_artist] if len(mard_atr_dict[input_artist]) > 0 else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "15177e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_list(input_artist):\n",
    "    extended_list = get_mard_review(input_artist)\n",
    "    extended_list.extend(get_pf_review(input_artist))\n",
    "    return extended_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5ab641ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mezzanine</td>\n",
       "      <td>massive attack</td>\n",
       "      <td>“Trip-hop” eventually became a ’90s punchline,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prelapsarian</td>\n",
       "      <td>krallice</td>\n",
       "      <td>Eight years, five albums, and two EPs in, the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all of them naturals</td>\n",
       "      <td>uranium club</td>\n",
       "      <td>Minneapolis’ Uranium Club seem to revel in bei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>first songs</td>\n",
       "      <td>kleenex, liliput</td>\n",
       "      <td>Kleenex began with a crash. It transpired one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new start</td>\n",
       "      <td>taso</td>\n",
       "      <td>It is impossible to consider a given release b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264449</th>\n",
       "      <td>Echo</td>\n",
       "      <td>After Jack</td>\n",
       "      <td>Something magical happens when these three lad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264450</th>\n",
       "      <td>Let's Move the World</td>\n",
       "      <td>Lila Garrett &amp; Keaton Simons</td>\n",
       "      <td>Keaton is a brilliant singer, and he and Ms. G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264451</th>\n",
       "      <td>Inside Llewyn Davis : Movie Soundtrack &amp; Other...</td>\n",
       "      <td>Ramblin' Jack Elliot, Joan Beaz,Andrew Rown Su...</td>\n",
       "      <td>A large varity of music, and groups I was not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264452</th>\n",
       "      <td>Inside Llewyn Davis : Movie Soundtrack &amp; Other...</td>\n",
       "      <td>Ramblin' Jack Elliot, Joan Beaz,Andrew Rown Su...</td>\n",
       "      <td>Good ole Ry Cooder. He must know every good mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264453</th>\n",
       "      <td>Inside Llewyn Davis : Movie Soundtrack &amp; Other...</td>\n",
       "      <td>Ramblin' Jack Elliot, Joan Beaz,Andrew Rown Su...</td>\n",
       "      <td>I saw the movie of the making of Llewyn Davis ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>282855 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "0                                               mezzanine   \n",
       "1                                            prelapsarian   \n",
       "2                                    all of them naturals   \n",
       "3                                             first songs   \n",
       "4                                               new start   \n",
       "...                                                   ...   \n",
       "264449                                               Echo   \n",
       "264450                               Let's Move the World   \n",
       "264451  Inside Llewyn Davis : Movie Soundtrack & Other...   \n",
       "264452  Inside Llewyn Davis : Movie Soundtrack & Other...   \n",
       "264453  Inside Llewyn Davis : Movie Soundtrack & Other...   \n",
       "\n",
       "                                                   artist  \\\n",
       "0                                          massive attack   \n",
       "1                                                krallice   \n",
       "2                                            uranium club   \n",
       "3                                        kleenex, liliput   \n",
       "4                                                    taso   \n",
       "...                                                   ...   \n",
       "264449                                         After Jack   \n",
       "264450                       Lila Garrett & Keaton Simons   \n",
       "264451  Ramblin' Jack Elliot, Joan Beaz,Andrew Rown Su...   \n",
       "264452  Ramblin' Jack Elliot, Joan Beaz,Andrew Rown Su...   \n",
       "264453  Ramblin' Jack Elliot, Joan Beaz,Andrew Rown Su...   \n",
       "\n",
       "                                                  content  \n",
       "0       “Trip-hop” eventually became a ’90s punchline,...  \n",
       "1       Eight years, five albums, and two EPs in, the ...  \n",
       "2       Minneapolis’ Uranium Club seem to revel in bei...  \n",
       "3       Kleenex began with a crash. It transpired one ...  \n",
       "4       It is impossible to consider a given release b...  \n",
       "...                                                   ...  \n",
       "264449  Something magical happens when these three lad...  \n",
       "264450  Keaton is a brilliant singer, and he and Ms. G...  \n",
       "264451  A large varity of music, and groups I was not ...  \n",
       "264452  Good ole Ry Cooder. He must know every good mu...  \n",
       "264453  I saw the movie of the making of Llewyn Davis ...  \n",
       "\n",
       "[282855 rows x 3 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([pf_reviews_df, mard_combined_dataframe]).drop(columns=[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c59044a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg: 9.068257123923129\n",
      "Median: 4.0\n",
      "Max: 52\n"
     ]
    }
   ],
   "source": [
    "combined_revs = np.array([ len(get_review_list(a)) if a else 0 for a in ALL_ARTISTS ])\n",
    "print('Avg:', np.average(combined_revs))\n",
    "print('Median:', np.median(combined_revs))\n",
    "print('Max:', np.max(combined_revs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "1727add7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(combined_revs, 20)\n",
    "plt.title('Distribution of number of reviews for each artist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dfd5977b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum reviews: THE BEATLES\n",
      "Minimum reviews: METRO BOOMIN\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum reviews:\", ALL_ARTISTS[np.argmax(combined_revs)])\n",
    "print(\"Minimum reviews:\", ALL_ARTISTS[np.argmin(combined_revs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b8273b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(combined_revs>0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf7752",
   "metadata": {},
   "source": [
    "# Analyzing Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba690fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/artist_reviews.json\") as f:\n",
    "    d = json.load(f)\n",
    "    f.close()\n",
    "REVIEWS_DICT = d\n",
    "ARTISTS_LIST = list(REVIEWS_DICT.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6866279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tfidf_vectorizer(input_artist_to_reviews, input_all_artists):\n",
    "    \"\"\"\n",
    "    Returns TFIDF Vectorizer matrix where each document is an artist and the terms are words in the reviews.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words={'english'}, max_df=0.8, min_df=2)\n",
    "    consolidated_reviews = []\n",
    "    for a in input_all_artists:\n",
    "        consolidated_reviews.append(\" \".join(input_artist_to_reviews[a]))\n",
    "    return vectorizer,vectorizer.fit_transform(consolidated_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2565c511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_count_vectorizer(input_artist_to_reviews, input_all_artists):\n",
    "    \"\"\"\n",
    "    Returns TFIDF Vectorizer matrix where each document is an artist and the terms are words in the reviews.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(analyzer='word', stop_words={'english'}, max_df=0.8, min_df=2)\n",
    "    consolidated_reviews = []\n",
    "    for a in input_all_artists:\n",
    "        consolidated_reviews.append(\" \".join(input_artist_to_reviews[a]))\n",
    "    return vectorizer,vectorizer.fit_transform(consolidated_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38ba3092",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdif_vec, tfidf_mat = make_tfidf_vectorizer(REVIEWS_DICT, ARTISTS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09da3192",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec, count_mat = make_count_vectorizer(REVIEWS_DICT, ARTISTS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "616c1b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '1960s', '1970', '1970s', '1980s', '1990', '1990s', '1997', '1998', '1999', '1st', '20', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '20th', '21', '22', '24', '25', '2nd', '30', '34', '40', '45', '50', '60', '60s', '70', '70s', '80', '80s', '90', '90s', 'aacute', 'ability', 'able', 'about', 'above', 'absolute', 'absolutely', 'accessible', 'accompaniment', 'accomplished', 'acoustic', 'across', 'act', 'action', 'acts', 'actual', 'actually', 'add', 'added', 'adding', 'addition', 'adds', 'admit', 'adult', 'aesthetic', 'aforementioned', 'afraid', 'after', 'again', 'against', 'age', 'aggressive', 'ago', 'agree', 'ahead', 'ain', 'air', 'al', 'album', 'albums', 'alive', 'all', 'allow', 'allowed', 'allows', 'almost', 'alone', 'along', 'alongside', 'already', 'also', 'alternate', 'alternative', 'although', 'always', 'am', 'amazing', 'amazon', 'ambient', 'ambitious', 'america', 'american', 'among', 'amor', 'amount', 'amp', 'an', 'angel', 'annoying', 'another', 'answer', 'anthem', 'anti', 'any', 'anybody', 'anymore', 'anyone', 'anything', 'anyway', 'anywhere', 'apart', 'apparent', 'apparently', 'appeal', 'appealing', 'appear', 'appearance', 'appeared', 'appears', 'appreciate', 'approach', 'appropriate', 'are', 'aren', 'arguably', 'arms', 'around', 'arranged', 'arrangement', 'arrangements', 'arrived', 'art', 'artist', 'artistic', 'artists', 'as', 'aside', 'ask', 'asked', 'asking', 'aspect', 'ass', 'at', 'atmosphere', 'atmospheric', 'attack', 'attempt', 'attempts', 'attention', 'attitude', 'audience', 'audiences', 'audio', 'available', 'average', 'avoid', 'aware', 'away', 'awesome', 'awful', 'awkward', 'baby', 'back', 'background', 'backing', 'bad', 'balance', 'ballad', 'ballads', 'band', 'bands', 'bar', 'barely', 'bars', 'base', 'based', 'basic', 'basically', 'bass', 'bassist', 'be', 'beach', 'bear', 'beat', 'beatles', 'beats', 'beautiful', 'beautifully', 'beauty', 'became', 'because', 'become', 'becomes', 'becoming', 'bed', 'been', 'before', 'began', 'begin', 'beginning', 'begins', 'behind', 'being', 'believe', 'beloved', 'beneath', 'besides', 'best', 'better', 'between', 'beyond', 'big', 'bigger', 'biggest', 'bill', 'bit', 'bits', 'black', 'blast', 'blend', 'blood', 'blown', 'blue', 'blues', 'bob', 'body', 'bonus', 'book', 'booklet', 'boring', 'born', 'both', 'bottom', 'bought', 'box', 'boy', 'boys', 'brain', 'brand', 'brass', 'break', 'breaking', 'breaks', 'breath', 'brian', 'bridge', 'brief', 'bright', 'brilliant', 'bring', 'bringing', 'brings', 'british', 'broke', 'broken', 'brother', 'brothers', 'brought', 'brown', 'build', 'building', 'builds', 'built', 'bunch', 'burning', 'business', 'but', 'buy', 'buying', 'by', 'call', 'called', 'calling', 'calls', 'calm', 'came', 'can', 'canciones', 'cannot', 'capable', 'capture', 'captured', 'captures', 'car', 'care', 'career', 'careful', 'carefully', 'carries', 'carry', 'case', 'cassette', 'cast', 'casual', 'catalog', 'catch', 'catchy', 'caught', 'cause', 'cd', 'cds', 'center', 'central', 'century', 'certain', 'certainly', 'chamber', 'chance', 'change', 'changed', 'changes', 'changing', 'character', 'characters', 'charm', 'charming', 'chart', 'charts', 'cheap', 'check', 'chicago', 'child', 'childhood', 'children', 'choice', 'choices', 'choir', 'choose', 'chord', 'chords', 'chorus', 'choruses', 'chose', 'chris', 'christmas', 'church', 'city', 'claim', 'clarity', 'class', 'classic', 'classical', 'classics', 'clean', 'clear', 'clearly', 'clever', 'close', 'closely', 'closer', 'closes', 'closest', 'closing', 'club', 'co', 'coast', 'cold', 'collaboration', 'collection', 'collections', 'collective', 'collector', 'college', 'color', 'com', 'combination', 'come', 'comes', 'comfort', 'comfortable', 'coming', 'commercial', 'common', 'como', 'company', 'compare', 'compared', 'comparison', 'comparisons', 'compelling', 'compilation', 'complete', 'completely', 'complex', 'composed', 'composer', 'composers', 'composition', 'compositions', 'computer', 'con', 'concept', 'concerned', 'concert', 'conclusion', 'condition', 'confidence', 'confident', 'connection', 'conscious', 'consider', 'considered', 'considering', 'consistent', 'consistently', 'constant', 'constantly', 'contain', 'contained', 'contains', 'contemporary', 'content', 'context', 'continue', 'continues', 'contrast', 'control', 'convincing', 'cool', 'copies', 'copy', 'core', 'could', 'couldn', 'count', 'country', 'couple', 'course', 'cover', 'covered', 'covers', 'crafted', 'crazy', 'create', 'created', 'creates', 'creating', 'creative', 'credit', 'credits', 'crew', 'crisp', 'critical', 'critics', 'cross', 'crowd', 'cry', 'cultural', 'culture', 'curious', 'current', 'currently', 'cut', 'cuts', 'da', 'damn', 'dance', 'dancing', 'dark', 'darker', 'date', 'daughter', 'dave', 'david', 'day', 'days', 'de', 'dead', 'deal', 'death', 'debut', 'decade', 'decades', 'decent', 'decided', 'dedicated', 'deep', 'deeper', 'deeply', 'definitely', 'definitive', 'del', 'delicate', 'deliver', 'delivered', 'delivers', 'delivery', 'dense', 'depth', 'describe', 'described', 'deserves', 'desire', 'despite', 'detail', 'details', 'developed', 'development', 'did', 'didn', 'die', 'died', 'difference', 'different', 'difficult', 'digital', 'direct', 'direction', 'directly', 'dirty', 'disappointed', 'disappointing', 'disappointment', 'disc', 'disco', 'discography', 'discovered', 'discs', 'display', 'distance', 'distant', 'distinct', 'distinctive', 'distorted', 'distortion', 'dj', 'do', 'does', 'doesn', 'doing', 'don', 'done', 'dont', 'door', 'double', 'doubt', 'down', 'download', 'dozen', 'drama', 'dramatic', 'draw', 'drawn', 'draws', 'dream', 'dreams', 'drive', 'driven', 'driving', 'drop', 'dropped', 'drops', 'drug', 'drum', 'drummer', 'drumming', 'drums', 'dry', 'due', 'duet', 'dull', 'duo', 'during', 'dvd', 'dynamic', 'dynamics', 'each', 'eacute', 'ear', 'earlier', 'early', 'earned', 'ears', 'earth', 'easily', 'easy', 'echo', 'edge', 'edition', 'effect', 'effective', 'effects', 'effort', 'efforts', 'eight', 'either', 'el', 'electric', 'electro', 'electronic', 'element', 'elements', 'else', 'elsewhere', 'em', 'emotion', 'emotional', 'emotionally', 'emotions', 'empty', 'en', 'end', 'ended', 'ending', 'endless', 'ends', 'energetic', 'energy', 'engaging', 'enjoy', 'enjoyable', 'enjoyed', 'enjoying', 'enough', 'entire', 'entirely', 'ep', 'epic', 'equal', 'equally', 'era', 'es', 'especially', 'esque', 'essence', 'essential', 'essentially', 'esta', 'established', 'este', 'etc', 'europe', 'even', 'eventually', 'ever', 'every', 'everybody', 'everyday', 'everyone', 'everything', 'everywhere', 'evident', 'evoke', 'ex', 'exact', 'exactly', 'example', 'examples', 'excellent', 'except', 'exception', 'exceptional', 'excited', 'excitement', 'exciting', 'exist', 'exitos', 'expect', 'expectations', 'expected', 'expecting', 'experience', 'experimental', 'explain', 'expression', 'expressive', 'extended', 'extra', 'extremely', 'eye', 'eyed', 'eyes', 'fabulous', 'face', 'fact', 'fail', 'failed', 'fair', 'fairly', 'faith', 'fall', 'falling', 'falls', 'fame', 'familiar', 'family', 'famous', 'fan', 'fans', 'fantastic', 'far', 'fascinating', 'fashion', 'fast', 'faster', 'father', 'favor', 'favorite', 'favorites', 'favourite', 'fear', 'feature', 'featured', 'features', 'featuring', 'feel', 'feeling', 'feelings', 'feels', 'feet', 'fell', 'fellow', 'felt', 'female', 'festival', 'few', 'fi', 'field', 'fifth', 'fight', 'figure', 'fill', 'filled', 'filler', 'film', 'final', 'finale', 'finally', 'find', 'finding', 'finds', 'fine', 'finest', 'finger', 'finish', 'fire', 'first', 'fit', 'fits', 'fitting', 'five', 'flat', 'flavor', 'floor', 'flow', 'flows', 'fly', 'focus', 'focused', 'folk', 'folks', 'follow', 'followed', 'following', 'follows', 'for', 'force', 'forced', 'forever', 'forget', 'forgotten', 'form', 'format', 'former', 'forms', 'formula', 'forth', 'fortunately', 'forward', 'found', 'four', 'fourth', 'frank', 'free', 'freedom', 'french', 'frequently', 'fresh', 'friend', 'friendly', 'friends', 'from', 'front', 'frontman', 'fuck', 'fucking', 'full', 'fully', 'fun', 'funk', 'funky', 'funny', 'further', 'fusion', 'future', 'game', 'gave', 'gem', 'general', 'generally', 'generation', 'genius', 'genre', 'genres', 'gentle', 'genuine', 'george', 'german', 'get', 'gets', 'getting', 'gift', 'girl', 'girls', 'give', 'given', 'gives', 'giving', 'glad', 'glass', 'glorious', 'glory', 'go', 'god', 'goes', 'going', 'gold', 'golden', 'gone', 'gonna', 'good', 'gorgeous', 'gospel', 'got', 'gotta', 'gotten', 'grab', 'grace', 'grand', 'granted', 'great', 'greater', 'greatest', 'green', 'grew', 'groove', 'grooves', 'ground', 'group', 'groups', 'grow', 'growing', 'grown', 'guess', 'guest', 'guitar', 'guitarist', 'guitars', 'guy', 'guys', 'ha', 'had', 'hadn', 'hair', 'half', 'hall', 'hand', 'handful', 'hands', 'happen', 'happened', 'happens', 'happy', 'hard', 'hardcore', 'harder', 'hardly', 'harmonies', 'harmony', 'has', 'hasn', 'hate', 'haunting', 'have', 'haven', 'having', 'he', 'head', 'heads', 'hear', 'heard', 'hearing', 'heart', 'heartfelt', 'hearts', 'heaven', 'heavily', 'heavy', 'held', 'hell', 'help', 'helped', 'helps', 'her', 'here', 'herself', 'hey', 'hi', 'high', 'higher', 'highest', 'highlight', 'highlights', 'highly', 'him', 'himself', 'hint', 'hints', 'hip', 'his', 'history', 'hit', 'hits', 'hitting', 'hold', 'holding', 'holds', 'homage', 'home', 'honest', 'honestly', 'hook', 'hooks', 'hop', 'hope', 'hopefully', 'hoping', 'horn', 'horns', 'hot', 'hour', 'hours', 'house', 'how', 'however', 'huge', 'human', 'humor', 'iacute', 'idea', 'ideal', 'ideas', 'identity', 'if', 'ii', 'ill', 'image', 'imagery', 'imagination', 'imagine', 'immediate', 'immediately', 'impact', 'import', 'important', 'impossible', 'impressed', 'impression', 'impressive', 'in', 'include', 'included', 'includes', 'including', 'increasingly', 'incredible', 'incredibly', 'indeed', 'indie', 'individual', 'industry', 'infectious', 'influence', 'influenced', 'influences', 'information', 'initial', 'initially', 'inner', 'inside', 'inspiration', 'inspired', 'inspiring', 'instance', 'instantly', 'instead', 'instrument', 'instrumental', 'instrumentals', 'instrumentation', 'instruments', 'intended', 'intense', 'intensity', 'interest', 'interested', 'interesting', 'international', 'internet', 'interpretation', 'interpretations', 'interview', 'interviews', 'intimate', 'into', 'intriguing', 'intro', 'introduced', 'introduction', 'involved', 'island', 'isn', 'issue', 'issues', 'item', 'its', 'itself', 'jack', 'jam', 'james', 'jazz', 'jesus', 'job', 'joe', 'john', 'joke', 'journey', 'joy', 'jump', 'just', 'keep', 'keeping', 'keeps', 'kept', 'key', 'keyboard', 'keyboards', 'keys', 'kick', 'kicks', 'kid', 'kids', 'kill', 'killer', 'kind', 'kinda', 'king', 'kiss', 'knew', 'know', 'knowing', 'known', 'knows', 'la', 'label', 'labels', 'lack', 'lacking', 'lacks', 'lady', 'laid', 'land', 'language', 'large', 'largely', 'las', 'last', 'late', 'later', 'latest', 'latin', 'latter', 'layers', 'le', 'lead', 'leading', 'leads', 'learn', 'learned', 'least', 'leave', 'leaves', 'leaving', 'led', 'left', 'legacy', 'legend', 'legendary', 'length', 'less', 'lesser', 'let', 'lets', 'letting', 'level', 'lies', 'life', 'light', 'like', 'liked', 'likely', 'likes', 'limited', 'line', 'liner', 'lines', 'list', 'listen', 'listened', 'listener', 'listeners', 'listening', 'listens', 'listing', 'literally', 'little', 'live', 'lived', 'lively', 'lives', 'living', 'll', 'lo', 'local', 'london', 'lonely', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'loop', 'loose', 'lord', 'los', 'lose', 'losing', 'loss', 'lost', 'lot', 'lots', 'loud', 'love', 'loved', 'lovely', 'lover', 'lovers', 'loves', 'loving', 'low', 'lower', 'lp', 'lps', 'lucky', 'lush', 'lyric', 'lyrical', 'lyrically', 'lyrics', 'machine', 'made', 'magic', 'magnificent', 'main', 'mainstream', 'major', 'majority', 'make', 'makes', 'making', 'male', 'man', 'managed', 'manages', 'manner', 'many', 'march', 'mark', 'marked', 'market', 'marks', 'martin', 'mas', 'massive', 'master', 'masterful', 'masterpiece', 'match', 'material', 'matter', 'mature', 'may', 'maybe', 'me', 'mean', 'meaning', 'means', 'meant', 'meanwhile', 'media', 'meet', 'meets', 'mejor', 'melancholy', 'mellow', 'melodic', 'melodies', 'melody', 'member', 'members', 'memorable', 'memories', 'memory', 'men', 'mention', 'mentioned', 'mere', 'merely', 'mess', 'message', 'met', 'metal', 'mexico', 'mi', 'mic', 'michael', 'mid', 'middle', 'might', 'mike', 'miles', 'million', 'mind', 'mine', 'minimal', 'minor', 'minute', 'minutes', 'miss', 'missed', 'missing', 'mistake', 'mix', 'mixed', 'mixes', 'mixing', 'mode', 'modern', 'mom', 'moment', 'moments', 'momentum', 'money', 'month', 'months', 'mood', 'moody', 'moon', 'more', 'morning', 'most', 'mostly', 'mother', 'motion', 'move', 'moved', 'movement', 'moves', 'movie', 'moving', 'mp3', 'mr', 'mtv', 'much', 'multi', 'multiple', 'music', 'musica', 'musical', 'musically', 'musician', 'musicians', 'musicianship', 'must', 'muy', 'my', 'myself', 'mystery', 'name', 'named', 'names', 'narrative', 'national', 'native', 'natural', 'naturally', 'nature', 'near', 'nearly', 'necessarily', 'necessary', 'need', 'needed', 'needs', 'neither', 'never', 'nevertheless', 'new', 'news', 'next', 'nice', 'nicely', 'night', 'nine', 'no', 'nobody', 'noise', 'non', 'none', 'nonetheless', 'nor', 'nos', 'nostalgia', 'not', 'notable', 'notch', 'note', 'notes', 'nothing', 'notice', 'now', 'nowhere', 'ntilde', 'number', 'numbers', 'oacute', 'obvious', 'obviously', 'occasional', 'occasionally', 'ocean', 'odd', 'oddly', 'off', 'offer', 'offered', 'offering', 'offers', 'official', 'often', 'oh', 'ok', 'okay', 'old', 'older', 'on', 'once', 'one', 'ones', 'only', 'onto', 'open', 'opened', 'opener', 'opening', 'opens', 'opera', 'opinion', 'opportunity', 'opposite', 'or', 'orchestra', 'orchestral', 'order', 'ordered', 'organ', 'oriented', 'original', 'originally', 'originals', 'other', 'others', 'otherwise', 'our', 'out', 'output', 'outside', 'outstanding', 'over', 'overall', 'own', 'pace', 'package', 'packaging', 'packed', 'page', 'paid', 'pain', 'pair', 'paper', 'par', 'para', 'parents', 'part', 'particular', 'particularly', 'parts', 'party', 'pass', 'passages', 'passed', 'passing', 'passion', 'passionate', 'past', 'path', 'paul', 'pay', 'peace', 'peak', 'peers', 'people', 'per', 'percussion', 'perfect', 'perfection', 'perfectly', 'perform', 'performance', 'performances', 'performed', 'performer', 'performers', 'performing', 'perhaps', 'period', 'pero', 'person', 'persona', 'personal', 'personality', 'personally', 'perspective', 'peter', 'phrase', 'phrasing', 'piano', 'pick', 'picked', 'picking', 'picture', 'piece', 'pieces', 'pink', 'pitch', 'place', 'placed', 'places', 'plain', 'play', 'played', 'player', 'players', 'playful', 'playing', 'plays', 'pleasant', 'please', 'pleased', 'pleasure', 'plenty', 'plus', 'poetry', 'point', 'points', 'polished', 'political', 'poor', 'pop', 'popular', 'popularity', 'por', 'position', 'positive', 'possible', 'possibly', 'post', 'potential', 'power', 'powerful', 'practically', 'praise', 'pre', 'precision', 'predecessor', 'prefer', 'presence', 'present', 'presented', 'presents', 'press', 'pretty', 'previous', 'previously', 'price', 'primarily', 'prime', 'print', 'prior', 'probably', 'problem', 'problems', 'process', 'produce', 'produced', 'producer', 'producers', 'producing', 'product', 'production', 'productions', 'professional', 'progression', 'project', 'projects', 'promise', 'proof', 'proper', 'proud', 'prove', 'proves', 'provide', 'provided', 'provides', 'providing', 'psychedelic', 'public', 'pull', 'pulls', 'punch', 'punk', 'purchase', 'purchased', 'pure', 'purpose', 'push', 'pushing', 'put', 'puts', 'putting', 'qualities', 'quality', 'quartet', 'que', 'queen', 'question', 'quick', 'quickly', 'quiet', 'quietly', 'quite', 'quot', 'radio', 'rain', 'range', 'rap', 'rapper', 'rappers', 'rapping', 'rare', 'rarely', 'rate', 'rather', 'rating', 'raw', 're', 'reach', 'read', 'reading', 'ready', 'real', 'reality', 'realize', 'realized', 'really', 'reason', 'reasons', 'recall', 'recalls', 'received', 'recent', 'recently', 'recognizable', 'recommend', 'recommended', 'record', 'recorded', 'recording', 'recordings', 'records', 'red', 'reference', 'references', 'refrain', 'refreshing', 'regardless', 'regret', 'regular', 'reissue', 'relationship', 'relatively', 'release', 'released', 'releases', 'releasing', 'remain', 'remaining', 'remains', 'remarkable', 'remastered', 'remember', 'remind', 'reminder', 'reminds', 'reminiscent', 'remix', 'remixes', 'rendition', 'repeat', 'repeated', 'repertoire', 'repetitive', 'replaced', 'represents', 'reputation', 'respect', 'response', 'rest', 'result', 'results', 'return', 'returned', 'returns', 'reveals', 'reverb', 'review', 'reviewer', 'reviewers', 'reviews', 'rhythm', 'rhythmic', 'rhythms', 'rich', 'richard', 'ride', 'riff', 'riffs', 'right', 'rip', 'rise', 'road', 'robert', 'rock', 'rocks', 'role', 'roll', 'rolling', 'romance', 'romantic', 'room', 'roots', 'rough', 'round', 'run', 'running', 'runs', 'rush', 'sad', 'sadly', 'safe', 'said', 'same', 'sample', 'samples', 'sang', 'satisfying', 'save', 'saw', 'say', 'saying', 'says', 'scale', 'scene', 'scenes', 'school', 'score', 'se', 'sea', 'search', 'searching', 'second', 'seconds', 'secret', 'section', 'sections', 'see', 'seeing', 'seem', 'seemed', 'seemingly', 'seems', 'seen', 'selection', 'selections', 'self', 'sell', 'seller', 'selling', 'send', 'sense', 'series', 'serious', 'seriously', 'serve', 'served', 'serves', 'service', 'session', 'sessions', 'set', 'sets', 'setting', 'seven', 'several', 'sex', 'sexy', 'shadow', 'shake', 'shame', 'shape', 'share', 'sharp', 'she', 'sheer', 'shift', 'shifts', 'shine', 'shines', 'shit', 'short', 'shot', 'should', 'shouldn', 'show', 'showcase', 'showcases', 'showing', 'shows', 'si', 'sick', 'side', 'sides', 'sign', 'signature', 'significant', 'silly', 'similar', 'similarly', 'simple', 'simply', 'sin', 'since', 'sing', 'singer', 'singers', 'singing', 'single', 'singles', 'sings', 'singular', 'sister', 'sit', 'sitting', 'six', 'skill', 'skills', 'skip', 'sky', 'sleep', 'slick', 'slight', 'slightly', 'slow', 'slower', 'slowly', 'small', 'smile', 'smooth', 'so', 'social', 'soft', 'sold', 'solid', 'solo', 'solos', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometimes', 'somewhat', 'somewhere', 'son', 'song', 'songs', 'songwriter', 'songwriting', 'sonic', 'soon', 'soothing', 'sophomore', 'sorry', 'sort', 'soul', 'soulful', 'sound', 'sounded', 'sounding', 'sounds', 'soundtrack', 'source', 'south', 'space', 'spanish', 'speak', 'speaking', 'speaks', 'special', 'specific', 'specifically', 'spectacular', 'speed', 'spend', 'spent', 'spirit', 'spiritual', 'split', 'spoken', 'spot', 'spots', 'spring', 'stage', 'stand', 'standard', 'standards', 'standing', 'standout', 'stands', 'star', 'stars', 'start', 'started', 'starting', 'starts', 'state', 'statement', 'states', 'status', 'stay', 'steady', 'stellar', 'step', 'stereo', 'steve', 'stick', 'still', 'stone', 'stop', 'store', 'stories', 'story', 'straight', 'straightforward', 'strange', 'street', 'strength', 'stretch', 'striking', 'string', 'strings', 'stripped', 'strong', 'stronger', 'strongest', 'strongly', 'structure', 'stuck', 'studio', 'stuff', 'stunning', 'style', 'styles', 'stylistic', 'su', 'subject', 'sublime', 'subsequent', 'subtle', 'success', 'successful', 'such', 'suddenly', 'suggest', 'suggests', 'suite', 'summer', 'sun', 'sung', 'super', 'superb', 'superior', 'support', 'supposed', 'sure', 'surely', 'surface', 'surprise', 'surprised', 'surprising', 'surprisingly', 'sus', 'sweet', 'swing', 'symphony', 'synth', 'synths', 'system', 'take', 'taken', 'takes', 'taking', 'talent', 'talented', 'talents', 'talk', 'talking', 'tape', 'tapes', 'taste', 'te', 'team', 'tears', 'technical', 'technique', 'techno', 'tell', 'telling', 'tells', 'tempo', 'tempos', 'ten', 'tend', 'tender', 'tension', 'term', 'terms', 'terrible', 'terrific', 'territory', 'test', 'texture', 'textures', 'than', 'thank', 'thankfully', 'thanks', 'that', 'their', 'them', 'theme', 'themes', 'themselves', 'then', 'there', 'these', 'they', 'thick', 'thin', 'thing', 'things', 'think', 'thinking', 'third', 'thoroughly', 'those', 'though', 'thought', 'thoughts', 'three', 'thrilling', 'through', 'throughout', 'throw', 'thus', 'tight', 'time', 'timeless', 'times', 'tired', 'title', 'titled', 'titles', 'today', 'todo', 'todos', 'together', 'told', 'tom', 'tone', 'tones', 'too', 'took', 'top', 'total', 'totally', 'touch', 'touches', 'tough', 'tour', 'toward', 'towards', 'town', 'track', 'tracks', 'trademark', 'tradition', 'traditional', 'treasure', 'treat', 'tribute', 'tried', 'tries', 'trio', 'trip', 'true', 'truly', 'trust', 'truth', 'try', 'trying', 'tu', 'tune', 'tunes', 'turn', 'turned', 'turning', 'turns', 'tv', 'twice', 'two', 'type', 'typical', 'typically', 'uacute', 'uk', 'ultimate', 'ultimately', 'un', 'una', 'under', 'underground', 'underrated', 'understand', 'understanding', 'unexpected', 'unfortunately', 'unique', 'unknown', 'unless', 'unlike', 'uno', 'unreleased', 'until', 'unusual', 'up', 'upbeat', 'uplifting', 'upon', 'ups', 'us', 'use', 'used', 'uses', 'using', 'usual', 'usually', 'vaguely', 'value', 'van', 'variations', 'varied', 'variety', 'various', 've', 'vein', 'verse', 'verses', 'version', 'versions', 'very', 'via', 'vibe', 'video', 'videos', 'view', 'vintage', 'vinyl', 'violin', 'vision', 'vocal', 'vocalist', 'vocals', 'voice', 'voices', 'vol', 'volume', 'wait', 'waiting', 'wake', 'walk', 'walking', 'wall', 'wanna', 'want', 'wanted', 'wanting', 'wants', 'war', 'warm', 'warmth', 'was', 'wasn', 'waste', 'watch', 'watching', 'water', 'wave', 'way', 'ways', 'we', 'weak', 'week', 'weeks', 'weight', 'weird', 'welcome', 'well', 'went', 'were', 'weren', 'west', 'western', 'what', 'whatever', 'when', 'where', 'whether', 'which', 'while', 'white', 'who', 'whole', 'whom', 'whose', 'why', 'wide', 'wife', 'wild', 'will', 'willing', 'wind', 'wise', 'wish', 'with', 'within', 'without', 'woman', 'women', 'won', 'wonder', 'wonderful', 'wonderfully', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worse', 'worst', 'worth', 'worthy', 'would', 'wouldn', 'wow', 'write', 'writer', 'writing', 'written', 'wrong', 'wrote', 'ya', 'yeah', 'year', 'years', 'yes', 'yet', 'yo', 'york', 'you', 'young', 'younger', 'your', 'yourself', 'youth']\n"
     ]
    }
   ],
   "source": [
    "print(review_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec750e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cossim(query_vec, input_mtx, input_artists=ARTISTS_LIST):\n",
    "    scores = input_mtx.dot(query_vec)\n",
    "    ranking = np.argsort(scores)\n",
    "    return [(input_artists[i], scores[i]) for i in ranking[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31cb9a40",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_df corresponds to < documents than min_df",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-9f6db543e3cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"in addition i arrived art artists behind being believe of course but why\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_query\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/singerenv/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1235\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_doc_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m                 raise ValueError(\n\u001b[0;32m-> 1237\u001b[0;31m                     \"max_df corresponds to < documents than min_df\")\n\u001b[0m\u001b[1;32m   1238\u001b[0m             X, self.stop_words_ = self._limit_features(X, vocabulary,\n\u001b[1;32m   1239\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
     ]
    }
   ],
   "source": [
    "test_query = \"in addition i arrived art artists behind being believe of course but why\"\n",
    "\n",
    "print(count_vec.fit_transform([test_query]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae02cdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'addition',\n",
       " 'arrived',\n",
       " 'art',\n",
       " 'artists',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'of',\n",
       " 'course',\n",
       " 'but',\n",
       " 'why']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_tokenizer = count_vec.build_tokenizer()\n",
    "\n",
    "string_tokenizer(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b03b9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtistReviewAnalyzer:\n",
    "    def __init__(self, fileloc, min_df=2, max_df=0.8):\n",
    "        self.__tokenizer = self.count_vectorizer.build_tokenizer()\n",
    "        self.raw = ArtistReviewAnalyzer.load_json(fileloc)\n",
    "        self.artists_list = list(self.raw.keys())\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words={'english'}, min_df=min_df, max_df=max_df)\n",
    "        self.count_vectorizer = CountVectorizer(analyzer='word', stop_words={'english'}, min_df=min_df, max_df=max_df)\n",
    "\n",
    "        # Spacy tool\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "        # List of lists of strings\n",
    "        self.tokenized_reviews = self.build_reviews_list(do_tokenize=True)\n",
    "\n",
    "        # Stop words\n",
    "        self.stop_words = self.build_stopwords()\n",
    "\n",
    "        self.bigrams = None\n",
    "        self.lemmatized_text = None\n",
    "        self.lda_model = self.make_lda_model(num_topics=20)\n",
    "\n",
    "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.raw_reviews)\n",
    "        self.count_matrix = self.count_vectorizer.fit_transform(self.raw_reviews)\n",
    "        self.raw_reviews = self.build_reviews_list()  # List of strings\n",
    "\n",
    "    def build_stopwords(self):\n",
    "        sw = stopwords.words('english')\n",
    "        sw.extend(['album', 'music', 'cd', 'track', 'song', 'sound'])\n",
    "        return sw\n",
    "\n",
    "    def update_stopwords(self, word_list):\n",
    "        self.stopwords.extend(word_list)\n",
    "        return None\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        self.tokenized_reviews = [[w for w in artist_review if w not in self.stop_words] for artist_review in self.tokenized_reviews]\n",
    "\n",
    "    def make_bigrams(self):\n",
    "        bg = gensim.models.Phrases(self.tokenized_reviews, min_count=5, threshold=100)\n",
    "        bg_mod = gensim.models.phrases.Phraser(bg)\n",
    "        return [bg_mod[d] for d in self.tokenized_reviews]\n",
    "\n",
    "    def lemmatize(self, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        texts_out = []\n",
    "        for review_words in self.tokenized_reviews:\n",
    "            joined_words = self.nlp(\" \".join(review_words))\n",
    "            texts_out.append([token.lemma_ for token in joined_words if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "\n",
    "    def make_lda_model(self, num_topics):\n",
    "        self.bigrams = self.make_bigrams()\n",
    "        self.lemmatized_text = self.lemmatize()\n",
    "        id2word = corpora.Dictionary(self.lemmatized_text)\n",
    "        corpus = [id2word.doc2bow(w) for w in self.lemmatized_text]\n",
    "        return gensim.models.ldamodel.LdaModel(corpus, id2word, num_topics)\n",
    "\n",
    "    def tokenize(self, input_string):\n",
    "        return self.__tokenizer(input_string)\n",
    "\n",
    "    def build_reviews_list(self, do_tokenize=False):\n",
    "        consolidated_reviews = []\n",
    "        for a in self.artists_list:\n",
    "            if do_tokenize:\n",
    "                consolidated_reviews.append(self.tokenize(\" \".join(self.raw[a])))\n",
    "            else:\n",
    "                consolidated_reviews.append(\" \".join(self.raw[a]))\n",
    "        return consolidated_reviews\n",
    "\n",
    "    def get_all_words(self, do_tokenize=False):\n",
    "        return self.tokenize(\" \".join(self.raw_reviews)) if do_tokenize else \" \".join(self.raw_reviews)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_json(fileloc):\n",
    "        with open(fileloc, 'r') as file:\n",
    "            d = json.load(file)\n",
    "            file.close()\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "11b57cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_analyzer = ArtistReviewAnalyzer(\"../data/processed/artist_reviews.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "af737390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17822772\n"
     ]
    }
   ],
   "source": [
    "artist_analyzer.tokenize(test_query)\n",
    "print(len(artist_analyzer.get_all_words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "93c3dd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/celinechoo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import nltk; nltk.download('stopwords')\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9701bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "04aa8350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 2), (15, 4), (16, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 2), (24, 5), (25, 1), (26, 6), (27, 10), (28, 1), (29, 1)]\n"
     ]
    }
   ],
   "source": [
    "id2word = corpora.Dictionary(artist_analyzer.tokenized_reviews)\n",
    "texts = artist_analyzer.tokenized_reviews\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bce8ad03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.046*\"the\" + 0.030*\"of\" + 0.021*\"and\" + 0.017*\"to\" + 0.016*\"is\" + '\n",
      "  '0.011*\"it\" + 0.011*\"in\" + 0.010*\"that\" + 0.009*\"this\" + 0.009*\"with\"'),\n",
      " (1,\n",
      "  '0.042*\"the\" + 0.023*\"of\" + 0.021*\"and\" + 0.020*\"to\" + 0.014*\"is\" + '\n",
      "  '0.010*\"in\" + 0.008*\"that\" + 0.008*\"it\" + 0.008*\"The\" + 0.008*\"this\"'),\n",
      " (2,\n",
      "  '0.032*\"the\" + 0.022*\"of\" + 0.019*\"and\" + 0.016*\"is\" + 0.015*\"to\" + '\n",
      "  '0.011*\"it\" + 0.010*\"with\" + 0.010*\"this\" + 0.008*\"in\" + 0.008*\"that\"'),\n",
      " (3,\n",
      "  '0.054*\"the\" + 0.028*\"and\" + 0.026*\"of\" + 0.019*\"to\" + 0.017*\"is\" + '\n",
      "  '0.014*\"in\" + 0.013*\"that\" + 0.010*\"on\" + 0.009*\"it\" + 0.009*\"this\"'),\n",
      " (4,\n",
      "  '0.032*\"the\" + 0.028*\"and\" + 0.023*\"of\" + 0.018*\"to\" + 0.014*\"is\" + '\n",
      "  '0.012*\"it\" + 0.012*\"in\" + 0.009*\"that\" + 0.008*\"with\" + 0.008*\"The\"'),\n",
      " (5,\n",
      "  '0.050*\"the\" + 0.031*\"of\" + 0.029*\"and\" + 0.017*\"to\" + 0.014*\"is\" + '\n",
      "  '0.011*\"in\" + 0.010*\"it\" + 0.009*\"that\" + 0.008*\"this\" + 0.008*\"for\"'),\n",
      " (6,\n",
      "  '0.034*\"the\" + 0.029*\"to\" + 0.029*\"of\" + 0.026*\"and\" + 0.013*\"is\" + '\n",
      "  '0.012*\"that\" + 0.011*\"in\" + 0.011*\"it\" + 0.008*\"this\" + 0.008*\"with\"'),\n",
      " (7,\n",
      "  '0.042*\"the\" + 0.029*\"and\" + 0.026*\"of\" + 0.024*\"to\" + 0.016*\"in\" + '\n",
      "  '0.014*\"is\" + 0.011*\"it\" + 0.009*\"this\" + 0.008*\"with\" + 0.008*\"that\"'),\n",
      " (8,\n",
      "  '0.047*\"the\" + 0.024*\"of\" + 0.024*\"and\" + 0.013*\"it\" + 0.012*\"to\" + '\n",
      "  '0.012*\"is\" + 0.011*\"in\" + 0.010*\"that\" + 0.008*\"on\" + 0.007*\"for\"'),\n",
      " (9,\n",
      "  '0.036*\"the\" + 0.026*\"of\" + 0.022*\"and\" + 0.016*\"to\" + 0.014*\"is\" + '\n",
      "  '0.011*\"that\" + 0.010*\"in\" + 0.009*\"it\" + 0.008*\"with\" + 0.008*\"The\"')]\n"
     ]
    }
   ],
   "source": [
    "num_topics = 10\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "64cc50cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['album', 'music', 'cd', 'track', 'song', 'sound'])\n",
    "\n",
    "reviews_nostop = [[w for w in artist_review if w not in stop_words] for artist_review in artist_analyzer.tokenized_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "65053ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    bigram = gensim.models.Phrases(artist_analyzer.tokenized_reviews, min_count=5, threshold=100)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "80099cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fe51c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bigrams = make_bigrams(reviews_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "da8faead",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lemmatized = lemmatization(test_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4f7ccd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id2word = corpora.Dictionary(test_lemmatized)\n",
    "test_lda_model = gensim.models.ldamodel.LdaModel(corpus=[test_id2word.doc2bow(text) for text in test_lemmatized],\n",
    "                                                 id2word=test_id2word,\n",
    "                                                 num_topics=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1860211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.010*\"album\" + 0.009*\"music\" + 0.008*\"sound\" + 0.007*\"song\" + 0.007*\"make\" '\n",
      "  '+ 0.006*\"record\" + 0.005*\"well\" + 0.005*\"good\" + 0.005*\"band\" + 0.005*\"cd\"'),\n",
      " (1,\n",
      "  '0.009*\"album\" + 0.007*\"song\" + 0.007*\"sound\" + 0.007*\"music\" + 0.006*\"good\" '\n",
      "  '+ 0.005*\"make\" + 0.005*\"first\" + 0.005*\"band\" + 0.005*\"record\" + '\n",
      "  '0.004*\"track\"'),\n",
      " (2,\n",
      "  '0.013*\"song\" + 0.012*\"album\" + 0.010*\"music\" + 0.008*\"cd\" + 0.008*\"good\" + '\n",
      "  '0.007*\"great\" + 0.007*\"time\" + 0.006*\"well\" + 0.006*\"make\" + 0.006*\"love\"'),\n",
      " (3,\n",
      "  '0.017*\"song\" + 0.014*\"album\" + 0.011*\"cd\" + 0.010*\"music\" + 0.009*\"good\" + '\n",
      "  '0.008*\"great\" + 0.007*\"sound\" + 0.007*\"love\" + 0.007*\"get\" + 0.007*\"track\"'),\n",
      " (4,\n",
      "  '0.013*\"album\" + 0.011*\"song\" + 0.008*\"good\" + 0.008*\"music\" + 0.007*\"sound\" '\n",
      "  '+ 0.007*\"get\" + 0.006*\"quot\" + 0.006*\"record\" + 0.006*\"track\" + 0.006*\"cd\"'),\n",
      " (5,\n",
      "  '0.020*\"album\" + 0.017*\"song\" + 0.011*\"music\" + 0.009*\"good\" + 0.008*\"sound\" '\n",
      "  '+ 0.007*\"make\" + 0.007*\"track\" + 0.007*\"band\" + 0.007*\"quot\" + '\n",
      "  '0.007*\"great\"'),\n",
      " (6,\n",
      "  '0.015*\"song\" + 0.009*\"album\" + 0.008*\"music\" + 0.008*\"sound\" + 0.007*\"make\" '\n",
      "  '+ 0.006*\"well\" + 0.006*\"record\" + 0.006*\"good\" + 0.006*\"get\" + '\n",
      "  '0.005*\"even\"'),\n",
      " (7,\n",
      "  '0.013*\"album\" + 0.013*\"quot\" + 0.011*\"song\" + 0.008*\"good\" + 0.008*\"music\" '\n",
      "  '+ 0.008*\"well\" + 0.006*\"cd\" + 0.006*\"get\" + 0.006*\"track\" + 0.005*\"sound\"'),\n",
      " (8,\n",
      "  '0.007*\"music\" + 0.006*\"album\" + 0.006*\"make\" + 0.005*\"sound\" + '\n",
      "  '0.005*\"record\" + 0.005*\"song\" + 0.005*\"get\" + 0.005*\"band\" + 0.005*\"time\" + '\n",
      "  '0.005*\"well\"'),\n",
      " (9,\n",
      "  '0.012*\"music\" + 0.012*\"song\" + 0.010*\"cd\" + 0.010*\"good\" + 0.008*\"album\" + '\n",
      "  '0.008*\"sound\" + 0.006*\"time\" + 0.006*\"get\" + 0.006*\"make\" + 0.006*\"track\"'),\n",
      " (10,\n",
      "  '0.021*\"song\" + 0.014*\"album\" + 0.011*\"quot\" + 0.010*\"music\" + 0.008*\"get\" + '\n",
      "  '0.008*\"good\" + 0.007*\"great\" + 0.007*\"sound\" + 0.007*\"love\" + 0.007*\"cd\"'),\n",
      " (11,\n",
      "  '0.015*\"song\" + 0.012*\"album\" + 0.007*\"music\" + 0.007*\"make\" + 0.007*\"sound\" '\n",
      "  '+ 0.007*\"great\" + 0.007*\"good\" + 0.005*\"record\" + 0.005*\"band\" + '\n",
      "  '0.005*\"first\"'),\n",
      " (12,\n",
      "  '0.011*\"album\" + 0.010*\"make\" + 0.006*\"music\" + 0.006*\"get\" + 0.006*\"sound\" '\n",
      "  '+ 0.006*\"song\" + 0.005*\"band\" + 0.005*\"record\" + 0.005*\"well\" + '\n",
      "  '0.005*\"good\"'),\n",
      " (13,\n",
      "  '0.010*\"album\" + 0.009*\"song\" + 0.009*\"music\" + 0.008*\"cd\" + 0.007*\"good\" + '\n",
      "  '0.006*\"record\" + 0.006*\"quot\" + 0.006*\"sound\" + 0.006*\"time\" + '\n",
      "  '0.005*\"make\"'),\n",
      " (14,\n",
      "  '0.012*\"album\" + 0.012*\"song\" + 0.010*\"music\" + 0.009*\"sound\" + '\n",
      "  '0.007*\"record\" + 0.007*\"track\" + 0.006*\"cd\" + 0.006*\"good\" + 0.006*\"well\" + '\n",
      "  '0.005*\"band\"'),\n",
      " (15,\n",
      "  '0.014*\"album\" + 0.010*\"song\" + 0.008*\"music\" + 0.007*\"sound\" + '\n",
      "  '0.006*\"record\" + 0.005*\"love\" + 0.005*\"get\" + 0.004*\"make\" + 0.004*\"time\" + '\n",
      "  '0.004*\"band\"'),\n",
      " (16,\n",
      "  '0.011*\"song\" + 0.010*\"album\" + 0.007*\"music\" + 0.006*\"get\" + 0.006*\"well\" + '\n",
      "  '0.006*\"sound\" + 0.005*\"band\" + 0.005*\"good\" + 0.005*\"great\" + '\n",
      "  '0.004*\"record\"'),\n",
      " (17,\n",
      "  '0.009*\"album\" + 0.008*\"sound\" + 0.007*\"band\" + 0.007*\"song\" + 0.007*\"get\" + '\n",
      "  '0.007*\"music\" + 0.006*\"make\" + 0.006*\"good\" + 0.005*\"record\" + '\n",
      "  '0.005*\"track\"'),\n",
      " (18,\n",
      "  '0.012*\"album\" + 0.010*\"music\" + 0.009*\"good\" + 0.008*\"song\" + 0.008*\"cd\" + '\n",
      "  '0.006*\"make\" + 0.006*\"sound\" + 0.005*\"time\" + 0.005*\"great\" + 0.005*\"well\"'),\n",
      " (19,\n",
      "  '0.013*\"album\" + 0.013*\"song\" + 0.009*\"good\" + 0.009*\"music\" + 0.008*\"cd\" + '\n",
      "  '0.007*\"sound\" + 0.007*\"quot\" + 0.005*\"get\" + 0.005*\"hear\" + 0.005*\"great\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(test_lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a8d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda98a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0cf33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3bed2332",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtistReviewAnalyzer:\n",
    "    def __init__(self, fileloc, min_df=2, max_df=0.8):\n",
    "        self.raw = ArtistReviewAnalyzer.load_json(fileloc)\n",
    "        self.artists_list = list(self.raw.keys())\n",
    "        # self.tfidf_vectorizer = TfidfVectorizer(analyzer='word', stop_words={'english'}, min_df=min_df, max_df=max_df)\n",
    "        self.count_vectorizer = CountVectorizer(analyzer='word', stop_words={'english'}, min_df=min_df, max_df=max_df)\n",
    "        self.__tokenizer = self.count_vectorizer.build_tokenizer()\n",
    "\n",
    "        # Spacy tool\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "        # List of lists of strings\n",
    "        self.tokenized_reviews = self.build_reviews_list(do_tokenize=True)\n",
    "\n",
    "        # Stop words\n",
    "        self.stop_words = self.build_stopwords()\n",
    "\n",
    "        self.bigrams = None\n",
    "        self.lemmatized_text = None\n",
    "        self.lda_model = self.make_lda_model(num_topics=20)\n",
    "\n",
    "        # self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.raw_reviews)\n",
    "        # self.count_matrix = self.count_vectorizer.fit_transform(self.raw_reviews)\n",
    "        # self.raw_reviews = self.build_reviews_list()  # List of strings\n",
    "\n",
    "    def build_stopwords(self):\n",
    "        sw = stopwords.words('english')\n",
    "        sw.extend(['album', 'music', 'cd', 'track', 'song', 'sound'])\n",
    "        return sw\n",
    "\n",
    "    def update_stopwords(self, word_list):\n",
    "        self.stopwords.extend(word_list)\n",
    "        return None\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        self.tokenized_reviews = [[w for w in artist_review if w not in self.stop_words] for artist_review in self.tokenized_reviews]\n",
    "\n",
    "    def make_bigrams(self):\n",
    "        bg = gensim.models.Phrases(self.tokenized_reviews, min_count=5, threshold=100)\n",
    "        bg_mod = gensim.models.phrases.Phraser(bg)\n",
    "        return [bg_mod[d] for d in self.tokenized_reviews]\n",
    "\n",
    "    def lemmatize(self, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        texts_out = []\n",
    "        for review_words in self.tokenized_reviews:\n",
    "            joined_words = self.nlp(\" \".join(review_words))\n",
    "            texts_out.append([token.lemma_ for token in joined_words if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "\n",
    "    def make_lda_model(self, num_topics):\n",
    "        self.bigrams = self.make_bigrams()\n",
    "        self.lemmatized_text = self.lemmatize()\n",
    "        id2word = corpora.Dictionary(self.lemmatized_text)\n",
    "        corpus = [id2word.doc2bow(w) for w in self.lemmatized_text]\n",
    "        return gensim.models.ldamodel.LdaModel(corpus, id2word, num_topics)\n",
    "\n",
    "    def tokenize(self, input_string):\n",
    "        return self.__tokenizer(input_string)\n",
    "\n",
    "    def build_reviews_list(self, do_tokenize=False):\n",
    "        consolidated_reviews = []\n",
    "        for a in self.artists_list:\n",
    "            if do_tokenize:\n",
    "                consolidated_reviews.append(self.tokenize(\" \".join(self.raw[a])))\n",
    "            else:\n",
    "                consolidated_reviews.append(\" \".join(self.raw[a]))\n",
    "        return consolidated_reviews\n",
    "\n",
    "    def get_all_words(self, do_tokenize=False):\n",
    "        return self.tokenize(\" \".join(self.raw_reviews)) if do_tokenize else \" \".join(self.raw_reviews)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_json(fileloc):\n",
    "        with open(fileloc, 'r') as file:\n",
    "            d = json.load(file)\n",
    "            file.close()\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "21f7ed4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-8660f5b79aa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0martist_analyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArtistReviewAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/processed/artist_reviews.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-dfaad51f9231>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fileloc, min_df, max_df)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlda_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_lda_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.raw_reviews)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-112-dfaad51f9231>\u001b[0m in \u001b[0;36mmake_lda_model\u001b[0;34m(self, num_topics)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatized_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.6/envs/singerenv/lib/python3.7/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_from_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "artist_analyzer = ArtistReviewAnalyzer(\"../data/processed/artist_reviews.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08721ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "singerenv",
   "language": "python",
   "name": "singerenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
